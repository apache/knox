{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#draft-work-in-progress","title":"DRAFT - WORK IN PROGRESS","text":""},{"location":"#apache-knox-documentation-home","title":"Apache Knox Documentation Home","text":""},{"location":"#contributing-to-apache-knox-documentation","title":"Contributing to Apache Knox Documentation","text":"<p>Documentation contributions are easily done in markdown format with pull requests like any other contribution in the project.</p> <p>MkDocs is used in Apache Knox to build and publish the documentation but you are not  required to use it in order to make minor changes.</p> <p>However, if you would like to contribute changes we will expect you to indicate how you tested it and to provide screen shots showing how it renders.</p> <p>For those that do want to make larger changes or want to use MkDocs the following may be a good starting point.</p>"},{"location":"#mkdocs-tips","title":"MkDocs Tips","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"admin_api/","title":"Admin API","text":""},{"location":"admin_api/#admin-api","title":"Admin API","text":"<p>Access to the administrator functions of Knox are provided by the Admin REST API.</p>"},{"location":"admin_api/#admin-api-url","title":"Admin API URL","text":"<p>The URL mapping for the Knox Admin API is:</p> Resource URL GatewayAPI <code>https://{gateway-host}:{gateway-port}/{gateway-path}/admin/api/v1</code> <p>Please note that to access this API, the user attempting to connect must have admin credentials configured on the LDAP Server</p>"},{"location":"admin_api/#api-documentation","title":"API Documentation","text":"Resource Operation Description version GET Get the gateway version and the associated version hash Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/version -H Accept:application/json</pre> Example Response <pre>\n{\n  \"ServerVersion\" : {\n    \"version\" : \"VERSION_ID\",\n    \"hash\" : \"VERSION_HASH\"\n  }\n}     </pre> topologies GET Get an enumeration of the topologies currently deployed in the gateway. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/topologies -H Accept:application/json</pre> Example Response <pre>\n{\n   \"topologies\" : {\n      \"topology\" : [ {\n         \"name\" : \"admin\",\n         \"timestamp\" : \"1501508536000\",\n         \"uri\" : \"https://localhost:8443/gateway/admin\",\n         \"href\" : \"https://localhost:8443/gateway/admin/api/v1/topologies/admin\"\n      }, {\n         \"name\" : \"sandbox\",\n         \"timestamp\" : \"1501508536000\",\n         \"uri\" : \"https://localhost:8443/gateway/sandbox\",\n         \"href\" : \"https://localhost:8443/gateway/admin/api/v1/topologies/sandbox\"\n      } ]\n   }\n}     </pre> topologies/{id} GET Get a JSON representation of the specified topology Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/topologies/admin -H Accept:application/json</pre> Example Response <pre>\n{\n  \"name\": \"admin\",\n  \"providers\": [{\n    \"enabled\": true,\n    \"name\": \"ShiroProvider\",\n    \"params\": {\n      \"sessionTimeout\": \"30\",\n      \"main.ldapRealm\": \"org.apache.knox.gateway.shirorealm.KnoxLdapRealm\",\n      \"main.ldapRealm.userDnTemplate\": \"uid={0},ou=people,dc=hadoop,dc=apache,dc=org\",\n      \"main.ldapRealm.contextFactory.url\": \"ldap://localhost:33389\",\n      \"main.ldapRealm.contextFactory.authenticationMechanism\": \"simple\",\n      \"urls./**\": \"authcBasic\"\n    },\n    \"role\": \"authentication\"\n  }, {\n    \"enabled\": true,\n    \"name\": \"AclsAuthz\",\n    \"params\": {\n      \"knox.acl\": \"admin;*;*\"\n    },\n    \"role\": \"authorization\"\n  }, {\n    \"enabled\": true,\n    \"name\": \"Default\",\n    \"params\": {},\n    \"role\": \"identity-assertion\"\n  }, {\n    \"enabled\": true,\n    \"name\": \"static\",\n    \"params\": {\n      \"localhost\": \"sandbox,sandbox.hortonworks.com\"\n    },\n    \"role\": \"hostmap\"\n  }],\n  \"services\": [{\n      \"name\": null,\n      \"params\": {},\n      \"role\": \"KNOX\",\n      \"url\": null\n  }],\n  \"timestamp\": 1406672646000,\n  \"uri\": \"https://localhost:8443/gateway/admin\"\n}     </pre> PUT Add (and deploy) a topology Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/topologies/mytopology \\\n     -X PUT \\\n     -H Content-Type:application/xml\n     -d \"@mytopology.xml\"</pre> Example Response <pre>\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;topology&gt;\n   &lt;uri&gt;https://localhost:8443/gateway/mytopology&lt;/uri&gt;\n   &lt;name&gt;mytopology&lt;/name&gt;\n   &lt;timestamp&gt;1509720338000&lt;/timestamp&gt;\n   &lt;gateway&gt;\n      &lt;provider&gt;\n         &lt;role&gt;authentication&lt;/role&gt;\n         &lt;name&gt;ShiroProvider&lt;/name&gt;\n         &lt;enabled&gt;true&lt;/enabled&gt;\n         &lt;param&gt;\n            &lt;name&gt;sessionTimeout&lt;/name&gt;\n            &lt;value&gt;30&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapRealm&lt;/name&gt;\n            &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n            &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n            &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n            &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n            &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n            &lt;value&gt;simple&lt;/value&gt;\n         &lt;/param&gt;\n         &lt;param&gt;\n            &lt;name&gt;urls./**&lt;/name&gt;\n            &lt;value&gt;authcBasic&lt;/value&gt;\n         &lt;/param&gt;\n      &lt;/provider&gt;\n      &lt;provider&gt;\n         &lt;role&gt;identity-assertion&lt;/role&gt;\n         &lt;name&gt;Default&lt;/name&gt;\n         &lt;enabled&gt;true&lt;/enabled&gt;\n      &lt;/provider&gt;\n      &lt;provider&gt;\n         &lt;role&gt;hostmap&lt;/role&gt;\n         &lt;name&gt;static&lt;/name&gt;\n         &lt;enabled&gt;true&lt;/enabled&gt;\n         &lt;param&gt;\n            &lt;name&gt;localhost&lt;/name&gt;\n            &lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;\n         &lt;/param&gt;\n      &lt;/provider&gt;\n   &lt;/gateway&gt;\n   &lt;service&gt;\n      &lt;role&gt;NAMENODE&lt;/role&gt;\n      &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;JOBTRACKER&lt;/role&gt;\n      &lt;url&gt;rpc://localhost:8050&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;WEBHDFS&lt;/role&gt;\n      &lt;url&gt;http://localhost:50070/webhdfs&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;WEBHCAT&lt;/role&gt;\n      &lt;url&gt;http://localhost:50111/templeton&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;OOZIE&lt;/role&gt;\n      &lt;url&gt;http://localhost:11000/oozie&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;WEBHBASE&lt;/role&gt;\n      &lt;url&gt;http://localhost:60080&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;HIVE&lt;/role&gt;\n      &lt;url&gt;http://localhost:10001/cliservice&lt;/url&gt;\n   &lt;/service&gt;\n   &lt;service&gt;\n      &lt;role&gt;RESOURCEMANAGER&lt;/role&gt;\n      &lt;url&gt;http://localhost:8088/ws&lt;/url&gt;\n   &lt;/service&gt;\n&lt;/topology&gt;</pre> DELETE Delete (and undeploy) a topology Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/topologies/mytopology -X DELETE</pre> Example Response <pre>{ \"deleted\" : true }</pre> providerconfig GET Get an enumeration of the shared provider configurations currently deployed to the gateway. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/providerconfig</pre> Example Response <pre>\n{\n  \"href\" : \"https://localhost:8443/gateway/admin/api/v1/providerconfig\",\n  \"items\" : [ {\n    \"href\" : \"https://localhost:8443/gateway/admin/api/v1/providerconfig/myproviders\",\n    \"name\" : \"myproviders.xml\"\n  },{\n   \"href\" : \"https://localhost:8443/gateway/admin/api/v1/providerconfig/sandbox-providers\",\n   \"name\" : \"sandbox-providers.xml\"\n  } ]\n}     </pre> providerconfig/{id} GET Get the XML content of the specified shared provider configuration. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/providerconfig/sandbox-providers \\\n     -H Accept:application/xml</pre> Example Response <pre>\n&lt;gateway&gt;\n    &lt;provider&gt;\n        &lt;role&gt;authentication&lt;/role&gt;\n        &lt;name&gt;ShiroProvider&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;sessionTimeout&lt;/name&gt;\n            &lt;value&gt;30&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm&lt;/name&gt;\n            &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n            &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n            &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n            &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n            &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n            &lt;value&gt;simple&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;urls./**&lt;/name&gt;\n            &lt;value&gt;authcBasic&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n\n    &lt;provider&gt;\n        &lt;role&gt;identity-assertion&lt;/role&gt;\n        &lt;name&gt;Default&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;/provider&gt;\n\n    &lt;provider&gt;\n        &lt;role&gt;hostmap&lt;/role&gt;\n        &lt;name&gt;static&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;localhost&lt;/name&gt;\n            &lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n&lt;/gateway&gt;</pre> PUT Add a shared provider configuration. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/providerconfig/sandbox-providers \\\n     -X PUT \\ \n     -H Content-Type:application/xml \\\n     -d \"@sandbox-providers.xml\"</pre> Example Response <pre>HTTP 201 Created</pre> DELETE Delete a shared provider configuration Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/providerconfig/sandbox-providers -X DELETE</pre> Example Response <pre>{ \"deleted\" : \"provider config sandbox-providers\" }</pre> descriptors GET Get an enumeration of the simple descriptors currently deployed to the gateway. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/descriptors -H Accept:application/json</pre> Example Response <pre>\n{\n   \"href\" : \"https://localhost:8443/gateway/admin/api/v1/descriptors\",\n   \"items\" : [ {\n      \"href\" : \"https://localhost:8443/gateway/admin/api/v1/descriptors/docker-sandbox\",\n      \"name\" : \"docker-sandbox.json\"\n   }, {\n      \"href\" : \"https://localhost:8443/gateway/admin/api/v1/descriptors/mytopology\",\n      \"name\" : \"mytopology.yml\"\n   } ]\n}     </pre> descriptors/{id} GET Get the content of the specified descriptor. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/descriptors/docker-sandbox \\\n     -H Accept:application/json</pre> Example Response <pre>\n{\n  \"discovery-type\":\"AMBARI\",\n  \"discovery-address\":\"http://sandbox.hortonworks.com:8080\",\n  \"provider-config-ref\":\"sandbox-providers\",\n  \"cluster\":\"Sandbox\",\n  \"services\":[\n    {\"name\":\"NAMENODE\"},\n    {\"name\":\"JOBTRACKER\"},\n    {\"name\":\"WEBHDFS\"},\n    {\"name\":\"WEBHCAT\"},\n    {\"name\":\"OOZIE\"},\n    {\"name\":\"WEBHBASE\"},\n    {\"name\":\"HIVE\"},\n    {\"name\":\"RESOURCEMANAGER\"} ]\n}    </pre> PUT Add a simple descriptor (and generate and deploy a full topology descriptor). Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/descriptors/docker-sandbox \\\n     -X PUT \\\n     -H Content-Type:application/json \\\n     -d \"@docker-sandbox.json\"</pre> Example Response <pre>HTTP 201 Created</pre> DELETE Delete a simple descriptor (and undeploy the associated topology) Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/descriptors/docker-sandbox -X DELETE</pre> Example Response <pre>{ \"deleted\" : \"descriptor docker-sandbox\" }</pre> aliases/{topology} GET Get the aliases associated with the specified topology. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/aliases/sandbox</pre> Example Response <pre>\n{\n  \"topology\":\"sandbox\",\n  \"aliases\":[\"myalias\",\"encryptquerystring\"]\n}\n      </pre> aliases/{topology}/{alias} PUT Add the specified alias for the specified topology. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/aliases/sandbox/putalias -X PUT \\\n     -H \"Content-Type: application/json\" \\\n     -d \"value=mysecret\"</pre> Example Response <pre>\n{\n  \"created\" : {\n    \"topology\": \"sandbox\",\n    \"alias\": \"putalias\"\n  }\n}</pre> POST Add the specified alias for the specified topology. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/aliases/sandbox/postalias -X POST \\\n     -H \"Content-Type: application/json\" \\\n     -d \"value=mysecret\"</pre> Example Response <pre>\n{\n  \"created\" : {\n    \"topology\": \"sandbox\",\n    \"alias\": \"postalias\"\n  }\n}</pre> DELETE Remove the specified alias for the specified topology. Example Request <pre>curl -iku admin:admin-password {GatewayAPI}/aliases/sandbox/myalias -X DELETE</pre> Example Response <pre>\n{\n  \"deleted\" : {\n    \"topology\": \"sandbox\",\n    \"alias\": \"myalias\"\n  }\n}</pre>"},{"location":"admin_ui/","title":"Admin UI","text":""},{"location":"admin_ui/#admin-ui","title":"Admin UI","text":"<p>The Admin UI is a web application hosted by Knox, which provides the ability to manage provider configurations, descriptors, topologies ans service definitions.</p> <p>As an authoring facility, it eliminates the need for ssh/scp access to the Knox host(s) to effect topology changes. Furthermore, using the Admin UI simplifies the management of topologies in Knox HA deployments by eliminating the need to copy files to multiple Knox hosts.</p>"},{"location":"admin_ui/#admin-ui-url","title":"Admin UI URL","text":"<p>The URL mapping for the Knox Admin UI is:</p> Name URL Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/manager/admin-ui/</code>"},{"location":"admin_ui/#authentication","title":"Authentication","text":"<p>The admin UI is deployed using the manager topology. The out-of-box authentication mechanism is KNOXSSO, backed by the demo LDAP server.  Only someone in the admin role can access the UI functionality.</p>"},{"location":"admin_ui/#basic-navigation","title":"Basic Navigation","text":"<p>Initially, the Admin UI presents the types of resources which can be managed: Provider Configurations, Descriptors, Topologies and Service Definitions.</p> <p></p> <p>Selecting a resource type yields a listing of the existing resources of that type in the adjacent column, and selecting an individual resource presents the details of that selected resource.</p> <p>For the provider configuration, descriptor and service definition resources types, the  icon next to the resource list header is the trigger for the respective facility for creating a new resource of that type. Modification options, including deletion, are available from the detail view for an individual resource.</p>"},{"location":"admin_ui/#provider-configurations","title":"Provider Configurations","text":"<p>The Admin UI lists the provider configurations currently deployed to Knox.</p> <p>By choosing a particular provider configuration from the list, its details can be viewed and edited. The provider configuration can also be deleted (as long as there are no referencing descriptors).</p> <p>By default, there is a provider configuration named default-providers.</p> <p></p>"},{"location":"admin_ui/#editing-provider-configurations","title":"Editing Provider Configurations","text":"<p>For each provider in a given provider configuration, the attributes can be modified:</p> <ul> <li>The provider can be enabled/disabled</li> <li>Parameters can be added () or removed ()</li> <li>Parameter values can be modified (by clicking on the value)   </li> </ul> <p> To persist changes, the  button must be clicked. To revert unsaved changes, click the  button or simply choose another resource. </p>"},{"location":"admin_ui/#create-provider-configurations","title":"Create Provider Configurations","text":"<p>The Admin UI provides the ability to define new provider configurations, which can subsequently be referenced by one or more descriptors.</p> <p>These provider configurations can be created based on the functionality needed, rather than requiring intimate knowledge of the various provider names and their respective parameter names.</p> <p>A provider configuration is a named set of providers. The wizard allows an administrator to specify the name, and add providers to it.</p> <p></p> <p>To add a provider, first a category must be chosen.</p> <p></p> <p>After choosing a category, the type within that category must be selected.</p> <p></p> <p>Finally, for the selected type, the type-specific parameter values can be specified.</p> <p></p> <p>After adding a provider, others can be added similarly by way of the Add Provider button.</p> <p></p>"},{"location":"admin_ui/#composite-provider-types","title":"Composite Provider Types","text":"<p>The wizard for some provider types, such as the HA provider, behave a little differently than the other provider types.</p> <p>For example, when you choose the HA provider category, you subsequently choose a service role (e.g., WEBHDFS), and specify the parameter values for that service role's entry in the HA provider.</p> <p></p> <p></p> <p>If multiple services are configured in this way, the result is still a single HA provider, which contains all of the service role configurations.</p> <p></p>"},{"location":"admin_ui/#persisting-the-new-provider-configuration","title":"Persisting the New Provider Configuration","text":"<p>After adding all the desired providers to the new configuration, choosing  persists it.</p> <p></p>"},{"location":"admin_ui/#descriptors","title":"Descriptors","text":"<p>A descriptor is essentially a named set of service roles to be proxied with a provider configuration reference. The Admin UI lists the descriptors currently deployed to Knox.</p> <p>By choosing a particular descriptor from the list, its details can be viewed and edited. The provider configuration can also be deleted.</p> <p>Modifications to descriptors will result in topology changes. When a descriptor is saved or deleted, the corresponding topology is [re]generated or deleted/undeployed respectively.</p> <p></p> <p></p> <p></p>"},{"location":"admin_ui/#create-descriptors","title":"Create Descriptors","text":"<p>The Admin UI provides the ability to define new descriptors, which result in the generation and deployment of corresponding topologies.</p> <p>The new descriptor dialog provides the ability to specify the name, which will also be the name of the resulting topology. It also allows one or more supported service roles to be selected for inclusion.</p> <p></p> <p>The provider configuration reference can entered manually, or the provider configuration selector can be used, to specify the name of an existing provider configuration.</p> <p></p> <p>Optionally, discovery details can also be specified to direct Knox to discover the endpoints for the declared service roles from a supported discovery source for the target cluster.</p> <p></p> <p>Choosing  results in the persistence of the descriptor, and subsequently, the generation and deployment of the associated topology.</p>"},{"location":"admin_ui/#service-discovery","title":"Service Discovery","text":"<p>Descriptors are a means to declaratively specify which services should be proxied by a particular topology, allowing Knox to interrogate a discovery source to determine the endpoint URLs for those declared services. The Service Discovery options tell Knox how to connect to the desired discovery source to perform this endpoint discovery.</p> <p>Type</p> <p>This property specifies the type of discovery source for the cluster hosting the services whose endpoints are to be discovered.</p> <p>Address</p> <p>This property specifies the address of the discovery source managing the cluster hosting the services whose endpoints are to be discovered.</p> <p>Cluster</p> <p>This property specifies from which of the clusters, among those being managed by the specified discovery source, the service endpoints should be determined.</p> <p>Username</p> <p>This is the identity of the discovery source user (e.g., Ambari Cluster User role), which will be used to get service configuration details from the discovery source.</p> <p>Password Alias</p> <p>This is the Knox alias whose value is the password associated with the specified username.</p> <p>This alias must have been defined prior to specifying it in a descriptor, or else the service discovery will fail for authentication reasons.</p> <p></p>"},{"location":"admin_ui/#topologies","title":"Topologies","text":"<p>The Admin UI allows an administrator to view, modify, duplicate and delete topologies which are currently deployed to the Knox instance. Changes to a topology results in the [re]deployment of that topology, and deleting a topology results in its undeployment.</p> <p></p> <p></p>"},{"location":"admin_ui/#read-only-protections","title":"Read-Only Protections","text":"<p>Topologies which are generated from descriptors are treated as read-only in the Admin UI. This is to avoid the potential confusion resulting from an administrator directly editing a generated topology only to have those changes overwritten by a regeneration of that same topology because the source descriptor or provider configuration changed.</p>"},{"location":"admin_ui/#knox-ha-considerations","title":"Knox HA Considerations","text":"<p>If the Knox instance which is hosting the Admin UI is configured for remote configuration monitoring, then provider configuration and descriptor changes will be persisted in the configured ZooKeeper ensemble. Then, every Knox instance which is also configured to monitor configuration in this same ZooKeeper will apply those changes, and [re]generate/[re]deploy the affected topologies. In this way, Knox HA deployments can be managed by making changes once, and from any of the Knox instances.</p>"},{"location":"admin_ui/#service-definitions","title":"Service Definitions","text":"<p>The Admin UI allows an administrator to view, modify, create and delete service definitions which are currently supported by Knox.</p> <p></p> <p>A service definition is a declarative way of plugging-in a new Service. It consists of two separate files as described in the relevant section in Knox's Developer Guide. The Admin UI lists the service definitions currently supported by Knox in a data table ordered by the service name. If a particular service has more than one service definition (for different versions), a separate service definition entry is displayed in the table. Under the table there is a pagination panel that let's end-users to navigate to the desired service definition.</p> <p>By choosing a particular service definition from the table, its details can be viewed and edited. The service definition can also be deleted.</p> <p></p>"},{"location":"admin_ui/#editing-service-definitions","title":"Editing Service Definitions","text":"<p>When a particular service definition is selected, the Admin UI displays a free-text area where the content can be updated and saved. End-users will see the following structure in this text area:</p> <pre><code>&lt;serviceDefinition&gt;\n    &lt;service&gt;\n    ...\n    &lt;/service&gt;\n    &lt;rules&gt;\n    ...\n    &lt;/rules&gt;\n&lt;/serviceDefinition&gt;\n</code></pre> <p>Everything within the <code>&lt;service&gt;</code> section will be written into the given service's <code>service.xml</code> file whereas the content of <code>rules</code> are going into the <code>rewrite.xml</code>.</p> <p>To persist changes, the  button must be clicked. To revert unsaved changes, simply choose another resource. In case you choose to save your changes, a confirmation window is shown asking for your approval, where you can make your changes final by clicking the &lt;img src=\"static/images/adminui/ok-button.png\"</p> <p> </p> <p>If you are unsure about the change you made, you can still click the <code>Cancel</code> button and select another resource to revert your unsaved change.</p> <p>Important note: editing a service definition will result in redeploying all topologies that include the updated service (identified by it's name and, optionally, version).</p>"},{"location":"admin_ui/#deleting-service-definitions","title":"Deleting Service Definitions","text":"<p>Similarly to the service definition editing function, end-users have to select the service defintion first they are about to remove.</p> <p>The service definition details are displayed along with the  button in the bottom-left corner of the service definition details window. To remove the selected service definition, you have to cick that button and you will be shown a confirmation window where you can verify the service definition removal by clicking the  button.</p> <p> </p> <p>Important note: deleting a service definition will result in redeploying all topologies that included the removed service (identified by it's name and, optionally, version).</p>"},{"location":"admin_ui/#creating-service-definitions","title":"Creating Service Definitions","text":"<p>The Admin UI provides the ability to define new service definitions which can be included in topologies later on.</p> <p>The new service definition dialog provides the ability to specify the service name, role and version as well as all the required information in <code>service.xml</code> and <code>rewrite.xml</code> files such as routes and rewrite rules.</p> <p>To create a new service provider, please click the  button after you selected <code>Service Definitions</code> from the <code>Resource Types</code> list.</p> <p></p> <p>After defining all details, you have to click the  button to save the newly created service definition on the disk.</p> <p> You may want to copy-paste a valid service definition before you open the new service definition dialog and self-tailor the content for your needs.</p> <p></p>"},{"location":"book/","title":"Book","text":""},{"location":"book/#apache-knox-gateway-21x-users-guide","title":"Apache Knox Gateway 2.1.x User's Guide","text":""},{"location":"book/#introduction","title":"Introduction","text":"<p>The Apache Knox Gateway is a system that provides a single point of authentication and access for Apache Hadoop services in a cluster. The goal is to simplify Hadoop security for both users (i.e. who access the cluster data and execute jobs) and operators (i.e. who control access and manage the cluster). The gateway runs as a server (or cluster of servers) that provide centralized access to one or more Hadoop clusters. In general the goals of the gateway are as follows:</p> <ul> <li>Provide perimeter security for Hadoop REST APIs to make Hadoop security easier to setup and use<ul> <li>Provide authentication and token verification at the perimeter</li> <li>Enable authentication integration with enterprise and cloud identity management systems</li> <li>Provide service level authorization at the perimeter</li> </ul> </li> <li>Expose a single URL hierarchy that aggregates REST APIs of a Hadoop cluster<ul> <li>Limit the network endpoints (and therefore firewall holes) required to access a Hadoop cluster</li> <li>Hide the internal Hadoop cluster topology from potential attackers</li> </ul> </li> </ul>"},{"location":"book/#export-controls","title":"Export Controls","text":"<p>Apache Knox Gateway includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See http://www.wassenaar.org for more information.</p> <p>The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.</p> <p>The following provides more details on the included cryptographic software:</p> <ul> <li>Apache Knox Gateway uses the ApacheDS which in turn uses Bouncy Castle generic encryption libraries.</li> <li>See http://www.bouncycastle.org for more details on Bouncy Castle.</li> <li>See http://directory.apache.org/apacheds for more details on ApacheDS.</li> </ul>"},{"location":"book_client-details/","title":"Discovering Resources","text":""},{"location":"book_client-details/#client-details","title":"Client Details","text":"<p>The KnoxShell release artifact provides a small footprint client environment that removes all unnecessary server dependencies, configuration, binary scripts, etc. It is comprised a couple different things that empower different sorts of users.</p> <ul> <li>A set of SDK type classes for providing access to Hadoop resources over HTTP</li> <li>A Groovy based DSL for scripting access to Hadoop resources based on the underlying SDK classes</li> <li>A KnoxShell Token based Sessions to provide a CLI SSO session for executing multiple scripts</li> </ul> <p>The following sections provide an overview and quickstart for the KnoxShell.</p>"},{"location":"book_client-details/#client-quickstart","title":"Client Quickstart","text":"<p>The following installation and setup instructions should get you started with using the KnoxShell very quickly.</p> <ol> <li> <p>Download a knoxshell-x.x.x.zip or tar file and unzip it in your preferred location <code>{GATEWAY_CLIENT_HOME}</code></p> <pre><code>home:knoxshell-0.12.0 larry$ ls -l\ntotal 296\n-rw-r--r--@  1 larry  staff  71714 Mar 14 14:06 LICENSE\n-rw-r--r--@  1 larry  staff    164 Mar 14 14:06 NOTICE\n-rw-r--r--@  1 larry  staff  71714 Mar 15 20:04 README\ndrwxr-xr-x@ 12 larry  staff    408 Mar 15 21:24 bin\ndrwxr--r--@  3 larry  staff    102 Mar 14 14:06 conf\ndrwxr-xr-x+  3 larry  staff    102 Mar 15 12:41 logs\ndrwxr-xr-x@ 18 larry  staff    612 Mar 14 14:18 samples\n</code></pre> Directory Description bin contains the main knoxshell jar and related shell scripts conf only contains log4j config logs contains the knoxshell.log file samples has numerous examples to help you get started </li> <li> <p>cd <code>{GATEWAY_CLIENT_HOME}</code></p> </li> <li>Get/setup truststore for the target Knox instance or fronting load balancer<ul> <li>As of 1.3.0 release you may use the KnoxShell command buildTrustStore to create the truststore. `</li> <li>if you have access to the server you may also use the command <code>knoxcli.sh export-cert --type JKS</code></li> <li>copy the resulting <code>gateway-client-identity.jks</code> to your user home directory</li> </ul> </li> <li>Execute the an example script from the <code>{GATEWAY_CLIENT_HOME}/samples</code> directory - for instance:<ul> <li><code>bin/knoxshell.sh samples/ExampleWebHdfsLs.groovy</code><pre><code>home:knoxshell-0.12.0 larry$ bin/knoxshell.sh samples/ExampleWebHdfsLs.groovy\nEnter username: guest\nEnter password:\n[app-logs, apps, mapred, mr-history, tmp, user]\n</code></pre> </li> </ul> </li> </ol> <p>At this point, you should have seen something similar to the above output - probably with different directories listed. You should get the idea from the above. Take a look at the sample that we ran above:</p> <pre><code>import groovy.json.JsonSlurper\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\n\nimport org.apache.knox.gateway.shell.Credentials\n\ngateway = \"https://localhost:8443/gateway/sandbox\"\n\ncredentials = new Credentials()\ncredentials.add(\"ClearInput\", \"Enter username: \", \"user\")\n                .add(\"HiddenInput\", \"Enter pas\" + \"sword: \", \"pass\")\ncredentials.collect()\n\nusername = credentials.get(\"user\").string()\npass = credentials.get(\"pass\").string()\n\nsession = Hadoop.login( gateway, username, pass )\n\ntext = Hdfs.ls( session ).dir( \"/\" ).now().string\njson = (new JsonSlurper()).parseText( text )\nprintln json.FileStatuses.FileStatus.pathSuffix\nsession.shutdown()\n</code></pre> <p>Some things to note about this sample:</p> <ol> <li>The gateway URL is hardcoded<ul> <li>Alternatives would be passing it as an argument to the script, using an environment variable or prompting for it with a ClearInput credential collector</li> </ul> </li> <li>Credential collectors are used to gather credentials or other input from various sources. In this sample the HiddenInput and ClearInput collectors prompt the user for the input with the provided prompt text and the values are acquired by a subsequent get call with the provided name value.</li> <li>The Hadoop.login method establishes a login session of sorts which will need to be provided to the various API classes as an argument.</li> <li>The response text is easily retrieved as a string and can be parsed by the JsonSlurper or whatever you like</li> </ol>"},{"location":"book_client-details/#build-truststore-for-use-with-knoxshell-client-applications","title":"Build Truststore for use with KnoxShell Client Applications","text":"<p>The buildTrustStore command in KnoxShell allows remote clients that only have access to the KnoxShell install to build a local trustore from the server they intend to use. It should be understood that this mechanism is less secure than getting the cert directly from the Knox CLI - as a MITM could present you with a certificate that will be trusted when doing this remotely.</p> <p>buildTrustStore  - downloads the given gateway server's public certificate and builds a trust store to be used by KnoxShell         example: knoxshell.sh buildTrustStore https://localhost:8443/"},{"location":"book_client-details/#client-token-sessions","title":"Client Token Sessions","text":"<p>Building on the Quickstart above we will drill into some of the token session details here and walk through another sample.</p> <p>Unlike the quickstart, token sessions require the server to be configured in specific ways to allow the use of token sessions/federation.</p>"},{"location":"book_client-details/#server-setup","title":"Server Setup","text":"<ol> <li> <p>KnoxToken service should be added to your <code>sandbox</code> descriptor - see the [KnoxToken Configuration] (#KnoxToken+Configuration)</p> <pre><code>\"services\": [\n  {\n    \"name\": \"KNOXTOKEN\",\n    \"params\": {\n      \"knox.token.ttl\": \"36000000\",\n      \"knox.token.audiences\": \"tokenbased\",\n      \"knox.token.target.url\": \"https://localhost:8443/gateway/tokenbased\"\n    }\n  }\n]\n</code></pre> </li> <li> <p>Include the following in the provider configuration referenced from the <code>tokenbased</code> descriptor to accept tokens as federation tokens for access to exposed resources with the JWTProvider</p> <pre><code>\"providers\": [\n  {\n    \"role\": \"federation\",\n    \"name\": \"JWTProvider\",\n    \"enabled\": \"true\",\n    \"params\": {\n      \"knox.token.audiences\": \"tokenbased\"\n    }\n  }\n]\n</code></pre> </li> <li> <p>Use the KnoxShell token commands to establish and manage your session</p> <ul> <li>bin/knoxshell.sh init https://localhost:8443/gateway/sandbox to acquire a token and cache in user home directory</li> <li>bin/knoxshell.sh list to display the details of the cached token, the expiration time and optionally the target url</li> <li>bin/knoxshell destroy to remove the cached session token and terminate the session</li> </ul> </li> <li> <p>Execute a script that can take advantage of the token credential collector and target URL</p> <pre><code>import groovy.json.JsonSlurper\nimport java.util.HashMap\nimport java.util.Map\nimport org.apache.knox.gateway.shell.Credentials\nimport org.apache.knox.gateway.shell.KnoxSession\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\n\ncredentials = new Credentials()\ncredentials.add(\"KnoxToken\", \"none: \", \"token\")\ncredentials.collect()\n\ntoken = credentials.get(\"token\").string()\n\ngateway = System.getenv(\"KNOXSHELL_TOPOLOGY_URL\")\nif (gateway == null || gateway.equals(\"\")) {\n  gateway = credentials.get(\"token\").getTargetUrl()\n}\n\nprintln \"\"\nprintln \"*****************************GATEWAY INSTANCE**********************************\"\nprintln gateway\nprintln \"*******************************************************************************\"\nprintln \"\"\n\nheaders = new HashMap()\nheaders.put(\"Authorization\", \"Bearer \" + token)\n\nsession = KnoxSession.login( gateway, headers )\n\nif (args.length &gt; 0) {\n  dir = args[0]\n} else {\n  dir = \"/\"\n}\n\ntext = Hdfs.ls( session ).dir( dir ).now().string\njson = (new JsonSlurper()).parseText( text )\nstatuses = json.get(\"FileStatuses\");\n\nprintln statuses\n\nsession.shutdown()\n</code></pre> </li> </ol> <p>Note the following about the above sample script:</p> <ol> <li>Use of the KnoxToken credential collector</li> <li>Use of the targetUrl from the credential collector</li> <li>Optional override of the target url with environment variable</li> <li>The passing of the headers map to the session creation in Hadoop.login</li> <li>The passing of an argument for the ls command for the path to list or default to \"/\"</li> </ol> <p>Also note that there is no reason to prompt for username and password as long as the token has not been destroyed or expired. There is also no hardcoded endpoint for using the token - it is specified in the token cache or overridden by environment variable.</p>"},{"location":"book_client-details/#client-dsl-and-sdk-details","title":"Client DSL and SDK Details","text":"<p>The lack of any formal SDK or client for REST APIs in Hadoop led to thinking about a very simple client that could help people use and evaluate the gateway. The list below outlines the general requirements for such a client.</p> <ul> <li>Promote the evaluation and adoption of the Apache Knox Gateway</li> <li>Simple to deploy and use on data worker desktops for access to remote Hadoop clusters</li> <li>Simple to extend with new commands both by other Hadoop projects and by the end user</li> <li>Support the notion of a SSO session for multiple Hadoop interactions</li> <li>Support the multiple authentication and federation token capabilities of the Apache Knox Gateway</li> <li>Promote the use of REST APIs as the dominant remote client mechanism for Hadoop services</li> <li>Promote the sense of Hadoop as a single unified product</li> <li>Aligned with the Apache Knox Gateway's overall goals for security</li> </ul> <p>The result is a very simple DSL (Domain Specific Language) of sorts that is used via Groovy scripts. Here is an example of a command that copies a file from the local file system to HDFS.</p> <p>Note: The variables <code>session</code>, <code>localFile</code> and <code>remoteFile</code> are assumed to be defined.</p> <pre><code>Hdfs.put(session).file(localFile).to(remoteFile).now()\n</code></pre> <p>This work is in very early development but is already very useful in its current state. We are very interested in receiving feedback about how to improve this feature and the DSL in particular.</p> <p>A note of thanks to REST-assured which provides a Fluent interface style DSL for testing REST services. It served as the initial inspiration for the creation of this DSL.</p>"},{"location":"book_client-details/#assumptions","title":"Assumptions","text":"<p>This document assumes a few things about your environment in order to simplify the examples.</p> <ul> <li>The JVM is executable as simply <code>java</code>.</li> <li>The Apache Knox Gateway is installed and functional.</li> <li>The example commands are executed within the context of the <code>GATEWAY_HOME</code> current directory. The <code>GATEWAY_HOME</code> directory is the directory within the Apache Knox Gateway installation that contains the README file and the bin, conf and deployments directories.</li> <li>A few examples require the use of commands from a standard Groovy installation.  These examples are optional but to try them you will need Groovy installed.</li> </ul>"},{"location":"book_client-details/#basics","title":"Basics","text":"<p>In order for secure connections to be made to the Knox gateway server over SSL, the user will need to trust the certificate presented by the gateway while connecting. The knoxcli command export-cert may be used to get access the gateway-identity cert. It can then be imported into cacerts on the client machine or put into a keystore that will be discovered in:</p> <ul> <li>The user's home directory</li> <li>In a directory specified in an environment variable: <code>KNOX_CLIENT_TRUSTSTORE_DIR</code></li> <li>In a directory specified with the above variable with the keystore filename specified in the variable: <code>KNOX_CLIENT_TRUSTSTORE_FILENAME</code></li> <li>Default password \"changeit\" or password may be specified in environment variable: <code>KNOX_CLIENT_TRUSTSTORE_PASS</code></li> <li>Or the JSSE system property <code>javax.net.ssl.trustStore</code> can be used to specify its location</li> </ul> <p>The DSL requires a shell to interpret the Groovy script. The shell can either be used interactively or to execute a script file. To simplify use, the distribution contains an embedded version of the Groovy shell.</p> <p>The shell can be run interactively. Use the command <code>exit</code> to exit.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>When running interactively it may be helpful to reduce some of the output generated by the shell console. Use the following command in the interactive shell to reduce that output. This only needs to be done once as these preferences are persisted.</p> <pre><code>set verbosity QUIET\nset show-last-result false\n</code></pre> <p>Also when running interactively use the <code>exit</code> command to terminate the shell. Using <code>^C</code> to exit can sometimes leaves the parent shell in a problematic state.</p> <p>The shell can also be used to execute a script by passing a single filename argument.</p> <pre><code>java -jar bin/shell.jar samples/ExampleWebHdfsPutGet.groovy\n</code></pre>"},{"location":"book_client-details/#examples","title":"Examples","text":"<p>Once the shell can be launched the DSL can be used to interact with the gateway and Hadoop. Below is a very simple example of an interactive shell session to upload a file to HDFS.</p> <pre><code>java -jar bin/shell.jar\nknox:000&gt; session = Hadoop.login( \"https://localhost:8443/gateway/sandbox\", \"guest\", \"guest-password\" )\nknox:000&gt; Hdfs.put( session ).file( \"README\" ).to( \"/tmp/example/README\" ).now()\n</code></pre> <p>The <code>knox:000&gt;</code> in the example above is the prompt from the embedded Groovy console. If you output doesn't look like this you may need to set the verbosity and show-last-result preferences as described above in the Usage section.</p> <p>If you receive an error <code>HTTP/1.1 403 Forbidden</code> it may be because that file already exists. Try deleting it with the following command and then try again.</p> <pre><code>knox:000&gt; Hdfs.rm(session).file(\"/tmp/example/README\").now()\n</code></pre> <p>Without using some other tool to browse HDFS it is hard to tell that this command did anything. Execute this to get a bit more feedback.</p> <pre><code>knox:000&gt; println \"Status=\" + Hdfs.put( session ).file( \"README\" ).to( \"/tmp/example/README2\" ).now().statusCode\nStatus=201\n</code></pre> <p>Notice that a different filename is used for the destination. Without this an error would have resulted. Of course the DSL also provides a command to list the contents of a directory.</p> <pre><code>knox:000&gt; println Hdfs.ls( session ).dir( \"/tmp/example\" ).now().string\n{\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":1363711366977,\"blockSize\":134217728,\"group\":\"hdfs\",\"length\":19395,\"modificationTime\":1363711366977,\"owner\":\"guest\",\"pathSuffix\":\"README\",\"permission\":\"644\",\"replication\":1,\"type\":\"FILE\"},{\"accessTime\":1363711375617,\"blockSize\":134217728,\"group\":\"hdfs\",\"length\":19395,\"modificationTime\":1363711375617,\"owner\":\"guest\",\"pathSuffix\":\"README2\",\"permission\":\"644\",\"replication\":1,\"type\":\"FILE\"}]}}\n</code></pre> <p>It is a design decision of the DSL to not provide type safe classes for various request and response payloads. Doing so would provide an undesirable coupling between the DSL and the service implementation. It also would make adding new commands much more difficult. See the Groovy section below for a variety capabilities and tools for working with JSON and XML to make this easy. The example below shows the use of JsonSlurper and GPath to extract content from a JSON response.</p> <pre><code>knox:000&gt; import groovy.json.JsonSlurper\nknox:000&gt; text = Hdfs.ls( session ).dir( \"/tmp/example\" ).now().string\nknox:000&gt; json = (new JsonSlurper()).parseText( text )\nknox:000&gt; println json.FileStatuses.FileStatus.pathSuffix\n[README, README2]\n</code></pre> <p>In the future, \"built-in\" methods to slurp JSON and XML may be added to make this a bit easier. This would allow for the following type of single line interaction:</p> <pre><code>println Hdfs.ls(session).dir(\"/tmp\").now().json().FileStatuses.FileStatus.pathSuffix\n</code></pre> <p>Shell sessions should always be ended with shutting down the session. The examples above do not touch on it but the DSL supports the simple execution of commands asynchronously. The shutdown command attempts to ensures that all asynchronous commands have completed before existing the shell.</p> <pre><code>knox:000&gt; session.shutdown()\nknox:000&gt; exit\n</code></pre> <p>All of the commands above could have been combined into a script file and executed as a single line.</p> <pre><code>java -jar bin/shell.jar samples/ExampleWebHdfsPutGet.groovy\n</code></pre> <p>This would be the content of that script.</p> <pre><code>import org.apache.knox.gateway.shell.Hadoop\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\nimport groovy.json.JsonSlurper\n\ngateway = \"https://localhost:8443/gateway/sandbox\"\nusername = \"guest\"\npassword = \"guest-password\"\ndataFile = \"README\"\n\nsession = Hadoop.login( gateway, username, password )\nHdfs.rm( session ).file( \"/tmp/example\" ).recursive().now()\nHdfs.put( session ).file( dataFile ).to( \"/tmp/example/README\" ).now()\ntext = Hdfs.ls( session ).dir( \"/tmp/example\" ).now().string\njson = (new JsonSlurper()).parseText( text )\nprintln json.FileStatuses.FileStatus.pathSuffix\nsession.shutdown()\nexit\n</code></pre> <p>Notice the <code>Hdfs.rm</code> command.  This is included simply to ensure that the script can be rerun. Without this an error would result the second time it is run.</p>"},{"location":"book_client-details/#futures","title":"Futures","text":"<p>The DSL supports the ability to invoke commands asynchronously via the later() invocation method. The object returned from the <code>later()</code> method is a <code>java.util.concurrent.Future</code> parameterized with the response type of the command. This is an example of how to asynchronously put a file to HDFS.</p> <pre><code>future = Hdfs.put(session).file(\"README\").to(\"/tmp/example/README\").later()\nprintln future.get().statusCode\n</code></pre> <p>The <code>future.get()</code> method will block until the asynchronous command is complete. To illustrate the usefulness of this however multiple concurrent commands are required.</p> <pre><code>readmeFuture = Hdfs.put(session).file(\"README\").to(\"/tmp/example/README\").later()\nlicenseFuture = Hdfs.put(session).file(\"LICENSE\").to(\"/tmp/example/LICENSE\").later()\nsession.waitFor( readmeFuture, licenseFuture )\nprintln readmeFuture.get().statusCode\nprintln licenseFuture.get().statusCode\n</code></pre> <p>The <code>session.waitFor()</code> method will wait for one or more asynchronous commands to complete.</p>"},{"location":"book_client-details/#closures","title":"Closures","text":"<p>Futures alone only provide asynchronous invocation of the command. What if some processing should also occur asynchronously once the command is complete. Support for this is provided by closures. Closures are blocks of code that are passed into the <code>later()</code> invocation method. In Groovy these are contained within <code>{}</code> immediately after a method. These blocks of code are executed once the asynchronous command is complete.</p> <pre><code>Hdfs.put(session).file(\"README\").to(\"/tmp/example/README\").later(){ println it.statusCode }\n</code></pre> <p>In this example the <code>put()</code> command is executed on a separate thread and once complete the <code>println it.statusCode</code> block is executed on that thread. The <code>it</code> variable is automatically populated by Groovy and is a reference to the result that is returned from the future or <code>now()</code> method. The future example above can be rewritten to illustrate the use of closures.</p> <pre><code>readmeFuture = Hdfs.put(session).file(\"README\").to(\"/tmp/example/README\").later() { println it.statusCode }\nlicenseFuture = Hdfs.put(session).file(\"LICENSE\").to(\"/tmp/example/LICENSE\").later() { println it.statusCode }\nsession.waitFor( readmeFuture, licenseFuture )\n</code></pre> <p>Again, the <code>session.waitFor()</code> method will wait for one or more asynchronous commands to complete.</p>"},{"location":"book_client-details/#constructs","title":"Constructs","text":"<p>In order to understand the DSL there are three primary constructs that need to be understood.</p>"},{"location":"book_client-details/#session","title":"Session","text":"<p>This construct encapsulates the client side session state that will be shared between all command invocations. In particular it will simplify the management of any tokens that need to be presented with each command invocation. It also manages a thread pool that is used by all asynchronous commands which is why it is important to call one of the shutdown methods.</p> <p>The syntax associated with this is expected to change. We expect that credentials will not need to be provided to the gateway. Rather it is expected that some form of access token will be used to initialize the session.</p>"},{"location":"book_client-details/#clientcontext","title":"ClientContext","text":"<p>The ClientContext encapsulates the connection parameters, such as the URL, socket timeout parameters, retry configuration and connection pool parameters.</p> <pre><code>ClientContext context = ClientContext.with(\"http://localhost:8443\");\ncontext.connection().retryCount(2).requestSentRetryEnabled(false).retryIntervalMillis(1000).end();\nKnoxSession session = KnoxSession.login(context);\n</code></pre> <ul> <li>retryCount - how many times to retry; -1 means no retries</li> <li>requestSentRetryEnabled - true if it's OK to retry requests that have been sent</li> <li>retryIntervalMillis - The interval between the subsequent auto-retries when the service is unavailable</li> </ul>"},{"location":"book_client-details/#services","title":"Services","text":"<p>Services are the primary extension point for adding new suites of commands. The current built-in examples are: Hdfs, Job and Workflow. The desire for extensibility is the reason for the slightly awkward <code>Hdfs.ls(session)</code> syntax. Certainly something more like <code>session.hdfs().ls()</code> would have been preferred but this would prevent adding new commands easily. At a minimum it would result in extension commands with a different syntax from the \"built-in\" commands.</p> <p>The service objects essentially function as a factory for a suite of commands.</p>"},{"location":"book_client-details/#commands","title":"Commands","text":"<p>Commands provide the behavior of the DSL. They typically follow a Fluent interface style in order to allow for single line commands. There are really three parts to each command: Request, Invocation, Response</p>"},{"location":"book_client-details/#request","title":"Request","text":"<p>The request is populated by all of the methods following the \"verb\" method and the \"invoke\" method. For example in <code>Hdfs.rm(session).ls(dir).now()</code> the request is populated between the \"verb\" method <code>rm()</code> and the \"invoke\" method <code>now()</code>.</p>"},{"location":"book_client-details/#invocation","title":"Invocation","text":"<p>The invocation method controls how the request is invoked. Currently supported synchronous and asynchronous invocation. The <code>now()</code> method executes the request and returns the result immediately. The <code>later()</code> method submits the request to be executed later and returns a future from which the result can be retrieved. In addition <code>later()</code> invocation method can optionally be provided a closure to execute when the request is complete. See the Futures and Closures sections below for additional detail and examples.</p>"},{"location":"book_client-details/#response","title":"Response","text":"<p>The response contains the results of the invocation of the request. In most cases the response is a thin wrapper over the HTTP response. In fact many commands will share a single BasicResponse type that only provides a few simple methods.</p> <pre><code>public int getStatusCode()\npublic long getContentLength()\npublic String getContentType()\npublic String getContentEncoding()\npublic InputStream getStream()\npublic String getString()\npublic byte[] getBytes()\npublic void close();\n</code></pre> <p>Thanks to Groovy these methods can be accessed as attributes. In the some of the examples the staticCode was retrieved for example.</p> <pre><code>println Hdfs.put(session).rm(dir).now().statusCode\n</code></pre> <p>Groovy will invoke the getStatusCode method to retrieve the statusCode attribute.</p> <p>The three methods <code>getStream()</code>, <code>getBytes()</code> and <code>getString()</code> deserve special attention. Care must be taken that the HTTP body is fully read once and only once. Therefore one of these methods (and only one) must be called once and only once. Calling one of these more than once will cause an error. Failing to call one of these methods once will result in lingering open HTTP connections. The <code>close()</code> method may be used if the caller is not interested in reading the result body. Most commands that do not expect a response body will call close implicitly. If the body is retrieved via <code>getBytes()</code> or <code>getString()</code>, the <code>close()</code> method need not be called. When using <code>getStream()</code>, care must be taken to consume the entire body otherwise lingering open HTTP connections will result. The <code>close()</code> method may be called after reading the body partially to discard the remainder of the body.</p>"},{"location":"book_client-details/#services_1","title":"Services","text":"<p>The built-in supported client DSL for each Hadoop service can be found in the #[Service Details] section.</p>"},{"location":"book_client-details/#extension","title":"Extension","text":"<p>Extensibility is a key design goal of the KnoxShell and client DSL. There are two ways to provide extended functionality for use with the shell. The first is to simply create Groovy scripts that use the DSL to perform a useful task. The second is to add new services and commands. In order to add new service and commands new classes must be written in either Groovy or Java and added to the classpath of the shell. Fortunately there is a very simple way to add classes and JARs to the shell classpath. The first time the shell is executed it will create a configuration file in the same directory as the JAR with the same base name and a <code>.cfg</code> extension.</p> <pre><code>bin/shell.jar\nbin/shell.cfg\n</code></pre> <p>That file contains both the main class for the shell as well as a definition of the classpath. Currently that file will by default contain the following.</p> <pre><code>main.class=org.apache.knox.gateway.shell.Shell\nclass.path=../lib; ../lib/*.jar; ../ext; ../ext/*.jar\n</code></pre> <p>Therefore to extend the shell you should copy any new service and command class either to the <code>ext</code> directory or if they are packaged within a JAR copy the JAR to the <code>ext</code> directory. The <code>lib</code> directory is reserved for JARs that may be delivered with the product.</p> <p>Below are samples for the service and command classes that would need to be written to add new commands to the shell. These happen to be Groovy source files but could - with very minor changes - be Java files. The easiest way to add these to the shell is to compile them directly into the <code>ext</code> directory. Note: This command depends upon having the Groovy compiler installed and available on the execution path.</p> <pre><code>groovy -d ext -cp bin/shell.jar samples/SampleService.groovy \\\n    samples/SampleSimpleCommand.groovy samples/SampleComplexCommand.groovy\n</code></pre> <p>These source files are available in the samples directory of the distribution but are included here for convenience.</p>"},{"location":"book_client-details/#sample-service-groovy","title":"Sample Service (Groovy)","text":"<pre><code>import org.apache.knox.gateway.shell.Hadoop\n\nclass SampleService {\n\n    static String PATH = \"/webhdfs/v1\"\n\n    static SimpleCommand simple( Hadoop session ) {\n        return new SimpleCommand( session )\n    }\n\n    static ComplexCommand.Request complex( Hadoop session ) {\n        return new ComplexCommand.Request( session )\n    }\n\n}\n</code></pre>"},{"location":"book_client-details/#sample-simple-command-groovy","title":"Sample Simple Command (Groovy)","text":"<pre><code>import org.apache.knox.gateway.shell.AbstractRequest\nimport org.apache.knox.gateway.shell.BasicResponse\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.http.client.methods.HttpGet\nimport org.apache.http.client.utils.URIBuilder\n\nimport java.util.concurrent.Callable\n\nclass SimpleCommand extends AbstractRequest&lt;BasicResponse&gt; {\n\n    SimpleCommand( Hadoop session ) {\n        super( session )\n    }\n\n    private String param\n    SimpleCommand param( String param ) {\n        this.param = param\n        return this\n    }\n\n    @Override\n    protected Callable&lt;BasicResponse&gt; callable() {\n        return new Callable&lt;BasicResponse&gt;() {\n            @Override\n            BasicResponse call() {\n                URIBuilder uri = uri( SampleService.PATH, param )\n                addQueryParam( uri, \"op\", \"LISTSTATUS\" )\n                HttpGet get = new HttpGet( uri.build() )\n                return new BasicResponse( execute( get ) )\n            }\n        }\n    }\n\n}\n</code></pre>"},{"location":"book_client-details/#sample-complex-command-groovy","title":"Sample Complex Command (Groovy)","text":"<pre><code>import com.jayway.jsonpath.JsonPath\nimport org.apache.knox.gateway.shell.AbstractRequest\nimport org.apache.knox.gateway.shell.BasicResponse\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.http.HttpResponse\nimport org.apache.http.client.methods.HttpGet\nimport org.apache.http.client.utils.URIBuilder\n\nimport java.util.concurrent.Callable\n\nclass ComplexCommand {\n\n    static class Request extends AbstractRequest&lt;Response&gt; {\n\n        Request( Hadoop session ) {\n            super( session )\n        }\n\n        private String param;\n        Request param( String param ) {\n            this.param = param;\n            return this;\n        }\n\n        @Override\n        protected Callable&lt;Response&gt; callable() {\n            return new Callable&lt;Response&gt;() {\n                @Override\n                Response call() {\n                    URIBuilder uri = uri( SampleService.PATH, param )\n                    addQueryParam( uri, \"op\", \"LISTSTATUS\" )\n                    HttpGet get = new HttpGet( uri.build() )\n                    return new Response( execute( get ) )\n                }\n            }\n        }\n\n    }\n\n    static class Response extends BasicResponse {\n\n        Response(HttpResponse response) {\n            super(response)\n        }\n\n        public List&lt;String&gt; getNames() {\n            return JsonPath.read( string, \"\\$.FileStatuses.FileStatus[*].pathSuffix\" )\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"book_client-details/#groovy","title":"Groovy","text":"<p>The shell included in the distribution is basically an unmodified packaging of the Groovy shell. The distribution does however provide a wrapper that makes it very easy to setup the class path for the shell. In fact the JARs required to execute the DSL are included on the class path by default. Therefore these command are functionally equivalent if you have Groovy installed. See below for a description of what is required for JARs required by the DSL from <code>lib</code> and <code>dep</code> directories.</p> <pre><code>java -jar bin/shell.jar samples/ExampleWebHdfsPutGet.groovy\ngroovy -classpath {JARs required by the DSL from lib and dep} samples/ExampleWebHdfsPutGet.groovy\n</code></pre> <p>The interactive shell isn't exactly equivalent. However the only difference is that the shell.jar automatically executes some additional imports that are useful for the KnoxShell client DSL. So these two sets of commands should be functionality equivalent. However there is currently a class loading issue that prevents the groovysh command from working properly.</p> <pre><code>java -jar bin/shell.jar\n\ngroovysh -classpath {JARs required by the DSL from lib and dep}\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\nimport org.apache.knox.gateway.shell.job.Job\nimport org.apache.knox.gateway.shell.workflow.Workflow\nimport java.util.concurrent.TimeUnit\n</code></pre> <p>Alternatively, you can use the Groovy Console which does not appear to have the same class loading issue.</p> <pre><code>groovyConsole -classpath {JARs required by the DSL from lib and dep}\n\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\nimport org.apache.knox.gateway.shell.job.Job\nimport org.apache.knox.gateway.shell.workflow.Workflow\nimport java.util.concurrent.TimeUnit\n</code></pre> <p>The JARs currently required by the client DSL are</p> <pre><code>lib/gateway-shell-{GATEWAY_VERSION}.jar\ndep/httpclient-4.3.6.jar\ndep/httpcore-4.3.3.jar\ndep/commons-lang3-3.4.jar\ndep/commons-codec-1.7.jar\n</code></pre> <p>So on Linux/MacOS you would need this command</p> <pre><code>groovy -cp lib/gateway-shell-0.10.0.jar:dep/httpclient-4.3.6.jar:dep/httpcore-4.3.3.jar:dep/commons-lang3-3.4.jar:dep/commons-codec-1.7.jar samples/ExampleWebHdfsPutGet.groovy\n</code></pre> <p>and on Windows you would need this command</p> <pre><code>groovy -cp lib/gateway-shell-0.10.0.jar;dep/httpclient-4.3.6.jar;dep/httpcore-4.3.3.jar;dep/commons-lang3-3.4.jar;dep/commons-codec-1.7.jar samples/ExampleWebHdfsPutGet.groovy\n</code></pre> <p>The exact list of required JARs is likely to change from release to release so it is recommended that you utilize the wrapper <code>bin/shell.jar</code>.</p> <p>In addition because the DSL can be used via standard Groovy, the Groovy integrations in many popular IDEs (e.g. IntelliJ, Eclipse) can also be used. This makes it particularly nice to develop and execute scripts to interact with Hadoop. The code-completion features in modern IDEs in particular provides immense value. All that is required is to add the <code>gateway-shell-{GATEWAY_VERSION}.jar</code> to the projects class path.</p> <p>There are a variety of Groovy tools that make it very easy to work with the standard interchange formats (i.e. JSON and XML). In Groovy the creation of XML or JSON is typically done via a \"builder\" and parsing done via a \"slurper\". In addition once JSON or XML is \"slurped\" the GPath, an XPath like feature build into Groovy can be used to access data.</p> <ul> <li>XML<ul> <li>Markup Builder Overview, API</li> <li>XML Slurper Overview, API</li> <li>XPath Overview, API</li> </ul> </li> <li>JSON<ul> <li>JSON Builder API</li> <li>JSON Slurper API</li> <li>JSON Path API</li> <li>GPath Overview</li> </ul> </li> </ul>"},{"location":"book_gateway-details/","title":"Gateway Overview","text":""},{"location":"book_gateway-details/#gateway-details","title":"Gateway Details","text":"<p>This section describes the details of the Knox Gateway itself. Including:</p> <ul> <li>How URLs are mapped between a gateway that services multiple Hadoop clusters and the clusters themselves</li> <li>How the gateway is configured through <code>gateway-site.xml</code> and cluster specific topology files</li> <li>How to configure the various policy enforcement provider features such as authentication, authorization, auditing, hostmapping, etc.</li> </ul>"},{"location":"book_gateway-details/#url-mapping","title":"URL Mapping","text":"<p>The gateway functions much like a reverse proxy. As such, it maintains a mapping of URLs that are exposed externally by the gateway to URLs that are provided by the Hadoop cluster.</p>"},{"location":"book_gateway-details/#default-topology-urls","title":"Default Topology URLs","text":"<p>In order to provide compatibility with the Hadoop Java client and existing CLI tools, the Knox Gateway has provided a feature called the Default Topology. This refers to a topology deployment that will be able to route URLs without the additional context that the gateway uses for differentiating from one Hadoop cluster to another. This allows the URLs to match those used by existing clients that may access WebHDFS through the Hadoop file system abstraction.</p> <p>When a topology file is deployed with a file name that matches the configured default topology name, a specialized mapping for URLs is installed for that particular topology. This allows the URLs that are expected by the existing Hadoop CLIs for WebHDFS to be used in interacting with the specific Hadoop cluster that is represented by the default topology file.</p> <p>The configuration for the default topology name is found in <code>gateway-site.xml</code> as a property called: <code>default.app.topology.name</code>.</p> <p>The default value for this property is empty.</p> <p>When deploying the <code>sandbox.xml</code> topology and setting <code>default.app.topology.name</code> to <code>sandbox</code>, both of the following example URLs work for the same underlying Hadoop cluster:</p> <pre><code>https://{gateway-host}:{gateway-port}/webhdfs\nhttps://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/webhdfs\n</code></pre> <p>These default topology URLs exist for all of the services in the topology.</p>"},{"location":"book_gateway-details/#fully-qualified-urls","title":"Fully Qualified URLs","text":"<p>Examples of mappings for WebHDFS, WebHCat, Oozie and HBase are shown below. These mapping are generated from the combination of the gateway configuration file (i.e. <code>{GATEWAY_HOME}/conf/gateway-site.xml</code>) and the cluster topology descriptors (e.g. <code>{GATEWAY_HOME}/conf/topologies/{cluster-name}.xml</code>). The port numbers shown for the Cluster URLs represent the default ports for these services. The actual port number may be different for a given cluster.</p> <ul> <li>WebHDFS<ul> <li>Gateway: <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/webhdfs</code></li> <li>Cluster: <code>http://{webhdfs-host}:50070/webhdfs</code></li> </ul> </li> <li>WebHCat (Templeton)<ul> <li>Gateway: <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/templeton</code></li> <li>Cluster: <code>http://{webhcat-host}:50111/templeton}</code></li> </ul> </li> <li>Oozie<ul> <li>Gateway: <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/oozie</code></li> <li>Cluster: <code>http://{oozie-host}:11000/oozie}</code></li> </ul> </li> <li>HBase<ul> <li>Gateway: <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/hbase</code></li> <li>Cluster: <code>http://{hbase-host}:8080</code></li> </ul> </li> <li>Hive JDBC<ul> <li>Gateway: <code>jdbc:hive2://{gateway-host}:{gateway-port}/;ssl=true;sslTrustStore={gateway-trust-store-path};trustStorePassword={gateway-trust-store-password};transportMode=http;httpPath={gateway-path}/{cluster-name}/hive</code></li> <li>Cluster: <code>http://{hive-host}:10001/cliservice</code></li> </ul> </li> </ul> <p>The values for <code>{gateway-host}</code>, <code>{gateway-port}</code>, <code>{gateway-path}</code> are provided via the gateway configuration file (i.e. <code>{GATEWAY_HOME}/conf/gateway-site.xml</code>).</p> <p>The value for <code>{cluster-name}</code> is derived from the file name of the cluster topology descriptor (e.g. <code>{GATEWAY_HOME}/deployments/{cluster-name}.xml</code>).</p> <p>The value for <code>{webhdfs-host}</code>, <code>{webhcat-host}</code>, <code>{oozie-host}</code>, <code>{hbase-host}</code> and <code>{hive-host}</code> are provided via the cluster topology descriptor (e.g. <code>{GATEWAY_HOME}/conf/topologies/{cluster-name}.xml</code>).</p> <p>Note: The ports 50070 (9870 for Hadoop 3.x), 50111, 11000, 8080 and 10001 are the defaults for WebHDFS, WebHCat, Oozie, HBase and Hive respectively. Their values can also be provided via the cluster topology descriptor if your Hadoop cluster uses different ports.</p> <p>Note: The HBase REST API uses port 8080 by default. This often clashes with other running services. In the Hortonworks Sandbox, Apache Ambari might be running on this port, so you might have to change it to a different port (e.g. 60080).</p>"},{"location":"book_getting-started/","title":"Book getting started","text":""},{"location":"book_getting-started/#apache-knox-details","title":"Apache Knox Details","text":"<p>This section provides everything you need to know to get the Knox gateway up and running against a Hadoop cluster.</p>"},{"location":"book_getting-started/#hadoop","title":"Hadoop","text":"<p>An existing Hadoop 2.x or 3.x cluster is required for Knox to sit in front of and protect. It is possible to use a Hadoop cluster deployed on EC2 but this will require additional configuration not covered here. It is also possible to protect access to a services of a Hadoop cluster that is secured with Kerberos. This too requires additional configuration that is described in other sections of this guide. See #[Supported Services] for details on what is supported for this release.</p> <p>The instructions that follow assume a few things:</p> <ol> <li>The gateway is not collocated with the Hadoop clusters themselves.</li> <li>The host names and IP addresses of the cluster services are accessible by the gateway where ever it happens to be running.</li> </ol> <p>All of the instructions and samples provided here are tailored and tested to work \"out of the box\" against a [Hortonworks Sandbox 2.x VM][sandbox].</p>"},{"location":"book_getting-started/#apache-knox-directory-layout","title":"Apache Knox Directory Layout","text":"<p>Knox can be installed by expanding the zip/archive file.</p> <p>The table below provides a brief explanation of the important files and directories within <code>{GATEWAY_HOME}</code></p> Directory Purpose conf/ Contains configuration files that apply to the gateway globally (i.e. not cluster specific ). data/ Contains security and topology specific artifacts that require read/write access at runtime conf/topologies/ Contains topology files that represent Hadoop clusters which the gateway uses to deploy cluster proxies data/security/ Contains the persisted master secret and keystore dir data/security/keystores/ Contains the gateway identity keystore and credential stores for the gateway and each deployed cluster topology data/services Contains service behavior definitions for the services currently supported. bin/ Contains the executable shell scripts, batch files and JARs for clients and servers. data/deployments/ Contains deployed cluster topologies used to protect access to specific Hadoop clusters. lib/ Contains the JARs for all the components that make up the gateway. dep/ Contains the JARs for all of the components upon which the gateway depends. ext/ A directory where user supplied extension JARs can be placed to extends the gateways functionality. pids/ Contains the process ids for running LDAP and gateway servers samples/ Contains a number of samples that can be used to explore the functionality of the gateway. templates/ Contains default configuration files that can be copied and customized. README Provides basic information about the Apache Knox Gateway. ISSUES Describes significant know issues. CHANGES Enumerates the changes between releases. LICENSE Documents the license under which this software is provided. NOTICE Documents required attribution notices for included dependencies."},{"location":"book_getting-started/#supported-services","title":"Supported Services","text":"<p>This table enumerates the versions of various Hadoop services that have been tested to work with the Knox Gateway.</p> Service Version Non-Secure Secure HA WebHDFS 2.4.0 ![y] ![y] ![y] WebHCat/Templeton 0.13.0 ![y] ![y] ![y] Oozie 4.0.0 ![y] ![y] ![y] HBase 0.98.0 ![y] ![y] ![y] Hive (via WebHCat) 0.13.0 ![y] ![y] ![y] Hive (via JDBC/ODBC) 0.13.0 ![y] ![y] ![y] Yarn ResourceManager 2.5.0 ![y] ![y] ![n] Kafka (via REST Proxy) 0.10.0 ![y] ![y] ![y] Storm 0.9.3 ![y] ![n] ![n] Solr 5.5+ and 6+ ![y] ![y] ![y]"},{"location":"book_getting-started/#more-examples","title":"More Examples","text":"<p>These examples provide more detail about how to access various Apache Hadoop services via the Apache Knox Gateway.</p> <ul> <li>WebHDFS Examples</li> <li>WebHCat Examples</li> <li>Oozie Examples</li> <li>HBase Examples</li> <li>Hive Examples</li> <li>Yarn Examples</li> <li>Storm Examples</li> </ul>"},{"location":"book_knox-samples/","title":"Book knox samples","text":""},{"location":"book_knox-samples/#gateway-samples","title":"Gateway Samples","text":"<p>The purpose of the samples within the <code>{GATEWAY_HOME}/samples</code> directory is to demonstrate the capabilities of the Apache Knox Gateway to provide access to the numerous APIs that are available from the service components of a Hadoop cluster.</p> <p>Depending on exactly how your Knox installation was done, there will be some number of steps required in order fully install and configure the samples for use.</p> <p>This section will help describe the assumptions of the samples and the steps to get them to work in a couple of different deployment scenarios.</p>"},{"location":"book_knox-samples/#assumptions-of-the-samples","title":"Assumptions of the Samples","text":"<p>The samples were initially written with the intent of working out of the box for the various Hadoop demo environments that are deployed as a single node cluster inside of a VM. The following assumptions were made from that context and should be understood in order to get the samples to work in other deployment scenarios:</p> <ul> <li>That there is a valid java JDK on the PATH for executing the samples</li> <li>The Knox Demo LDAP server is running on localhost and port 33389 which is the default port for the ApacheDS LDAP server.</li> <li>That the LDAP directory in use has a set of demo users provisioned with the convention of username and username\"-password\" as the password. Most of the samples have some variation of this pattern with \"guest\" and \"guest-password\".</li> <li>That the Knox Gateway instance is running on the same machine which you will be running the samples from - therefore \"localhost\" and that the default port of \"8443\" is being used.</li> <li>Finally, that there is a properly provisioned sandbox.xml topology in the <code>{GATEWAY_HOME}/conf/topologies</code> directory that is configured to point to the actual host and ports of running service components.</li> </ul>"},{"location":"book_knox-samples/#steps-for-demo-single-node-clusters","title":"Steps for Demo Single Node Clusters","text":"<p>There should be little to do if anything in a demo environment that has been provisioned with illustrating the use of Apache Knox.</p> <p>However, the following items will be worth ensuring before you start:</p> <ol> <li>The <code>sandbox.xml</code> topology is configured properly for the deployed services</li> <li>That there is a LDAP server running with guest/guest-password user available in the directory</li> </ol>"},{"location":"book_knox-samples/#steps-for-ambari-deployed-knox-gateway","title":"Steps for Ambari deployed Knox Gateway","text":"<p>Apache Knox instances that are under the management of Ambari are generally assumed not to be demo instances. These instances are in place to facilitate development, testing or production Hadoop clusters.</p> <p>The Knox samples can however be made to work with Ambari managed Knox instances with a few steps:</p> <ol> <li>You need to have SSH access to the environment in order for the localhost assumption within the samples to be valid</li> <li>The Knox Demo LDAP Server is started - you can start it from Ambari</li> <li>The <code>default.xml</code> topology file can be copied to <code>sandbox.xml</code> in order to satisfy the topology name assumption in the samples</li> <li> <p>Be sure to use an actual Java JRE to run the sample with something like:</p> <p>/usr/jdk64/jdk1.7.0_67/bin/java -jar bin/shell.jar samples/ExampleWebHdfsLs.groovy</p> </li> </ol>"},{"location":"book_knox-samples/#steps-for-a-manually-installed-knox-gateway","title":"Steps for a manually installed Knox Gateway","text":"<p>For manually installed Knox instances, there is really no way for the installer to know how to configure the topology file for you.</p> <p>Essentially, these steps are identical to the Ambari deployed instance except that #3 should be replaced with the configuration of the out of the box <code>sandbox.xml</code> to point the configuration at the proper hosts and ports.</p> <ol> <li>You need to have SSH access to the environment in order for the localhost assumption within the samples to be valid.</li> <li>The Knox Demo LDAP Server is started - you can start it from Ambari</li> <li>Change the hosts and ports within the <code>{GATEWAY_HOME}/conf/topologies/sandbox.xml</code> to reflect your actual cluster service locations.</li> <li> <p>Be sure to use an actual Java JRE to run the sample with something like:</p> <p>/usr/jdk64/jdk1.7.0_67/bin/java -jar bin/shell.jar samples/ExampleWebHdfsLs.groovy</p> </li> </ol>"},{"location":"book_limitations/","title":"Book limitations","text":""},{"location":"book_limitations/#limitations","title":"Limitations","text":""},{"location":"book_limitations/#secure-oozie-postput-request-payload-size-restriction","title":"Secure Oozie POST/PUT Request Payload Size Restriction","text":"<p>With one exception there are no known size limits for requests or responses payloads that pass through the gateway. The exception involves POST or PUT request payload sizes for Oozie in a Kerberos secured Hadoop cluster. In this one case there is currently a 4Kb payload size limit for the first request made to the Hadoop cluster. This is a result of how the gateway negotiates a trust relationship between itself and the cluster via SPNEGO. There is an undocumented configuration setting to modify this limit's value if required. In the future this will be made more easily configurable and at that time it will be documented.</p>"},{"location":"book_limitations/#group-membership-propagation","title":"Group Membership Propagation","text":"<p>Groups that are acquired via Shiro Group Lookup and/or Identity Assertion Group Principal Mapping are not propagated to the Hadoop services. Therefore, groups used for Service Level Authorization policy may not match those acquired within the cluster via GroupMappingServiceProvider plugins.</p>"},{"location":"book_limitations/#knox-consumer-restriction","title":"Knox Consumer Restriction","text":"<p>Consumption of messages via Knox at this time is not supported.  The Confluent Kafka REST Proxy that Knox relies upon is stateful when used for consumption of messages.</p>"},{"location":"book_service-details/","title":"Book service details","text":""},{"location":"book_service-details/#service-details","title":"Service Details","text":"<p>In the sections that follow, the integrations currently available out of the box with the gateway will be described. In general these sections will include examples that demonstrate how to access each of these services via the gateway. In many cases this will include both the use of [cURL][curl] as a REST API client as well as the use of the Knox Client DSL. You may notice that there are some minor differences between using the REST API of a given service via the gateway. In general this is necessary in order to achieve the goal of not leaking internal Hadoop cluster details to the client.</p> <p>Keep in mind that the gateway uses a plugin model for supporting Hadoop services. Check back with the [Apache Knox][site] site for the latest news on plugin availability. You can also create your own custom plugin to extend the capabilities of the gateway.</p> <p>These are the current Hadoop services with built-in support.</p> <ul> <li>WebHDFS </li> <li>WebHCat </li> <li>Oozie </li> <li>HBase </li> <li>Hive </li> <li>Yarn </li> <li>Kafka </li> <li>Storm </li> <li>Solr </li> <li>Configuration </li> <li>Default HA </li> <li>Avatica </li> <li>Cloudera Manager </li> <li>Livy </li> <li>Elasticsearch </li> <li>SSL Certificate Trust </li> <li>Service Test</li> </ul>"},{"location":"book_service-details/#assumptions","title":"Assumptions","text":"<p>This document assumes a few things about your environment in order to simplify the examples.</p> <ul> <li>The JVM is executable as simply <code>java</code>.</li> <li>The Apache Knox Gateway is installed and functional.</li> <li>The example commands are executed within the context of the <code>GATEWAY_HOME</code> current directory. The <code>GATEWAY_HOME</code> directory is the directory within the Apache Knox Gateway installation that contains the README file and the bin, conf and deployments directories.</li> <li>The [cURL][curl] command line HTTP client utility is installed and functional.</li> <li>A few examples optionally require the use of commands from a standard Groovy installation. These examples are optional but to try them you will need Groovy installed.</li> <li>The default configuration for all of the samples is setup for use with Hortonworks' [Sandbox][sandbox] version 2.</li> </ul>"},{"location":"book_service-details/#customization","title":"Customization","text":"<p>Using these samples with other Hadoop installations will require changes to the steps described here as well as changes to referenced sample scripts. This will also likely require changes to the gateway's default configuration. In particular host names, ports, user names and password may need to be changed to match your environment. These changes may need to be made to gateway configuration and also the Groovy sample script files in the distribution. All of the values that may need to be customized in the sample scripts can be found together at the top of each of these files.</p>"},{"location":"book_service-details/#curl","title":"cURL","text":"<p>The cURL HTTP client command line utility is used extensively in the examples for each service. In particular this form of the cURL command line is used repeatedly.</p> <pre><code>curl -i -k -u guest:guest-password ...\n</code></pre> <p>The option <code>-i</code> (aka <code>--include</code>) is used to output HTTP response header information. This will be important when the content of the HTTP Location header is required for subsequent requests.</p> <p>The option <code>-k</code> (aka <code>--insecure</code>) is used to avoid any issues resulting from the use of demonstration SSL certificates.</p> <p>The option <code>-u</code> (aka <code>--user</code>) is used to provide the credentials to be used when the client is challenged by the gateway.</p> <p>Keep in mind that the samples do not use the cookie features of cURL for the sake of simplicity. Therefore each request via cURL will result in an authentication.</p>"},{"location":"book_topology_port_mapping/","title":"Book topology port mapping","text":""},{"location":"book_topology_port_mapping/#topology-port-mapping","title":"Topology Port Mapping","text":"<p>This feature allows mapping of a topology to a port, as a result one can have a specific topology listening exclusively on a configured port. This feature  routes URLs to these port-mapped topologies without the additional context that the gateway uses for differentiating from one Hadoop cluster to another, just like the #[Default Topology URLs] feature, but on a dedicated port. </p> <p>NOTE: Once the topologies are configured to listen on a dedicated port they will not be available on the default gateway port.</p> <p>The configuration for Topology Port Mapping goes in <code>gateway-site.xml</code> file. The configuration uses the property name and value model. The format for the property name is <code>gateway.port.mapping.{topologyName}</code> and value is the port number that this topology will listen on. </p> <p>In the following example, the topology <code>development</code> will listen on 9443 (if the port is not already taken).</p> <pre><code>  &lt;property&gt;\n      &lt;name&gt;gateway.port.mapping.development&lt;/name&gt;\n      &lt;value&gt;9443&lt;/value&gt;\n      &lt;description&gt;Topology and Port mapping&lt;/description&gt;\n  &lt;/property&gt;\n</code></pre> <p>An example of how one can access WebHDFS URL using the above configuration is</p> <pre><code> https://{gateway-host}:9443/webhdfs\n https://{gateway-host}:9443/{gateway-path}/development/webhdfs\n</code></pre> <p>All of the above URL will be valid URLs for the above described configuration.</p> <p>This feature is turned on by default. Use the property <code>gateway.port.mapping.enabled</code> to turn it on/off. e.g.</p> <pre><code> &lt;property&gt;\n     &lt;name&gt;gateway.port.mapping.enabled&lt;/name&gt;\n     &lt;value&gt;true&lt;/value&gt;\n     &lt;description&gt;Enable/Disable port mapping feature.&lt;/description&gt;\n &lt;/property&gt;\n</code></pre> <p>If a topology mapped port is in use by another topology or a process, an ERROR message is logged and gateway startup continues as normal.  Default gateway port cannot be used for port mapping, use #[Default Topology URLs] feature instead.</p>"},{"location":"book_troubleshooting/","title":"General Troubleshooting","text":""},{"location":"book_troubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"book_troubleshooting/#finding-logs","title":"Finding Logs","text":"<p>When things aren't working the first thing you need to do is examine the diagnostic logs. Depending upon how you are running the gateway these diagnostic logs will be output to different locations.</p>"},{"location":"book_troubleshooting/#java-jar-bingatewayjar","title":"java -jar bin/gateway.jar","text":"<p>When the gateway is run this way the diagnostic output is written directly to the console. If you want to capture that output you will need to redirect the console output to a file using OS specific techniques.</p> <pre><code>java -jar bin/gateway.jar &gt; gateway.log\n</code></pre>"},{"location":"book_troubleshooting/#bingatewaysh-start","title":"bin/gateway.sh start","text":"<p>When the gateway is run this way the diagnostic output is written to <code>{GATEWAY_HOME}/log/knox.out</code> and <code>{GATEWAY_HOME}/log/knox.err</code>. Typically only knox.out will have content.</p>"},{"location":"book_troubleshooting/#increasing-logging","title":"Increasing Logging","text":"<p>The <code>log4j.properties</code> files <code>{GATEWAY_HOME}/conf</code> can be used to change the granularity of the logging done by Knox. The Knox server must be restarted in order for these changes to take effect. There are various useful loggers pre-populated but commented out.</p> <pre><code>log4j.logger.org.apache.knox.gateway=DEBUG # Use this logger to increase the debugging of Apache Knox itself.\nlog4j.logger.org.apache.shiro=DEBUG          # Use this logger to increase the debugging of Apache Shiro.\nlog4j.logger.org.apache.http=DEBUG           # Use this logger to increase the debugging of Apache HTTP components.\nlog4j.logger.org.apache.http.client=DEBUG    # Use this logger to increase the debugging of Apache HTTP client component.\nlog4j.logger.org.apache.http.headers=DEBUG   # Use this logger to increase the debugging of Apache HTTP header.\nlog4j.logger.org.apache.http.wire=DEBUG      # Use this logger to increase the debugging of Apache HTTP wire traffic.\n</code></pre>"},{"location":"book_troubleshooting/#ldap-server-connectivity-issues","title":"LDAP Server Connectivity Issues","text":"<p>If the gateway cannot contact the configured LDAP server you will see errors in the gateway diagnostic output.</p> <pre><code>13/11/15 16:30:17 DEBUG authc.BasicHttpAuthenticationFilter: Attempting to execute login with headers [Basic Z3Vlc3Q6Z3Vlc3QtcGFzc3dvcmQ=]\n13/11/15 16:30:17 DEBUG ldap.JndiLdapRealm: Authenticating user 'guest' through LDAP\n13/11/15 16:30:17 DEBUG ldap.JndiLdapContextFactory: Initializing LDAP context using URL    [ldap://localhost:33389] and principal [uid=guest,ou=people,dc=hadoop,dc=apache,dc=org] with pooling disabled\n13/11/15 16:30:17 DEBUG servlet.SimpleCookie: Added HttpServletResponse Cookie [rememberMe=deleteMe; Path=/gateway/vaultservice; Max-Age=0; Expires=Thu, 14-Nov-2013 21:30:17 GMT]\n13/11/15 16:30:17 DEBUG authc.BasicHttpAuthenticationFilter: Authentication required: sending 401 Authentication challenge response.\n</code></pre> <p>The client should see something along the lines of:</p> <pre><code>HTTP/1.1 401 Unauthorized\nWWW-Authenticate: BASIC realm=\"application\"\nContent-Length: 0\nServer: Jetty(8.1.12.v20130726)\n</code></pre> <p>Resolving this will require ensuring that the LDAP server is running and that connection information is correct. The LDAP server connection information is configured in the cluster's topology file (e.g. {GATEWAY_HOME}/deployments/sandbox.xml).</p>"},{"location":"book_troubleshooting/#hadoop-cluster-connectivity-issues","title":"Hadoop Cluster Connectivity Issues","text":"<p>If the gateway cannot contact one of the services in the configured Hadoop cluster you will see errors in the gateway diagnostic output.</p> <pre><code>13/11/18 18:49:45 WARN knox.gateway: Connection exception dispatching request: http://localhost:50070/webhdfs/v1/?user.name=guest&amp;op=LISTSTATUS org.apache.http.conn.HttpHostConnectException: Connection to http://localhost:50070 refused\norg.apache.http.conn.HttpHostConnectException: Connection to http://localhost:50070 refused\n  at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:190)\n  at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:294)\n  at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:645)\n  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:480)\n  at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)\n  at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)\n  at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)\n  at org.apache.knox.gateway.dispatch.HttpClientDispatch.executeRequest(HttpClientDispatch.java:99)\n</code></pre> <p>The resulting behavior on the client will differ by client. For the client DSL executing the <code>{GATEWAY_HOME}/samples/ExampleWebHdfsLs.groovy</code> the output will look like this.</p> <pre><code>Caught: org.apache.knox.gateway.shell.HadoopException: org.apache.knox.gateway.shell.ErrorResponse: HTTP/1.1 500 Server Error\norg.apache.knox.gateway.shell.HadoopException: org.apache.knox.gateway.shell.ErrorResponse: HTTP/1.1 500 Server Error\n  at org.apache.knox.gateway.shell.AbstractRequest.now(AbstractRequest.java:72)\n  at org.apache.knox.gateway.shell.AbstractRequest$now.call(Unknown Source)\n  at ExampleWebHdfsLs.run(ExampleWebHdfsLs.groovy:28)\n</code></pre> <p>When executing commands requests via cURL the output might look similar to the following example.</p> <pre><code>Set-Cookie: JSESSIONID=16xwhpuxjr8251ufg22f8pqo85;Path=/gateway/sandbox;Secure\nContent-Type: text/html;charset=ISO-8859-1\nCache-Control: must-revalidate,no-cache,no-store\nContent-Length: 21856\nServer: Jetty(8.1.12.v20130726)\n\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/&gt;\n&lt;title&gt;Error 500 Server Error&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 500&lt;/h2&gt;\n</code></pre> <p>Resolving this will require ensuring that the Hadoop services are running and that connection information is correct. Basic Hadoop connectivity can be evaluated using cURL as described elsewhere. Otherwise the Hadoop cluster connection information is configured in the cluster's topology file (e.g. <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>).</p>"},{"location":"book_troubleshooting/#http-vs-https-protocol-issues","title":"HTTP vs HTTPS protocol issues","text":"<p>When Knox is configured to accept requests over SSL and is presented with a request over plain HTTP, the client is presented with an error such as seen in the following:</p> <pre><code>curl -i -k -u guest:guest-password -X GET 'http://localhost:8443/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS'\nthe following error is returned\ncurl: (52) Empty reply from server\n</code></pre> <p>This is the default behavior for Jetty SSL listener. While the credentials to the default authentication provider continue to be username and password, we do not want to encourage sending these in clear text. Since preemptively sending BASIC credentials is a common pattern with REST APIs it would be unwise to redirect to a HTTPS listener thus allowing clear text passwords.</p> <p>To resolve this issue, we have two options:</p> <ol> <li>change the scheme in the URL to https and deal with any trust relationship issues with the presented server certificate</li> <li>Disabling SSL in gateway-site.xml - this is not encouraged due to the reasoning described above.</li> </ol>"},{"location":"book_troubleshooting/#check-hadoop-cluster-access-via-curl","title":"Check Hadoop Cluster Access via cURL","text":"<p>When you are experiencing connectivity issue it can be helpful to \"bypass\" the gateway and invoke the Hadoop REST APIs directly. This can easily be done using the cURL command line utility or many other REST/HTTP clients. Exactly how to use cURL depends on the configuration of your Hadoop cluster. In general however you will use a command line the one that follows.</p> <pre><code>curl -ikv -X GET 'http://namenode-host:50070/webhdfs/v1/?op=LISTSTATUS'\n</code></pre> <p>If you are using Sandbox the WebHDFS or NameNode port will be mapped to localhost so this command can be used.</p> <pre><code>curl -ikv -X GET 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS'\n</code></pre> <p>If you are using a cluster secured with Kerberos you will need to have used <code>kinit</code> to authenticate to the KDC. Then the command below should verify that WebHDFS in the Hadoop cluster is accessible.</p> <pre><code>curl -ikv --negotiate -u : -X 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS'\n</code></pre>"},{"location":"book_troubleshooting/#authentication-issues","title":"Authentication Issues","text":"<p>The following log information is available when you enable debug level logging for shiro. This can be done within the conf/log4j.properties file. Not the \"Password not correct for user\" message.</p> <pre><code>13/11/15 16:37:15 DEBUG authc.BasicHttpAuthenticationFilter: Attempting to execute login with headers [Basic Z3Vlc3Q6Z3Vlc3QtcGFzc3dvcmQw]\n13/11/15 16:37:15 DEBUG ldap.JndiLdapRealm: Authenticating user 'guest' through LDAP\n13/11/15 16:37:15 DEBUG ldap.JndiLdapContextFactory: Initializing LDAP context using URL [ldap://localhost:33389] and principal [uid=guest,ou=people,dc=hadoop,dc=apache,dc=org] with pooling disabled\n2013-11-15 16:37:15,899 INFO  Password not correct for user 'uid=guest,ou=people,dc=hadoop,dc=apache,dc=org'\n2013-11-15 16:37:15,899 INFO  Authenticator org.apache.directory.server.core.authn.SimpleAuthenticator@354c78e3 failed to authenticate: BindContext for DN 'uid=guest,ou=people,dc=hadoop,dc=apache,dc=org', credentials &lt;0x67 0x75 0x65 0x73 0x74 0x2D 0x70 0x61 0x73 0x73 0x77 0x6F 0x72 0x64 0x30 &gt;\n2013-11-15 16:37:15,899 INFO  Cannot bind to the server\n13/11/15 16:37:15 DEBUG servlet.SimpleCookie: Added HttpServletResponse Cookie [rememberMe=deleteMe; Path=/gateway/vaultservice; Max-Age=0; Expires=Thu, 14-Nov-2013 21:37:15 GMT]\n13/11/15 16:37:15 DEBUG authc.BasicHttpAuthenticationFilter: Authentication required: sending 401 Authentication challenge response.\n</code></pre> <p>The client will likely see something along the lines of:</p> <pre><code>HTTP/1.1 401 Unauthorized\nWWW-Authenticate: BASIC realm=\"application\"\nContent-Length: 0\nServer: Jetty(8.1.12.v20130726)\n</code></pre>"},{"location":"book_troubleshooting/#using-ldapsearch-to-verify-ldap-connectivity-and-credentials","title":"Using ldapsearch to verify LDAP connectivity and credentials","text":"<p>If your authentication to Knox fails and you believe you're using correct credentials, you could try to verify the connectivity and credentials using ldapsearch, assuming you are using LDAP directory for authentication.</p> <p>Assuming you are using the default values that came out of box with Knox, your ldapsearch command would be like the following</p> <pre><code>ldapsearch -h localhost -p 33389 -D \"uid=guest,ou=people,dc=hadoop,dc=apache,dc=org\" -w guest-password -b \"uid=guest,ou=people,dc=hadoop,dc=apache,dc=org\" \"objectclass=*\"\n</code></pre> <p>This should produce output like the following</p> <pre><code># extended LDIF\n\nLDAPv3\nbase &lt;uid=guest,ou=people,dc=hadoop,dc=apache,dc=org&gt; with scope subtree\nfilter: objectclass=*\nrequesting: ALL\n\n\n# guest, people, hadoop.apache.org\ndn: uid=guest,ou=people,dc=hadoop,dc=apache,dc=org\nobjectClass: organizationalPerson\nobjectClass: person\nobjectClass: inetOrgPerson\nobjectClass: top\nuid: guest\ncn: Guest\nsn: User\nuserpassword:: Z3Vlc3QtcGFzc3dvcmQ=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n</code></pre> <p>In a more general form the ldapsearch command would be</p> <pre><code>ldapsearch -h {HOST} -p {PORT} -D {DN of binding user} -w {bind password} -b {DN of binding user} \"objectclass=*}\n</code></pre>"},{"location":"book_troubleshooting/#hostname-resolution-issues","title":"Hostname Resolution Issues","text":"<p>The deployments/sandbox.xml topology file has the host mapping feature enabled. This is required due to the way networking is setup in the Sandbox VM. Specifically the VM's internal hostname is sandbox.hortonworks.com. Since this hostname cannot be resolved to the actual VM Knox needs to map that hostname to something resolvable.</p> <p>If for example host mapping is disabled but the Sandbox VM is still used you will see an error in the diagnostic output similar to the below.</p> <pre><code>13/11/18 19:11:35 WARN knox.gateway: Connection exception dispatching request: http://sandbox.hortonworks.com:50075/webhdfs/v1/user/guest/example/README?op=CREATE&amp;namenoderpcaddress=sandbox.hortonworks.com:8020&amp;user.name=guest&amp;overwrite=false java.net.UnknownHostException: sandbox.hortonworks.com\njava.net.UnknownHostException: sandbox.hortonworks.com\n  at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n</code></pre> <p>On the other hand if you are migrating from the Sandbox based configuration to a cluster you have deployment you may see a similar error. However in this case you may need to disable host mapping. This can be done by modifying the topology file (e.g. deployments/sandbox.xml) for the cluster.</p> <pre><code>...\n&lt;provider&gt;\n    &lt;role&gt;hostmap&lt;/role&gt;\n    &lt;name&gt;static&lt;/name&gt;\n    &lt;enabled&gt;false&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;localhost&lt;/name&gt;&lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n....\n</code></pre>"},{"location":"book_troubleshooting/#job-submission-issues-hdfs-home-directories","title":"Job Submission Issues - HDFS Home Directories","text":"<p>If you see error like the following in your console  while submitting a Job using groovy shell, it is likely that the authenticated user does not have a home directory on HDFS.</p> <pre><code>Caught: org.apache.knox.gateway.shell.HadoopException: org.apache.knox.gateway.shell.ErrorResponse: HTTP/1.1 403 Forbidden\norg.apache.knox.gateway.shell.HadoopException: org.apache.knox.gateway.shell.ErrorResponse: HTTP/1.1 403 Forbidden\n</code></pre> <p>You would also see this error if you try file operation on the home directory of the authenticating user.</p> <p>The error would look a little different as shown below  if you are attempting to the operation with cURL.</p> <pre><code>{\"RemoteException\":{\"exception\":\"AccessControlException\",\"javaClassName\":\"org.apache.hadoop.security.AccessControlException\",\"message\":\"Permission denied: user=tom, access=WRITE, inode=\\\"/user\\\":hdfs:hdfs:drwxr-xr-x\"}}*\n</code></pre>"},{"location":"book_troubleshooting/#resolution","title":"Resolution","text":"<p>Create the home directory for the user on HDFS. The home directory is typically of the form <code>/user/{userid}</code> and should be owned by the user. user 'hdfs' can create such a directory and make the user owner of the directory.</p>"},{"location":"book_troubleshooting/#job-submission-issues-os-accounts","title":"Job Submission Issues - OS Accounts","text":"<p>If the Hadoop cluster is not secured with Kerberos, the user submitting a job need not have an OS account on the Hadoop NodeManagers.</p> <p>If the Hadoop cluster is secured with Kerberos, the user submitting the job should have an OS account on Hadoop NodeManagers.</p> <p>In either case if the user does not have such OS account, his file permissions are based on user ownership of files or \"other\" permission in \"ugo\" posix permission. The user does not get any file permission as a member of any group if you are using default <code>hadoop.security.group.mapping</code>.</p> <p>TODO: add sample error message from running test on secure cluster with missing OS account</p>"},{"location":"book_troubleshooting/#hbase-issues","title":"HBase Issues","text":"<p>If you experience problems running the HBase samples with the Sandbox VM it may be necessary to restart HBase and the HBASE REST API. This can sometimes occur with the Sandbox VM is restarted from a saved state. If the client hangs after emitting the last line in the sample output below you are most likely affected.</p> <pre><code>System version : {...}\nCluster version : 0.96.0.2.0.6.0-76-hadoop2\nStatus : {...}\nCreating table 'test_table'...\n</code></pre> <p>HBase and the HBASE REST API can be restarted using the following commands on the Hadoop Sandbox VM. You will need to ssh into the VM in order to run these commands.</p> <pre><code>sudo -u hbase /usr/lib/hbase/bin/hbase-daemon.sh stop master\nsudo -u hbase /usr/lib/hbase/bin/hbase-daemon.sh start master\nsudo -u hbase /usr/lib/hbase/bin/hbase-daemon.sh restart rest\n</code></pre>"},{"location":"book_troubleshooting/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<p>Clients that do not trust the certificate presented by the server will behave in different ways. A browser will typically warn you of the inability to trust the received certificate and give you an opportunity to add an exception for the particular certificate. Curl will present you with the follow message and instructions for turning of certificate verification:</p> <pre><code>curl performs SSL certificate verification by default, using a \"bundle\" \n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn't adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented \n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you'd like to turn off curl's verification of the certificate, use\n the -k (or --insecure) option.\n</code></pre>"},{"location":"book_troubleshooting/#spnego-authentication-issues","title":"SPNego Authentication Issues","text":"<p>Calls from Knox to Secure Hadoop Cluster fails, with SPNego authentication problems, if there was a TGT for Knox in disk cache when Knox was started.</p> <p>You are likely to run into this situation on developer machines where the developer could have kinited for some testing.</p> <p>Work Around: clear TGT of Knox from disk cache (calling <code>kdestroy</code> would do it), before starting Knox</p>"},{"location":"book_troubleshooting/#filing-bugs","title":"Filing Bugs","text":"<p>Bugs can be filed using [Jira][jira]. Please include the results of this command below in the Environment section. Also include the version of Hadoop being used in the same section.</p> <pre><code>cd {GATEWAY_HOME}\njava -jar bin/gateway.jar -version\n</code></pre>"},{"location":"book_ui_service_details/","title":"Book ui service details","text":""},{"location":"book_ui_service_details/#ui-service-details","title":"UI Service Details","text":"<p>In the sections that follow, the integrations for proxying various UIs currently available out of the box with the gateway will be described. These sections will include examples that demonstrate how to access each of these services via the gateway.</p> <p>These are the current Hadoop services with built-in support for their UIs.</p> <ul> <li>Name Node UI</li> <li>Job History UI</li> <li>Oozie UI</li> <li>HBase UI</li> <li>Yarn UI</li> <li>Spark UI</li> <li>Ambari UI</li> <li>Ranger Admin Console</li> <li>Atlas UI</li> <li>Zeppelin UI</li> <li>Nifi UI</li> <li>Hue UI</li> </ul>"},{"location":"book_ui_service_details/#assumptions","title":"Assumptions","text":"<p>This section assumes an environment setup similar to the one in the REST services section #[Service Details]</p>"},{"location":"book_ui_service_details/#name-node-ui","title":"Name Node UI","text":"<p>The Name Node UI is available on the same host and port combination that WebHDFS is available on. As mentioned in the WebHDFS REST service configuration section, the values for the host and port can be obtained from the following properties in hdfs-site.xml</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:50070&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;dfs.https.namenode.https-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:50470&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The values above need to be reflected in each topology descriptor file deployed to the gateway. The gateway by default includes a sample topology descriptor file <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;HDFSUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:50070&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>In addition to the service configuration for HDFSUI, the REST service configuration for WEBHDFS is also required.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;NAMENODE&lt;/role&gt;\n    &lt;url&gt;hdfs://sandbox.hortonworks.com:8020&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;WEBHDFS&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:50070/webhdfs&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>By default the gateway is configured to use the HTTP endpoint for WebHDFS in the Sandbox. This could alternatively be configured to use the HTTPS endpoint by providing the correct address.</p>"},{"location":"book_ui_service_details/#name-node-ui-url-mapping","title":"Name Node UI URL Mapping","text":"<p>For Name Node UI URLs, the mapping of Knox Gateway accessible HDFS UI URLs to direct HDFS UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/hdfs</code> <code>http://{webhdfs-host}:50070/</code> <p>For example to browse the file system using the NameNode UI the URL in a web browser would be:</p> <pre><code>http://sandbox.hortonworks.com:50070/explorer.html#\n</code></pre> <p>And using the gateway to access the same page the URL would be (where the gateway host:port is 'localhost:8443')</p> <pre><code>https://localhost:8443/gateway/sandbox/hdfs/explorer.html#\n</code></pre>"},{"location":"book_ui_service_details/#job-history-ui","title":"Job History UI","text":"<p>The Job History UI service can be configured in a topology by adding the following snippet. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;JOBHISTORYUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:19888&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The values for the host and port can be obtained from the following property in mapred-site.xml</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:19888&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"book_ui_service_details/#job-history-ui-url-mapping","title":"Job History UI URL Mapping","text":"<p>For Job History UI URLs, the mapping of Knox Gateway accessible Job History UI URLs to direct Job History UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/jobhistory</code> <code>http://{jobhistory-host}:19888/jobhistory</code>"},{"location":"book_ui_service_details/#oozie-ui","title":"Oozie UI","text":"<p>The Oozie UI service can be configured in a topology by adding the following snippet. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;OOZIEUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:11000/oozie&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The value for the URL can be obtained from the following property in oozie-site.xml</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;oozie.base.url&lt;/name&gt;\n    &lt;value&gt;http://sandbox.hortonworks.com:11000/oozie&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"book_ui_service_details/#oozie-ui-url-mapping","title":"Oozie UI URL Mapping","text":"<p>For Oozie UI URLs, the mapping of Knox Gateway accessible Oozie UI URLs to direct Oozie UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/oozie/</code> <code>http://{oozie-host}:11000/oozie/</code>"},{"location":"book_ui_service_details/#hbase-ui","title":"HBase UI","text":"<p>The HBase UI service can be configured in a topology by adding the following snippet. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;HBASEUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:16010&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The values for the host and port can be obtained from the following property in hbase-site.xml. Below the hostname of the HBase master is used since the bindAddress is 0.0.0.0</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hbase.master.info.bindAddress&lt;/name&gt;\n    &lt;value&gt;0.0.0.0&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hbase.master.info.port&lt;/name&gt;\n    &lt;value&gt;16010&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"book_ui_service_details/#hbase-ui-url-mapping","title":"HBase UI URL Mapping","text":"<p>For HBase UI URLs, the mapping of Knox Gateway accessible HBase UI URLs to direct HBase Master UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/hbase/webui/</code> <code>http://{hbase-master-host}:16010/</code>"},{"location":"book_ui_service_details/#yarn-ui","title":"YARN UI","text":"<p>The YARN UI service can be configured in a topology by adding the following snippet. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;YARNUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:8088&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The values for the host and port can be obtained from the following property in mapred-site.xml</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:8088&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"book_ui_service_details/#yarn-ui-url-mapping","title":"YARN UI URL Mapping","text":"<p>For Resource Manager UI URLs, the mapping of Knox Gateway accessible Resource Manager UI URLs to direct Resource Manager UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/yarn</code> <code>http://{resource-manager-host}:8088/cluster</code>"},{"location":"book_ui_service_details/#spark-ui","title":"Spark UI","text":"<p>The Spark History UI service can be configured in a topology by adding the following snippet. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;SPARKHISTORYUI&lt;/role&gt;\n    &lt;url&gt;http://sandbox.hortonworks.com:18080/&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>Please, note that for Spark versions older than 2.4.0 you need to set <code>spark.ui.proxyBase</code> to <code>/{gateway-path}/{cluster-name}/sparkhistory</code> (for more information, please refer to SPARK-24209).</p> <p>Moreover, before Spark 2.3.1, there is a bug is Spark which prevents the UI to work properly when the proxy is accessed using a link which ends with \"/\" (SPARK-23644). So if the list of the applications is empty when you access the UI though the gateway, please check that there is a \"/\" at the end of the URL you are using.</p>"},{"location":"book_ui_service_details/#spark-history-ui-url-mapping","title":"Spark History UI URL Mapping","text":"<p>For Spark History UI URLs, the mapping of Knox Gateway accessible Spark History UI URLs to direct Spark History UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/sparkhistory</code> <code>http://{spark-history-host}:18080</code>"},{"location":"book_ui_service_details/#ambari-ui","title":"Ambari UI","text":"<p>Ambari UI has functionality around provisioning and managing services in a Hadoop cluster. This UI can now be used  behind the Knox gateway.</p> <p>To enable this functionality, a topology file needs to have the following configuration (for AMBARIUI and AMBARIWS):</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;AMBARIUI&lt;/role&gt;\n    &lt;url&gt;http://&lt;hostname&gt;:&lt;port&gt;&lt;/url&gt;\n&lt;/service&gt;\n\n&lt;service&gt;\n    &lt;role&gt;AMBARIWS&lt;/role&gt;\n    &lt;url&gt;ws://&lt;hostname&gt;:&lt;port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default Ambari http port is 8080. Also, please note that the UI service also requires the Ambari REST API service and Ambari Websocket service  to be enabled to function properly. An example of a more complete topology is given below.</p>"},{"location":"book_ui_service_details/#ambari-ui-url-mapping","title":"Ambari UI URL Mapping","text":"<p>For Ambari UI URLs, the mapping of Knox Gateway accessible URLs to direct Ambari UI URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/ambari/</code> <code>http://{ambari-host}:{ambari-port}/</code>"},{"location":"book_ui_service_details/#example-topology","title":"Example Topology","text":"<p>The Ambari UI service may require a separate topology file due to its requirements around authentication. Knox passes through authentication challenge and credentials to the service in this case.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        &lt;provider&gt;\n            &lt;role&gt;authentication&lt;/role&gt;\n            &lt;name&gt;Anonymous&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;identity-assertion&lt;/role&gt;\n            &lt;name&gt;Default&lt;/name&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n        &lt;/provider&gt;\n    &lt;/gateway&gt;\n    &lt;service&gt;\n        &lt;role&gt;AMBARI&lt;/role&gt;\n        &lt;url&gt;http://localhost:8080&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;AMBARIUI&lt;/role&gt;\n        &lt;url&gt;http://localhost:8080&lt;/url&gt;\n    &lt;/service&gt;\n    &lt;service&gt;\n        &lt;role&gt;AMBARIWS&lt;/role&gt;\n        &lt;url&gt;ws://localhost:8080&lt;/url&gt;\n    &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre> <p>Please look at JIRA issue [KNOX-705] for a known issue with this release.</p>"},{"location":"book_ui_service_details/#ranger-admin-console","title":"Ranger Admin Console","text":"<p>The Ranger Admin console can now be used behind the Knox gateway.</p> <p>To enable this functionality, a topology file needs to have the following configuration:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;RANGERUI&lt;/role&gt;\n    &lt;url&gt;http://&lt;hostname&gt;:&lt;port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default Ranger http port is 8060. Also, please note that the UI service also requires the Ranger REST API service  to be enabled to function properly. An example of a more complete topology is given below.</p>"},{"location":"book_ui_service_details/#ranger-admin-console-url-mapping","title":"Ranger Admin Console URL Mapping","text":"<p>For Ranger Admin console URLs, the mapping of Knox Gateway accessible URLs to direct Ranger Admin console URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/ranger/</code> <code>http://{ranger-host}:{ranger-port}/</code>"},{"location":"book_ui_service_details/#example-topology_1","title":"Example Topology","text":"<p>The Ranger UI service may require a separate topology file due to its requirements around authentication. Knox passes through authentication challenge and credentials to the service in this case.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        &lt;provider&gt;\n            &lt;role&gt;authentication&lt;/role&gt;\n            &lt;name&gt;Anonymous&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;identity-assertion&lt;/role&gt;\n            &lt;name&gt;Default&lt;/name&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n        &lt;/provider&gt;\n    &lt;/gateway&gt;\n    &lt;service&gt;\n        &lt;role&gt;RANGER&lt;/role&gt;\n        &lt;url&gt;http://localhost:8060&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;RANGERUI&lt;/role&gt;\n        &lt;url&gt;http://localhost:8060&lt;/url&gt;\n    &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre>"},{"location":"book_ui_service_details/#atlas-ui","title":"Atlas UI","text":""},{"location":"book_ui_service_details/#atlas-rest-api","title":"Atlas Rest API","text":"<p>The Atlas Rest API can now be used behind the Knox gateway. To enable this functionality, a topology file needs to have the following configuration.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;ATLAS-API&lt;/role&gt;\n    &lt;url&gt;http://&lt;ATLAS_HOST&gt;:&lt;ATLAS_PORT&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default Atlas http port is 21000. Also, please note that the UI service also requires the Atlas REST API service to be enabled to function properly. An example of a more complete topology is given below.</p>"},{"location":"book_ui_service_details/#atlas-rest-api-url-mapping","title":"Atlas Rest API URL Mapping","text":"<p>For Atlas Rest URLs, the mapping of Knox Gateway accessible URLs to direct Atlas Rest URLs is:</p> Gateway Cluster <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{topology}/atlas/</code> <code>http://{atlas-host}:{atlas-port}/</code> <p>Access Atlas API using cULR call</p> <pre><code> curl -i -k -L -u admin:admin -X GET \\\n           'https://knox-gateway:8443/gateway/{topology}/atlas/api/atlas/v2/types/typedefs?type=classification&amp;_=1495442879421'\n</code></pre>"},{"location":"book_ui_service_details/#atlas-ui_1","title":"Atlas UI","text":"<p>In addition to the Atlas REST API, from this release there is the ability to access some of the functionality via a web. The initial functionality is very limited and serves more as a starting point/placeholder. The details are below.</p>"},{"location":"book_ui_service_details/#atlas-ui-url-mapping","title":"Atlas UI URL Mapping","text":"<p>The URL mapping for the Atlas UI is:</p> Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{topology}/atlas/index.html</code>"},{"location":"book_ui_service_details/#example-topology-for-atlas","title":"Example Topology for Atlas","text":"<pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        &lt;provider&gt;\n            &lt;role&gt;authentication&lt;/role&gt;\n            &lt;name&gt;Anonymous&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;identity-assertion&lt;/role&gt;\n            &lt;name&gt;Default&lt;/name&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n        &lt;/provider&gt;\n    &lt;/gateway&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;ATLAS-API&lt;/role&gt;\n        &lt;url&gt;http://&lt;ATLAS_HOST&gt;:&lt;ATLAS_PORT&gt;&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;ATLAS&lt;/role&gt;\n        &lt;url&gt;http://&lt;ATLAS_HOST&gt;:&lt;ATLAS_PORT&gt;&lt;/url&gt;\n    &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre> <p>Note: This feature will allow for 'anonymous' authentication. Essentially bypassing any LDAP or other authentication done by Knox and allow the proxied service to do the actual authentication.</p>"},{"location":"book_ui_service_details/#zeppelin-ui","title":"Zeppelin UI","text":"<p>Apache Knox can be used to proxy Zeppelin UI and also supports WebSocket protocol used by Zeppelin. </p>"},{"location":"book_ui_service_details/#zeppelin-ui-url-mapping","title":"Zeppelin UI URL Mapping","text":"<p>The URL mapping for the Zeppelin UI is:</p> Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{topology}/zeppelin/</code> <p>By default WebSocket functionality is disabled, it needs to be enabled for Zeppelin UI to work properly, it can be enabled by changing the <code>gateway.websocket.feature.enabled</code> property to 'true' in <code>&lt;KNOX-HOME&gt;/conf/gateway-site.xml</code> file, for e.g.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.websocket.feature.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Enable/Disable websocket feature.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Example service definition for Zeppelin in topology file is as follows, note that both ZEPPELINWS and ZEPPELINUI service declarations are required.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;ZEPPELINWS&lt;/role&gt;\n    &lt;url&gt;ws://&lt;ZEPPELIN_HOST&gt;:&lt;ZEPPELIN_PORT&gt;/ws&lt;/url&gt;\n&lt;/service&gt;\n\n&lt;service&gt;\n    &lt;role&gt;ZEPPELINUI&lt;/role&gt;\n    &lt;url&gt;http://&lt;ZEPPELIN_HOST&gt;:&lt;ZEPPELIN_PORT&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>Knox also supports secure Zeppelin UIs, for secure UIs one needs to provision Zeppelin certificate into Knox truststore.  </p>"},{"location":"book_ui_service_details/#nifi-ui","title":"Nifi UI","text":"<p>You can use the Apache Knox Gateway to provide authentication access security for your NiFi services.</p>"},{"location":"book_ui_service_details/#nifi-ui-url-mapping","title":"Nifi UI URL Mapping","text":"<p>The URL mapping for the NiFi UI is:</p> Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{topology}/nifi-app/nifi/</code> <p>The Gateway can be configured for Nifi by modifying the topology XML file.</p> <p>In the topology XML file, add the following with the correct hostname and port:</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;NIFI&lt;/role&gt;\n  &lt;url&gt;&lt;NIFI_HTTP_SCHEME&gt;://&lt;NIFI_HOST&gt;:&lt;NIFI_HTTP_SCHEME_PORT&gt;&lt;/url&gt;\n  &lt;param name=\"useTwoWaySsl\" value=\"true\"/&gt;\n&lt;/service&gt;\n</code></pre> <p>Note the setting of the useTwoWaySsl param above. Nifi requires mutual authentication via SSL and this param tells the dispatch to present a client cert to the server.</p>"},{"location":"book_ui_service_details/#hue-ui","title":"Hue UI","text":"<p>You can use the Apache Knox Gateway to provide authentication access security for your Hue UI.</p>"},{"location":"book_ui_service_details/#hue-ui-url-mapping","title":"Hue UI URL Mapping","text":"<p>The URL mapping for the Hue UI is:</p> Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{topology}/hue/</code> <p>The Gateway can be configured for Hue by modifying the topology XML file.</p> <p>In the topology XML file, add the following with the correct hostname and port:</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;HUE&lt;/role&gt;\n  &lt;url&gt;&lt;HUE_HTTP_SCHEME&gt;://&lt;HUE_HOST&gt;:&lt;HUE_HTTP_SCHEME_PORT&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"config/","title":"Gateway Config Overview","text":""},{"location":"config/#configuration","title":"Configuration","text":""},{"location":"config/#related-cluster-configuration","title":"Related Cluster Configuration","text":"<p>The following configuration changes must be made to your cluster to allow Apache Knox to dispatch requests to the various service components on behalf of end users.</p>"},{"location":"config/#grant-proxy-privileges-for-knox-user-in-core-sitexml-on-hadoop-master-nodes","title":"Grant Proxy privileges for Knox user in <code>core-site.xml</code> on Hadoop master nodes","text":"<p>Update <code>core-site.xml</code> and add the following lines towards the end of the file.</p> <p>Replace <code>FQDN_OF_KNOX_HOST</code> with the fully qualified domain name of the host running the Knox gateway. You can usually find this by running <code>hostname -f</code> on that host.</p> <p>You can use <code>*</code> for local developer testing if the Knox host does not have a static IP.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.proxyuser.knox.groups&lt;/name&gt;\n    &lt;value&gt;users&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hadoop.proxyuser.knox.hosts&lt;/name&gt;\n    &lt;value&gt;FQDN_OF_KNOX_HOST&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"config/#grant-proxy-privilege-for-knox-in-webhcat-sitexml-on-hadoop-master-nodes","title":"Grant proxy privilege for Knox in <code>webhcat-site.xml</code> on Hadoop master nodes","text":"<p>Update <code>webhcat-site.xml</code> and add the following lines towards the end of the file.</p> <p>Replace <code>FQDN_OF_KNOX_HOST</code> with the fully qualified domain name of the host running the Knox gateway. You can use <code>*</code> for local developer testing if the Knox host does not have a static IP.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;webhcat.proxyuser.knox.groups&lt;/name&gt;\n    &lt;value&gt;users&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;webhcat.proxyuser.knox.hosts&lt;/name&gt;\n    &lt;value&gt;FQDN_OF_KNOX_HOST&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"config/#grant-proxy-privilege-for-knox-in-oozie-sitexml-on-oozie-host","title":"Grant proxy privilege for Knox in <code>oozie-site.xml</code> on Oozie host","text":"<p>Update <code>oozie-site.xml</code> and add the following lines towards the end of the file.</p> <p>Replace <code>FQDN_OF_KNOX_HOST</code> with the fully qualified domain name of the host running the Knox gateway. You can use <code>*</code> for local developer testing if the Knox host does not have a static IP.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;oozie.service.ProxyUserService.proxyuser.knox.groups&lt;/name&gt;\n    &lt;value&gt;users&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;oozie.service.ProxyUserService.proxyuser.knox.hosts&lt;/name&gt;\n    &lt;value&gt;FQDN_OF_KNOX_HOST&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"config/#enable-http-transport-mode-and-use-substitution-in-hiveserver2","title":"Enable http transport mode and use substitution in HiveServer2","text":"<p>Update <code>hive-site.xml</code> and set the following properties on HiveServer2 hosts. Some of the properties may already be in the hive-site.xml.  Ensure that the values match the ones below.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hive.server2.allow.user.substitution&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.transport.mode&lt;/name&gt;\n    &lt;value&gt;http&lt;/value&gt;\n    &lt;description&gt;Server transport mode. \"binary\" or \"http\".&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.http.port&lt;/name&gt;\n    &lt;value&gt;10001&lt;/value&gt;\n    &lt;description&gt;Port number when in HTTP mode.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.http.path&lt;/name&gt;\n    &lt;value&gt;cliservice&lt;/value&gt;\n    &lt;description&gt;Path component of URL endpoint when in HTTP mode.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"config/#gateway-server-configuration","title":"Gateway Server Configuration","text":"<p>The following table illustrates the configurable elements of the Apache Knox Gateway at the server level via gateway-site.xml.</p> Property Description Default <code>gateway.deployment.dir</code> The directory within <code>GATEWAY_HOME</code> that contains gateway topology deployments <code>{GATEWAY_HOME}/data/deployments</code> <code>gateway.security.dir</code> The directory within <code>GATEWAY_HOME</code> that contains the required security artifacts <code>{GATEWAY_HOME}/data/security</code> <code>gateway.data.dir</code> The directory within <code>GATEWAY_HOME</code> that contains the gateway instance data <code>{GATEWAY_HOME}/data</code> <code>gateway.services.dir</code> The directory within <code>GATEWAY_HOME</code> that contains the gateway services definitions <code>{GATEWAY_HOME}/services</code> <code>gateway.hadoop.conf.dir</code> The directory within <code>GATEWAY_HOME</code> that contains the gateway configuration <code>{GATEWAY_HOME}/conf</code> <code>gateway.frontend.url</code> The URL that should be used during rewriting so that it can rewrite the URLs with the correct \"frontend\" URL none <code>gateway.server.header.enabled</code> Indicates whether Knox displays service info in HTTP response <code>false</code> <code>gateway.xforwarded.enabled</code> Indicates whether support for some X-Forwarded-* headers is enabled <code>true</code> <code>gateway.trust.all.certs</code> Indicates whether all presented client certs should establish trust <code>false</code> <code>gateway.client.auth.needed</code> Indicates whether clients are required to establish a trust relationship with client certificates <code>false</code> <code>gateway.truststore.password.alias</code> OPTIONAL Alias for the password to the truststore file holding the trusted client certificates. NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the master secret will be used. <code>gateway-truststore-password</code> <code>gateway.truststore.path</code> Location of the truststore for client certificates to be trusted <code>null</code> <code>gateway.truststore.type</code> Indicates the type of truststore at the path declared in <code>gateway.truststore.path</code> <code>JKS</code> <code>gateway.jdk.tls.ephemeralDHKeySize</code> <code>jdk.tls.ephemeralDHKeySize</code>, is defined to customize the ephemeral DH key sizes. The minimum acceptable DH key size is 1024 bits, except for exportable cipher suites or legacy mode (<code>jdk.tls.ephemeralDHKeySize=legacy</code>) <code>2048</code> <code>gateway.threadpool.max</code> The maximum concurrent requests the server will process. The default is 254. Connections beyond this will be queued. <code>254</code> <code>gateway.httpclient.connectionTimeout</code> The amount of time to wait when attempting a connection. The natural unit is milliseconds, but a 's' or 'm' suffix may be used for seconds or minutes respectively. <code>20s</code> <code>gateway.httpclient.maxConnections</code> The maximum number of connections that a single HttpClient will maintain to a single host:port. <code>32</code> <code>gateway.httpclient.socketTimeout</code> The amount of time to wait for data on a socket before aborting the connection. The natural unit is milliseconds, but a 's' or 'm' suffix may be used for seconds or minutes respectively. <code>20s</code> <code>gateway.httpclient.truststore.password.alias</code> OPTIONAL Alias for the password to the truststore file holding the trusted service certificates. NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the master secret will be used. <code>gateway-httpclient-truststore-password</code> <code>gateway.httpclient.truststore.path</code> Location of the truststore for service certificates to be trusted <code>null</code> <code>gateway.httpclient.truststore.type</code> Indicates the type of truststore at the path declared in <code>gateway.httpclient.truststore.path</code> <code>JKS</code> <code>gateway.httpserver.requestBuffer</code> The size of the HTTP server request buffer in bytes <code>16384</code> <code>gateway.httpserver.requestHeaderBuffer</code> The size of the HTTP server request header buffer in bytes <code>8192</code> <code>gateway.httpserver.responseBuffer</code> The size of the HTTP server response buffer in bytes <code>32768</code> <code>gateway.httpserver.responseHeaderBuffer</code> The size of the HTTP server response header buffer in bytes <code>8192</code> <code>gateway.websocket.feature.enabled</code> Enable/Disable WebSocket feature <code>false</code> <code>gateway.tls.keystore.password.alias</code> OPTIONAL Alias for the password to the keystore file holding the Gateway's TLS certificate and keypair. NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the master secret will be used. <code>gateway-identity-keystore-password</code> <code>gateway.tls.keystore.path</code> OPTIONAL The path to the keystore file where the Gateway's TLS certificate and keypair are stored. If not set, the default keystore file will be used - data/security/keystores/gateway.jks. null <code>gateway.tls.keystore.type</code> OPTIONAL The type of the keystore file where the Gateway's TLS certificate and keypair are stored. See <code>gateway.tls.keystore.path</code>. <code>JKS</code> <code>gateway.tls.key.alias</code> OPTIONAL The alias for the Gateway's TLS certificate and keypair within the default keystore or the keystore specified via <code>gateway.tls.keystore.path</code>. <code>gateway-identity</code> <code>gateway.tls.key.passphrase.alias</code> OPTIONAL The alias for passphrase for the Gateway's TLS private key stored within the default keystore or the keystore specified via <code>gateway.tls.keystore.path</code>.  NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the keystore password or the master secret will be used. See <code>gateway.tls.keystore.password.alias</code> <code>gateway-identity-passphrase</code> <code>gateway.signing.keystore.name</code> OPTIONAL Filename of keystore file that contains the signing keypair. NOTE: An alias needs to be created using <code>knoxcli.sh create-alias</code> for the alias name <code>signing.key.passphrase</code> in order to provide the passphrase to access the keystore. null <code>gateway.signing.keystore.password.alias</code> OPTIONAL Alias for the password to the keystore file holding the signing keypair. NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the master secret will be used. <code>signing.keystore.password</code> <code>gateway.signing.keystore.type</code> OPTIONAL The type of the keystore file where the signing keypair is stored. See <code>gateway.signing.keystore.name</code>. <code>JKS</code> <code>gateway.signing.key.alias</code> OPTIONAL alias for the signing keypair within the keystore specified via <code>gateway.signing.keystore.name</code> null <code>gateway.signing.key.passphrase.alias</code> OPTIONAL The alias for passphrase for signing private key stored within the default keystore or the keystore specified via <code>gateway.signing.keystore.name</code>.  NOTE: An alias with the provided name should be created using <code>knoxcli.sh create-alias</code> inorder to provide the password; else the keystore password or the master secret will be used. See <code>gateway.signing.keystore.password.alias</code> <code>signing.key.passphrase</code> <code>ssl.enabled</code> Indicates whether SSL is enabled for the Gateway <code>true</code> <code>ssl.include.ciphers</code> A comma or pipe separated list of ciphers to accept for SSL. See the JSSE Provider docs for possible ciphers. These can also contain regular expressions as shown in the Jetty documentation. all <code>ssl.exclude.ciphers</code> A comma or pipe separated list of ciphers to reject for SSL. See the JSSE Provider docs for possible ciphers. These can also contain regular expressions as shown in the Jetty documentation. none <code>ssl.exclude.protocols</code> Excludes a comma or pipe separated list of protocols to not accept for SSL or \"none\" <code>SSLv3</code> <code>gateway.remote.config.monitor.client</code> A reference to the remote configuration registry client the remote configuration monitor will employ null <code>gateway.remote.config.monitor.client.allowUnauthenticatedReadAccess</code> When a remote registry client is configured to access a registry securely, this property can be set to allow unauthenticated clients to continue to read the content from that registry by setting the ACLs accordingly. <code>false</code> <code>gateway.remote.config.registry.&lt;name&gt;</code> A named remote configuration registry client definition, where name is an arbitrary identifier for the connection null <code>gateway.cluster.config.monitor.ambari.enabled</code> Indicates whether the Ambari cluster monitoring and associated dynamic topology updating is enabled <code>false</code> <code>gateway.cluster.config.monitor.ambari.interval</code> The interval (in seconds) at which the Ambari cluster monitor will poll for cluster configuration changes <code>60</code> <code>gateway.cluster.config.monitor.cm.enabled</code> Indicates whether the ClouderaManager cluster monitoring and associated dynamic topology updating is enabled <code>false</code> <code>gateway.cluster.config.monitor.cm.interval</code> The interval (in seconds) at which the ClouderaManager cluster monitor will poll for cluster configuration changes <code>60</code> <code>gateway.remote.alias.service.enabled</code> Turn on/off remote alias service <code>true</code> <code>gateway.read.only.override.topologies</code> A comma-delimited list of topology names which should be forcibly treated as read-only. none <code>gateway.discovery.default.address</code> The default discovery address, which is applied if no address is specified in a descriptor. null <code>gateway.discovery.default.cluster</code> The default discovery cluster name, which is applied if no cluster name is specified in a descriptor. null <code>gateway.dispatch.whitelist</code> A semicolon-delimited list of regular expressions for controlling to which endpoints Knox dispatches and redirects will be permitted. If <code>DEFAULT</code> is specified, or the property is omitted entirely, then a default domain-based whitelist will be derived from the Knox host. If <code>HTTPS_ONLY</code> is specified a default domain-based whitelist will be derived from the Knox host for only HTTPS urls. An empty value means no dispatches will be permitted. null <code>gateway.dispatch.whitelist.services</code> A comma-delimited list of service roles to which the <code>gateway.dispatch.whitelist</code> will be applied. none <code>gateway.strict.topology.validation</code> If true, topology XML files will be validated against the topology schema during redeploy <code>false</code> <code>gateway.topology.redeploy.requires.changes</code> If <code>true</code>, XML topology redeployment will happen only if the topology content is different than the actually deployed one. That is, a simple <code>touch</code> command will not yield in topology redeployment in this case. <code>false</code> <code>gateway.global.rules.services</code> Set the list of service names that have global rules, all services that are not in this list have rules that are treated as scoped to only to that service. <code>\"NAMENODE\",\"JOBTRACKER\", \"WEBHDFS\", \"WEBHCAT\", \"OOZIE\", \"WEBHBASE\", \"HIVE\", \"RESOURCEMANAGER\"</code> <code>gateway.xforwarded.header.context.append.servicename</code> Add service name to x-forward-context header for the defined list of services. <code>LIVYSERVER</code> <code>gateway.knox.token.exp.server-managed</code> Default server-managed token state configuration for all KnoxToken service and JWT provider deployments <code>false</code> <code>gateway.knox.token.eviction.interval</code> The period (seconds) about which the token state reaper will evict state for expired tokens. This configuration only applies when server-managed token state is enabled either in gateway-site or at the topology level. <code>300</code> (5 minutes) <code>gateway.knox.token.eviction.grace.period</code> A duration (seconds) beyond a token's expiration to wait before evicting its state. This configuration only applies when server-managed token state is enabled either in gateway-site or at the topology level. <code>86400</code> (24 hours) <code>gateway.knox.token.permissive.validation</code> When this feature is enabled and server managed state is enabled and Knox is presented with a valid token which is absent in server managed state, Knox will verify it without throwing an UnknownTokenException <code>false</code> <code>gateway.jetty.max.form.content.size</code> This optional parameter allows end-user to configure the form content in Knox's embedded Jetty server that a request can process is limited to protect from Denial of Service attacks. The size in bytes is limited by Jetty's ContextHandler#getMaxFormContentSize() or if there is no context then the \"org.eclipse.jetty.server.Request.maxFormContentSize\" attribute. <code>200000</code> <code>gateway.jetty.max.form.keys</code> This optional parameter allows end-user to configure the number of parameters keys is limited by Knox's embedded Jetty's ContextHandler#getMaxFormKeys() or if there is no context then the <code>org.eclipse.jetty.server.Request.maxFormKeys</code> attribute. <code>1000</code> <code>gateway.strict.transport.enabled</code> This parameter enables the HTTP Strict-Transport-Security response header globally for every response. The topology wide config with WebAppSec provider takes precedence if it is enabled. <code>false</code> <code>gateway.strict.transport.option</code> This optional parameter specifies a particular value for the HTTP Strict-Transport-Security header in case the global config is enabled. <code>max-age=31536000; includeSubDomains</code> <code>gateway.server.append.classpath</code> A <code>;</code> delimited list of paths that are appended to the gateway server's classpath. null <code>gateway.server.prepend.classpath</code> A <code>;</code> delimited list of paths that are prepended to the gateway server's classpath. null"},{"location":"config/#topology-descriptors","title":"Topology Descriptors","text":"<p>The topology descriptor files provide the gateway with per-cluster configuration information. This includes configuration for both the providers within the gateway and the services within the Hadoop cluster. These files are located in <code>{GATEWAY_HOME}/conf/topologies</code>. The general outline of this document looks like this.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        &lt;provider&gt;\n        &lt;/provider&gt;\n    &lt;/gateway&gt;\n    &lt;service&gt;\n    &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre> <p>There are typically multiple <code>&lt;provider&gt;</code> and <code>&lt;service&gt;</code> elements.</p> <p>/topology : Defines the provider and configuration and service topology for a single Hadoop cluster.</p> <p>/topology/gateway : Groups all of the provider elements</p> <p>/topology/gateway/provider : Defines the configuration of a specific provider for the cluster.</p> <p>/topology/service : Defines the location of a specific Hadoop service within the Hadoop cluster.</p>"},{"location":"config/#provider-configuration","title":"Provider Configuration","text":"<p>Provider configuration is used to customize the behavior of a particular gateway feature. The general outline of a provider element looks like this.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;&lt;/name&gt;\n        &lt;value&gt;&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>/topology/gateway/provider : Groups information for a specific provider.</p> <p>/topology/gateway/provider/role : Defines the role of a particular provider. There are a number of pre-defined roles used by out-of-the-box provider plugins for the gateway. These roles are: authentication, identity-assertion, rewrite and hostmap</p> <p>/topology/gateway/provider/name : Defines the name of the provider for which this configuration applies. There can be multiple provider implementations for a given role. Specifying the name is used to identify which particular provider is being configured. Typically each topology descriptor should contain only one provider for each role but there are exceptions.</p> <p>/topology/gateway/provider/enabled : Allows a particular provider to be enabled or disabled via <code>true</code> or <code>false</code> respectively. When a provider is disabled any filters associated with that provider are excluded from the processing chain.</p> <p>/topology/gateway/provider/param : These elements are used to supply provider configuration. There can be zero or more of these per provider.</p> <p>/topology/gateway/provider/param/name : The name of a parameter to pass to the provider.</p> <p>/topology/gateway/provider/param/value : The value of a parameter to pass to the provider.</p>"},{"location":"config/#service-configuration","title":"Service Configuration","text":"<p>Service configuration is used to specify the location of services within the Hadoop cluster. The general outline of a service element looks like this.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;WEBHDFS&lt;/role&gt;\n    &lt;url&gt;http://localhost:50070/webhdfs&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>/topology/service : Provider information about a particular service within the Hadoop cluster. Not all services are necessarily exposed as gateway endpoints.</p> <p>/topology/service/role : Identifies the role of this service. Currently supported roles are: WEBHDFS, WEBHCAT, WEBHBASE, OOZIE, HIVE, NAMENODE, JOBTRACKER, RESOURCEMANAGER Additional service roles can be supported via plugins. Note: The role names are case sensitive and must be upper case.</p> <p>topology/service/url : The URL identifying the location of a particular service within the Hadoop cluster.</p>"},{"location":"config/#hostmap-provider","title":"Hostmap Provider","text":"<p>The purpose of the Hostmap provider is to handle situations where hosts are known by one name within the cluster and another name externally. This frequently occurs when virtual machines are used and in particular when using cloud hosting services. Currently, the Hostmap provider is configured as part of the topology file. The basic structure is shown below.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        ...\n        &lt;provider&gt;\n            &lt;role&gt;hostmap&lt;/role&gt;\n            &lt;name&gt;static&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;&lt;name&gt;external-host-name&lt;/name&gt;&lt;value&gt;internal-host-name&lt;/value&gt;&lt;/param&gt;\n        &lt;/provider&gt;\n        ...\n    &lt;/gateway&gt;\n    ...\n&lt;/topology&gt;\n</code></pre> <p>This mapping is required because the Hadoop services running within the cluster are unaware that they are being accessed from outside the cluster. Therefore URLs returned as part of REST API responses will typically contain internal host names. Since clients outside the cluster will be unable to resolve those host name they must be mapped to external host names.</p>"},{"location":"config/#hostmap-provider-example-ec2","title":"Hostmap Provider Example - EC2","text":"<p>Consider an EC2 example where two VMs have been allocated. Each VM has an external host name by which it can be accessed via the internet. However the EC2 VM is unaware of this external host name and instead is configured with the internal host name.</p> <pre><code>External HOSTNAMES:\nec2-23-22-31-165.compute-1.amazonaws.com\nec2-23-23-25-10.compute-1.amazonaws.com\n\nInternal HOSTNAMES:\nip-10-118-99-172.ec2.internal\nip-10-39-107-209.ec2.internal\n</code></pre> <p>The Hostmap configuration required to allow access external to the Hadoop cluster via the Apache Knox Gateway would be this:</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        ...\n        &lt;provider&gt;\n            &lt;role&gt;hostmap&lt;/role&gt;\n            &lt;name&gt;static&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;ec2-23-22-31-165.compute-1.amazonaws.com&lt;/name&gt;\n                &lt;value&gt;ip-10-118-99-172.ec2.internal&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;ec2-23-23-25-10.compute-1.amazonaws.com&lt;/name&gt;\n                &lt;value&gt;ip-10-39-107-209.ec2.internal&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n        ...\n    &lt;/gateway&gt;\n    ...\n&lt;/topology&gt;\n</code></pre>"},{"location":"config/#hostmap-provider-example-sandbox","title":"Hostmap Provider Example - Sandbox","text":"<p>The Hortonworks Sandbox 2.x poses a different challenge for host name mapping. This version of the Sandbox uses port mapping to make the Sandbox VM appear as though it is accessible via localhost. However the Sandbox VM is internally configured to consider sandbox.hortonworks.com as the host name. So from the perspective of a client accessing Sandbox the external host name is localhost. The Hostmap configuration required to allow access to Sandbox from the host operating system is this.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        ...\n        &lt;provider&gt;\n            &lt;role&gt;hostmap&lt;/role&gt;\n            &lt;name&gt;static&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;localhost&lt;/name&gt;\n                &lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n        ...\n    &lt;/gateway&gt;\n    ...\n&lt;/topology&gt;\n</code></pre>"},{"location":"config/#hostmap-provider-configuration","title":"Hostmap Provider Configuration","text":"<p>Details about each provider configuration element is enumerated below.</p> <p>topology/gateway/provider/role : The role for a Hostmap provider must always be <code>hostmap</code>.</p> <p>topology/gateway/provider/name : The Hostmap provider supplied out-of-the-box is selected via the name <code>static</code>.</p> <p>topology/gateway/provider/enabled : Host mapping can be enabled or disabled by providing <code>true</code> or <code>false</code>.</p> <p>topology/gateway/provider/param : Host mapping is configured by providing parameters for each external to internal mapping.</p> <p>topology/gateway/provider/param/name : The parameter names represent the external host names associated with the internal host names provided by the value element. This can be a comma separated list of host names that all represent the same physical host. When mapping from internal to external host name the first external host name in the list is used.</p> <p>topology/gateway/provider/param/value : The parameter values represent the internal host names associated with the external host names provider by the name element. This can be a comma separated list of host names that all represent the same physical host. When mapping from external to internal host names the first internal host name in the list is used.</p>"},{"location":"config/#simplified-topology-descriptors","title":"Simplified Topology Descriptors","text":"<p>Simplified descriptors are a means to facilitate provider configuration sharing and service endpoint discovery. Rather than editing an XML topology descriptor, it's possible to create a simpler YAML (or JSON) descriptor specifying the desired contents of a topology, which will yield a full topology descriptor and deployment.</p>"},{"location":"config/#externalized-provider-configurations","title":"Externalized Provider Configurations","text":"<p>Sometimes, the same provider configuration is applied to multiple Knox topologies. With the provider configuration externalized from the simple descriptors, a single configuration can be referenced by multiple topologies. This helps reduce the duplication of configuration, and the need to update multiple configuration files when a policy change is required. Updating a provider configuration will trigger an update to all those topologies that reference it.</p> <p>The contents of externalized provider configuration details are identical to the contents of the gateway element from a full topology descriptor. The only difference is that those details are defined in a separate JSON/YAML file in <code>{GATEWAY_HOME}/conf/shared-providers/</code>, which is then referenced by one or more descriptors.</p> <p>Provider Configuration Example</p> <pre><code>{\n  \"providers\": [\n    {\n      \"role\": \"authentication\",\n      \"name\": \"ShiroProvider\",\n      \"enabled\": \"true\",\n      \"params\": {\n        \"sessionTimeout\": \"30\",\n        \"main.ldapRealm\": \"org.apache.knox.gateway.shirorealm.KnoxLdapRealm\",\n        \"main.ldapContextFactory\": \"org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory\",\n        \"main.ldapRealm.contextFactory\": \"$ldapContextFactory\",\n        \"main.ldapRealm.userDnTemplate\": \"uid={0},ou=people,dc=hadoop,dc=apache,dc=org\",\n        \"main.ldapRealm.contextFactory.url\": \"ldap://localhost:33389\",\n        \"main.ldapRealm.contextFactory.authenticationMechanism\": \"simple\",\n        \"urls./**\": \"authcBasic\"\n      }\n    },\n    {\n      \"name\": \"static\",\n      \"role\": \"hostmape\",\n      \"enabled\": \"true\",\n      \"params\": {\n        \"localhost\": \"sandbox,sandbox.hortonworks.com\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"config/#sharing-ha-providers","title":"Sharing HA Providers","text":"<p>HA Providers are a special concern with respect to sharing provider configuration because they include service-specific (and possibly cluster-specific) configuration.</p> <p>This requires extra attention because the service configurations corresponding to the associated HA Provider configuration must contain the correct content to function properly.</p> <p>For a shared provider configuration with an HA Provider service:</p> <ul> <li>If the referencing descriptor does not declare the corresponding service, then the HA Provider configuration is effectively ignored since the service isn't exposed by the topology.</li> <li>If a corresponding service is declared in the descriptor<ul> <li>If service endpoint discovery is employed, then Knox should populate the URLs correctly to support the HA behavior.</li> <li>Otherwise, the URLs must be explicitly specified for that service in the descriptor.</li> </ul> </li> <li>If the descriptor content is correct, but the cluster service is not configured for HA, then the HA behavior obviously won't work.</li> </ul> <p>Apache ZooKeeper-based HA Provider Services</p> <p>The HA Provider configuration for some services (e.g., HiveServer2, Kafka) includes references to Apache ZooKeeper hosts (i.e., the ZooKeeper ensemble) and namespaces. It's important to understand the relationship of that ensemble configuration to the topologies referencing it. These ZooKeeper details are often cluster-specific. If the ZooKeeper ensemble in the provider configuration is part of cluster A, then it's probably incorrect to reference it in a topology for cluster B since the Hadoop service endpoints will probably be the wrong ones. However, if multiple clusters are working with a common ZooKeeper ensemble, then sharing this provider configuration may be appropriate.</p> <p>It's always best to specify cluster-specific details in a descriptor rather than a provider configuration.</p> <p>All of the service attributes, which can be specified in the HaProvider, can also be specified as params in the corresponding service declaration in the descriptor. If an attribute is specified in both the service declaration and the HaProvider, then the service-level value overrides the HaProvider-level value.</p> <pre><code>\"services\": [\n  {\n    \"name\": \"HIVE\",\n    \"params\": {\n      \"enabled\": \"true\",\n      \"zookeeperEnsemble\": \"host1:2181,host2:2181,host3:2181\",\n      \"zookeeperNamespace\" : \"hiveserver2\",\n      \"maxRetryAttempts\" : \"100\"\n    }\n  }\n]\n</code></pre> <p>Note that Knox can dynamically determine these ZooKeeper ensemble details for some services; for others, they are static provider configuration details. The services for which Knox can discover the cluster-specific ZooKeeper details include:</p> <ul> <li>YARN</li> <li>HIVE</li> <li>WEBHDFS</li> <li>WEBHBASE</li> <li>WEBHCAT</li> <li>OOZIE</li> <li>ATLAS</li> <li>ATLAS-API</li> <li>KAFKA</li> </ul> <p>For a subset of these supported services, Knox can also determine whether ZooKeeper-based HA is enabled or not. This means that the enabled attribute of the HA Provider configuration for these services may be set to auto, and Knox will determine whether or not it is enabled based on that service's configuration in the target cluster.</p> <pre><code>{\n  \"providers\": [\n    {\n      \"role\": \"ha\",\n      \"name\": \"HaProvider\",\n      \"enabled\": \"true\",\n      \"params\": {\n        \"WEBHDFS\": \"maxFailoverAttempts=3;failoverSleep=1000;maxRetryAttempts=3;retrySleep=1000;enabled=true\",\n        \"HIVE\": \"maxFailoverAttempts=10;failoverSleep=1000;maxRetryAttempts=5;retrySleep=1000;enabled=auto\",\n        \"YARN\": \"maxFailoverAttempts=5;failoverSleep=5000;maxRetryAttempts=3;retrySleep=1000;enabled=auto\"\n      }\n    }\n  ]\n}\n</code></pre> <p>These services include:</p> <ul> <li>YARN</li> <li>HIVE</li> <li>ATLAS</li> <li>ATLAS-API</li> </ul> <p>Be sure to pay extra attention when sharing HA Provider configuration across topologies.</p>"},{"location":"config/#simplified-descriptor-files","title":"Simplified Descriptor Files","text":"<p>Simplified descriptors allow service URLs to be defined explicitly, just like full topology descriptors. However, if URLs are omitted for a service, Knox will attempt to discover that service's URLs from the Hadoop cluster. Currently, this behavior is only supported for clusters managed by Apache Ambari or Cloudera Manager. In any case, the simplified descriptors are much more concise than a full topology descriptor.</p> <p>Descriptor Properties</p> Property Description <code>discovery-type</code> The discovery source type. (Currently, the only supported types are <code>AMBARI</code> and <code>ClouderaManager</code>). <code>discovery-address</code> The endpoint address for the discovery source. <code>discovery-user</code> The username with permission to access the discovery source. Optional, if the discovery type supports default credential aliases. <code>discovery-pwd-alias</code> The alias of the password for the user with permission to access the discovery source. Optional, if the discovery type supports default credential aliases. <code>provider-config-ref</code> A reference to a provider configuration in <code>{GATEWAY_HOME}/conf/shared-providers/</code>. <code>cluster</code> The name of the cluster from which the topology service endpoints should be determined. <code>services</code> The collection of services to be included in the topology. <code>applications</code> The collection of applications to be included in the topology. <p>Two file formats are supported for two distinct purposes.</p> <ul> <li>YAML is intended for the individual hand-editing a simplified descriptor because of its readability.</li> <li>JSON is intended to be used for API interactions.</li> </ul> <p>That being said, there is nothing preventing the hand-editing of files in the JSON format. However, the API will not accept YAML files as input.</p> <p>YAML Example (based on the HDP Docker Sandbox)</p> <pre><code>---\n# Discovery source config\ndiscovery-type : AMBARI\ndiscovery-address : http://sandbox.hortonworks.com:8080\n\n# If this is not specified, the alias ambari.discovery.user is checked for a username\ndiscovery-user : maria_dev\n\n# If this is not specified, the default alias ambari.discovery.password is used\ndiscovery-pwd-alias : sandbox.discovery.password\n\n# Provider config reference, the contents of which will be included in the resulting topology descriptor\nprovider-config-ref : sandbox-providers\n\n# The cluster for which the details should be discovered\ncluster: Sandbox\n\n# The services to declare in the resulting topology descriptor, whose URLs will be discovered (unless a value is specified)\nservices:\n    - name: NAMENODE\n    - name: JOBTRACKER\n    - name: WEBHDFS\n    - name: WEBHCAT\n    - name: OOZIE\n    - name: WEBHBASE\n    - name: HIVE\n    - name: RESOURCEMANAGER\n    - name: KNOXSSO\n      params:\n          knoxsso.cookie.secure.only: true\n          knoxsso.token.ttl: 100000\n    - name: AMBARI\n      urls:\n          - http://sandbox.hortonworks.com:8080\n    - name: AMBARIUI\n      urls:\n          - http://sandbox.hortonworks.com:8080\n    - name: AMBARIWS\n      urls:\n          - ws://sandbox.hortonworks.com:8080\n</code></pre> <p>JSON Example (based on the HDP Docker Sandbox)</p> <pre><code>{\n  \"discovery-type\":\"AMBARI\",\n  \"discovery-address\":\"http://sandbox.hortonworks.com:8080\",\n  \"discovery-user\":\"maria_dev\",\n  \"discovery-pwd-alias\":\"sandbox.discovery.password\",\n  \"provider-config-ref\":\"sandbox-providers\",\n  \"cluster\":\"Sandbox\",\n  \"services\":[\n    {\"name\":\"NAMENODE\"},\n    {\"name\":\"JOBTRACKER\"},\n    {\"name\":\"WEBHDFS\"},\n    {\"name\":\"WEBHCAT\"},\n    {\"name\":\"OOZIE\"},\n    {\"name\":\"WEBHBASE\"},\n    {\"name\":\"HIVE\"},\n    {\"name\":\"RESOURCEMANAGER\"},\n    {\"name\":\"KNOXSSO\",\n      \"params\":{\n      \"knoxsso.cookie.secure.only\":\"true\",\n      \"knoxsso.token.ttl\":\"100000\"\n      }\n    },\n    {\"name\":\"AMBARI\", \"urls\":[\"http://sandbox.hortonworks.com:8080\"]},\n    {\"name\":\"AMBARIUI\", \"urls\":[\"http://sandbox.hortonworks.com:8080\"],\n    {\"name\":\"AMBARIWS\", \"urls\":[\"ws://sandbox.hortonworks.com:8080\"]}\n  ]\n}\n</code></pre> <p>Both of these examples illustrate the specification of credentials for the interaction with the discovery source. If no credentials are specified, then the default aliases are queried (if default aliases are supported for that discovery type). Use of the default aliases is sufficient for scenarios where Knox will only discover topology details from a single source. For multiple discovery sources however, it's most likely that each will require different sets of credentials. The discovery-user and discovery-pwd-alias properties exist for this purpose. Note that whether using the default credential aliases or specifying a custom password alias, these aliases must be defined prior to any attempt to deploy a topology using a simplified descriptor.</p>"},{"location":"config/#cluster-discovery-provider-extensions","title":"Cluster Discovery Provider Extensions","text":"<p>To support the ability to dynamically discover the endpoints for services being proxied, Knox provides cluster discovery provider extensions for Apache Ambari and Cloudera Manager. The Ambari support has been available since Knox 1.1.0, and limited support for Cloudera Manager has been added in Knox 1.3.0.</p> <p>These extensions allow discovery sources of the respective types to be queried for cluster details used to generate topologies from simplified descriptors. The extension to be employed is specified on a per-descriptor basis, using the <code>discovery-type</code> descriptor property.</p>"},{"location":"config/#deployment-directories","title":"Deployment Directories","text":"<p>Effecting topology changes is as simple as modifying files in two specific directories.</p> <p>The <code>{GATEWAY_HOME}/conf/shared-providers/</code> directory is the location where Knox looks for provider configurations. This directory is monitored for changes, such that modifying a provider configuration file therein will trigger updates to any referencing simplified descriptors in the <code>{GATEWAY_HOME}/conf/descriptors/</code> directory. Care should be taken when deleting these files if there are referencing descriptors; any subsequent modifications of referencing descriptors will fail when the deleted provider configuration cannot be found. The references should all be modified before deleting the provider configuration.</p> <p>Likewise, the <code>{GATEWAY_HOME}/conf/descriptors/</code> directory is monitored for changes, such that adding or modifying a simplified descriptor file in this directory will trigger the generation and deployment of a topology descriptor. Deleting a descriptor from this directory will conversely result in the removal of the previously-generated topology descriptor, and the associated topology will be undeployed.</p> <p>If the service details for a deployed (generated) topology are changed in the cluster, then the Knox topology can be updated by 'touch'ing the simplified descriptor. This will trigger discovery and regeneration/redeployment of the topology descriptor.</p> <p>Note that deleting a generated topology descriptor from <code>{GATEWAY_HOME}/conf/topologies/</code> is not sufficient for its removal. If the source descriptor is modified, or Knox is restarted, the topology descriptor will be regenerated and deployed. Removing generated topology descriptors should be done by removing the associated simplified descriptor. For the same reason, editing generated topology descriptors is strongly discouraged since they can be inadvertently overwritten.</p> <p>Another means by which these topology changes can be effected is the Admin API.</p>"},{"location":"config/#cloud-federation-configuration","title":"Cloud Federation Configuration","text":"<p>Cloud Federation feature allows for a topology based federation from one Knox instance to another (from on-prem Knox instance to cloud knox instance).</p>"},{"location":"config/#cluster-configuration-monitoring","title":"Cluster Configuration Monitoring","text":"<p>Another benefit gained through the use of simplified topology descriptors, and the associated service discovery, is the ability to monitor clusters for configuration changes. This is currently only available for clusters managed by Ambari and ClouderaManager.</p> <p>The gateway can monitor cluster configurations, and respond to changes by dynamically regenerating and redeploying the affected topologies. The following properties in gateway-site.xml can be used to control this behavior.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.cluster.config.monitor.ambari.enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n    &lt;description&gt;Enable/disable Ambari cluster configuration monitoring.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cluster.config.monitor.ambari.interval&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n    &lt;description&gt;The interval (in seconds) for polling Ambari for cluster configuration changes.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;!-- Cloudera Manager specific configuration --&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cluster.config.monitor.cm.enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n    &lt;description&gt;Enable/disable Cloudera Manager cluster configuration monitoring.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cluster.config.monitor.cm.interval&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n    &lt;description&gt;The interval (in seconds) for polling Cloudera Manager for cluster configuration changes.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cloudera.manager.descriptors.monitor.interval&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n    &lt;description&gt;The interval (in milliseconds) for monitoring Hadoop XML resources in \"GATEWAY_HOME/data/descriptors\" with `.hxr` file postfix (see details below). If this property is set to a non-positive integer, monitoring of Hadoop XML resources is disabled.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cloudera.manager.advanced.service.discovery.config.monitor.interval&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n    &lt;description&gt;The interval (in milliseconds) for monitoring GATEWAY_HOME/conf/auto-discovery-advanced-configuration.properties (if exists) and notifies any AdvancedServiceDiscoveryConfigChangeListener if the file is changed since the last time it was loaded. Advanced configuration processing in Knox's service discovery flow helps fine-tuned service-level enablement on the topology level (see details below). If this property is set to a non-positive integer, this feature is disabled.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cloudera.manager.service.discovery.maximum.retry.attemps&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n    &lt;description&gt;The maximum number of attempts Knox will try to connect to the configured Cloudera Manager cluster if there was a connection error.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.cloudera.manager.service.discovery.repository.cache.entry.ttl&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n    &lt;description&gt;Upon a successful Cloudera Manager service discovery event, Knox maintains an in-memory cache (repository) of discovered data (cluster, service, and role configs). This property indicates the entry TTL of this cache. See KNOX-2680 for more details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Since service discovery supports multiple Ambari or ClouderaManager instances as discovery sources, multiple instances can be monitored for cluster configuration changes.</p> <p>For example, if the cluster monitor is enabled, deployment of the following simple descriptor would trigger monitoring of the Sandbox cluster managed by Ambari @ http://sandbox.hortonworks.com:8080</p> <pre><code>---\ndiscovery-address : http://sandbox.hortonworks.com:8080\ndiscovery-user : maria_dev\ndiscovery-pwd-alias : sandbox.discovery.password\ncluster: Sandbox\nprovider-config-ref : sandbox-providers\nservices:\n    - name: NAMENODE\n    - name: JOBTRACKER\n    - name: WEBHDFS\n    - name: WEBHCAT\n    - name: OOZIE\n    - name: WEBHBASE\n    - name: HIVE\n    - name: RESOURCEMANAGER\n</code></pre> <p>Another Sandbox cluster, managed by a different Ambari instance, could simultaneously be monitored by the same gateway instance.</p> <p>Now, topologies can be kept in sync with their respective target cluster configurations, without administrator intervention or service interruption.</p>"},{"location":"config/#hadoop-xml-resource-monitoring","title":"Hadoop XML resource monitoring","text":"<p>As described in KNOX-2160, to support topology management in Cloudera Manager it's beneficial that Knox is able to process a descriptor that CM can generate natively. As of now, Cloudera Manager's CSD framework is capable of producing a file of its parameters in the following formats:  - Hadoop XML  - properties  - gflags</p> <p>As the <code>gateway-site.xml</code> uses the Hadoop XML format it's quite obvious that the first option is the one that fits Knox the most. One XML type descriptor file may contain one or more Knox descriptors using the following structure:</p> <ul> <li>the configuration <code>name</code> would indicate the descriptor (topology) name</li> <li>the configuration <code>value</code> would list all properties of a Knox    descriptor<ul> <li>service discovery related information (type, address,    cluster, user/password alias)</li> <li>services<ul> <li>name</li> <li>url</li> <li>version (optional)</li> <li>parameters (optional)</li> </ul> </li> <li>applications (optional)<ul> <li>name</li> <li>parameters (optional)</li> </ul> </li> </ul> </li> </ul> <p>A sample descriptor file would look like this:</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;topology1&lt;/name&gt;\n    &lt;value&gt;\n        discoveryType=ClouderaManager;\n        discoveryAddress=http://host:123;\n        discoveryUser=user;\n        discoveryPasswordAlias=alias;\n        cluster=Cluster 1;\n        providerConfigRef=topology1-provider;\n        app:knoxauth:param1.name=param1.value;\n        app:KNOX;\n        HIVE:url=http://localhost:389;\n        HIVE:version=1.0;\n        HIVE:httpclient.connectionTimeout=5m;\n        HIVE:httpclient.socketTimeout=100m\n    &lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;topology2&lt;/name&gt;\n    &lt;value&gt;\n        discoveryType=ClouderaManager;\n        discoveryAddress=http://host:123;\n        discoveryUser=user;\n        discoveryPasswordAlias=alias;\n        cluster=Cluster 1;\n        providerConfigRef=topology2-provider;\n        app:KNOX;\n        HDFS.url=https://localhost:443;\n        HDFS:httpclient.connectionTimeout=5m;\n        HDFS:httpclient.socketTimeout=100m\n     &lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Workflow:</p> <ol> <li>this kind of descriptor should also be placed in Knox's descriptor directory using a <code>.hxr</code> file prefix (e.g. <code>cm-descriptors.hxr</code>)</li> <li>once it's added or modified Knox's Hadoop XML resource monitor should parse the XML and build one or more instance(s) of  <code>org.apache.knox.gateway.topology.simple.SimpleDescriptor</code></li> <li>after the Java object(s) got created it (they) should be saved in the Knox descriptor directory in JSON format. As a result, the same monitor should parse the new/modified JSON descriptor(s) and re-deploys it (them) using the already existing mechanism</li> </ol>"},{"location":"config/#advanced-service-discovery-configuration-monitoring","title":"Advanced service discovery configuration monitoring","text":"<p>With the above-discussed Hadoop XML resource monitoring Knox is capable of processing a Hadoop XML configuration file and turn its content into Knox providers.</p> <p>Advanced service discovery configuration monitor extends that feature in a way it supports for the following use cases that are also Cloudera Manager integration specific:</p> <p>1.) Cloudera Manager reports if auto-discovery is enabled for each known services. That is, a list of boolean properties can be generated by CM indicating if <code>SERVICE_X</code> is enabled or not in the following form: <code>gateway.auto.discovery.enabled.SERVICE_NAME=[true|false]</code></p> <p>The new Hadoop XML configuration parser should take this information into account, and add a certain service into the generated Knox descriptor iff that service is explicitly enabled or there is no boolean flag within the CM generated properties with that service name (indicating an unknown - custom - service).</p> <p>Additionally, a set of known (expected) topologies can be listed by CM. These topologies (Knox descriptors) should be populated with an enabled service (only with its name) even if that particular service was not listed in the new style Hadoop XML configuration file within the descriptor that matches any of the expected topology names.</p> <p>2.) There are some services - mainly UI services - that are not working without some more required services in place (mainly their API counterpart). For instance: <code>RANGERUI</code> won't work properly if <code>RANGER</code> is not available.</p> <p>Knox's Hadoop XML configuration parser was modified to exclude any service from the generated Knox descriptor unless  - all required services are available (if any)  - all required services are enabled (see the previous point)</p>"},{"location":"config/#remote-configuration-monitor","title":"Remote Configuration Monitor","text":"<p>In addition to monitoring local directories for provider configurations and simplified descriptors, the gateway similarly supports monitoring either ZooKeeper or an SQL database.</p>"},{"location":"config/#zookeeper-based-monitor","title":"Zookeeper based monitor","text":"<p>This monitor depends on a remote configuration registry client, and that client must be specified by setting the following property in gateway-site.xml</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.service.remoteconfigurationmonitor.impl&lt;/name&gt;\n    &lt;value&gt;org.apache.knox.gateway.topology.monitor.db.ZkRemoteConfigurationMonitorService&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.monitor.client&lt;/name&gt;\n    &lt;value&gt;sandbox-zookeeper-client&lt;/value&gt;\n    &lt;description&gt;Remote configuration monitor client name.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>This client identifier is a reference to a remote configuration registry client, as in this example (also defined in gateway-site.xml)</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.registry.sandbox-zookeeper-client&lt;/name&gt;\n    &lt;value&gt;type=ZooKeeper;address=localhost:2181&lt;/value&gt;\n    &lt;description&gt;ZooKeeper configuration registry client details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>The actual name of the client (e.g., sandbox-zookeeper-client) is not important, except that the reference matches the name specified in the client definition.</p> <p>With this configuration, the gateway will monitor the following znodes in the specified ZooKeeper instance</p> <pre><code>/knox\n    /config\n        /shared-providers\n        /descriptors\n</code></pre> <p>The creation of these znodes, and the population of their respective contents, is an activity not currently managed by the gateway. However, the KNOX CLI includes commands for managing the contents of these znodes.</p> <p>These znodes are treated similarly to the local shared-providers and descriptors directories described in Deployment Directories. When the monitor notices a change to these znodes, it will attempt to effect the same change locally.</p> <p>If a provider configuration is added to the /knox/config/shared-providers znode, the monitor will download the new configuration to the local shared-providers directory. Likewise, if a descriptor is added to the /knox/config/descriptors znode, the monitor will download the new descriptor to the local descriptors directory, which will trigger an attempt to generate and deploy a corresponding topology.</p> <p>Modifications to the contents of these znodes, will yield the same behavior as can be seen resulting from the corresponding local modification.</p> znode action result <code>/knox/config/shared-providers</code> add Download the new file to the local shared-providers directory <code>/knox/config/shared-providers</code> modify Download the new file to the local shared-providers directory; If there are any existing descriptor references, then topology will be regenerated and redeployed for those referencing descriptors. <code>/knox/config/shared-providers</code> delete Delete the corresponding file from the local shared-providers directory <code>/knox/config/descriptors</code> add Download the new file to the local descriptors directory; A corresponding topology will be generated and deployed. <code>/knox/config/descriptors</code> modify Download the new file to the local descriptors directory; The corresponding topology will be regenerated and redeployed. <code>/knox/config/descriptors</code> delete Delete the corresponding file from the local descriptors directory <p>This simplifies the configuration for HA gateway deployments, in that the gateway instances can all be configured to monitor the same ZooKeeper instance, and changes to the znodes' contents will be applied to all those gateway instances. With this approach, it is no longer necessary to manually deploy topologies to each of the gateway instances.</p> <p>A Note About ACLs</p> <pre><code>While the gateway does not currently require secure interactions with remote registries, it is recommended\nthat ACLs be applied to restrict at least writing of the entries referenced by this monitor. If write\naccess is available to everyone, then the contents of the configuration cannot be known to be trustworthy,\nand there is the potential for malicious activity. Be sure to carefully consider who will have the ability\nto define configuration in monitored remote registries and apply the necessary measures to ensure its\ntrustworthiness.\n</code></pre>"},{"location":"config/#remote-configuration-registry-clients","title":"Remote Configuration Registry Clients","text":"<p>One or more features of the gateway employ remote configuration registry (e.g., ZooKeeper) clients. These clients are configured by setting properties in the gateway configuration (<code>gateway-site.xml</code>).</p> <p>Each client configuration is a single property, the name of which is prefixed with gateway.remote.config.registry. and suffixed by the client identifier. The value of such a property, is a registry-type-specific set of semicolon-delimited properties for that client, including the type of registry with which it will interact.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.registry.a-zookeeper-client&lt;/name&gt;\n    &lt;value&gt;type=ZooKeeper;address=zkhost1:2181,zkhost2:2181,zkhost3:2181&lt;/value&gt;\n    &lt;description&gt;ZooKeeper configuration registry client details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>In the preceeding example, the client identifier is a-zookeeper-client, by way of the property name gateway.remote.config.registry.a-zookeeper-client.</p> <p>The property value specifies that the client is intended to interact with ZooKeeper. It also specifies the particular ZooKeeper ensemble with which it will interact; this could be a single ZooKeeper instance as well.</p> <p>The property value may also include an optional namespace, to which the client will be restricted (i.e., \"chroot\" the client).</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.registry.a-zookeeper-client&lt;/name&gt;\n    &lt;value&gt;type=ZooKeeper;address=zkhost1:2181,zkhost2:2181,zkhost3:2181;namespace=/knox/config&lt;/value&gt;\n    &lt;description&gt;ZooKeeper configuration registry client details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>At least for the ZooKeeper type, authentication details may also be specified as part of the property value, for interacting with instances for which authentication is required.</p> <p>Digest Authentication Example</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.registry.a-zookeeper-client&lt;/name&gt;\n    &lt;value&gt;type=ZooKeeper;address=zkhost1:2181,zkhost2:2181,zkhost3:2181;authType=Digest;principal=myzkuser;credentialAlias=myzkpass&lt;/value&gt;\n    &lt;description&gt;ZooKeeper configuration registry client details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Kerberos Authentication Example</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.config.registry.a-zookeeper-client&lt;/name&gt;\n    &lt;value&gt;type=ZooKeeper;address=zkhost1:2181,zkhost2:2181,zkhost3:2181;authType=Kerberos;principal=myzkuser;keytab=/home/user/myzk.keytab;useKeyTab=true;useTicketCache=false&lt;/value&gt;\n    &lt;description&gt;ZooKeeper configuration registry client details.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>While multiple such clients can be configured, for ZooKeeper clients, there is currently a limitation with respect to authentication. Multiple clients cannot each have distinct authentication configurations. This limitation is imposed by the underlying ZooKeeper client. Therefore, the clients must all be insecure (no authentication configured), or they must all authenticate to the same ZooKeeper using the same credentials.</p> <p>The remote configuration monitor facility uses these client configurations to perform its function.</p>"},{"location":"config/#sql-databse-based-monitor","title":"SQL databse based monitor","text":"<p>The SQL based remote configuration monitor works like the Zookeeper monitor, but it monitors a relational database.</p> <p>Enabling this monitor requires setting gateway.service.remoteconfigurationmonitor.impl in gateway-site.xml to org.apache.knox.gateway.topology.monitor.db.DbRemoteConfigurationMonitorService.</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;gateway.service.remoteconfigurationmonitor.impl&lt;/name&gt;\n  &lt;value&gt;org.apache.knox.gateway.topology.monitor.db.DbRemoteConfigurationMonitorService&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Valid database settings need to be configured in gateway-site.xml. See \"Configuring the JDBC token state service\" for more information about the database configuration.</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;gateway.database.type&lt;/name&gt;\n  &lt;value&gt;mysql&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;gateway.database.connection.url&lt;/name&gt;\n  &lt;value&gt;jdbc:mysql://localhost:3306/knox&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>With this configuration, the gateway will periodically check the content of the KNOX_PROVIDERS and KNOX_DESCRIPTORS tables and it will modify (create, update or delete) the local files under the shared-providers and descriptors directories respectively.</p> <p>The interval (in seconds) at which the remote configuration monitor will poll the database is controlled by the following property.</p> <pre><code>gateway.remote.config.monitor.db.poll.interval.seconds\n</code></pre> <p>The default value is 30 seconds.    </p> <ul> <li> <p>If a remote configuration exists in the datbase but doesn't exist on the file system, then the monitor is going to create the file with corresponding content.</p> </li> <li> <p>If an existing remote configuration was deleted from the database (logical deletion) but it still exists on the local file system, then the monitor is going to delete the corresponding file. The logically deleted records are eventually cleared up by the monitor.</p> </li> <li> <p>If a remote configuration exists in the database with a different content than the local file, then the monitor is going to update the content of the local file with the content from the database. However to avoid unnecessary IO operations the monitor only updates the content once and if there were no further changes in the database since this last update time, then it will skip changing the local content until a new change happens in the database (indicated by the last_modified_time column). This means you can do temporary changes on the local file system without losing your modifications (until a change happens in the database).</p> </li> </ul> <p>The content of the database must be changed by the Knox Admin UI or the by the Admin API.</p>"},{"location":"config/#remote-alias-service","title":"Remote Alias Service","text":"<p>Knox can be configured to use a remote alias service. The remote alias service is pluggable to support multiple different backends. The feature can be disabled by setting the property <code>gateway.remote.alias.service.enabled</code> to <code>false</code> in <code>gateway-site.xml</code>. Knox needs to be restarted for this change to take effect.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.remote.alias.service.enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n    &lt;description&gt;Turn on/off Remote Alias service (true by default)&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>The type of remote alias service can be configured by default using <code>gateway.remote.alias.service.config.type</code>. If necessary the remote alias service config prefix can be changed with <code>gateway.remote.alias.service.config.prefix</code>. Changing the prefix affects all remote alias service configurations.</p>"},{"location":"config/#remote-alias-service-hashicorp-vault","title":"Remote Alias Service - HashiCorp Vault","text":"<p>The HashiCorp Vault remote alias service is deigned to store aliases into HashiCorp Vault. It is configured by setting <code>gateway.remote.alias.service.config.type</code> to <code>hashicorp.vault</code> in gateway-site.xml. The table below highlights configuration parameters for the HashiCorp Vault remote alias service. Knox needs to be restarted for this change to take effect.</p> Property Description <code>gateway.remote.alias.service.config.hashicorp.vault.address</code> Address of the HashiCorp Vault server <code>gateway.remote.alias.service.config.hashicorp.vault.secrets.engine</code> HashiCorp Vault secrets engine <code>gateway.remote.alias.service.config.hashicorp.vault.path.prefix</code> HashiCorp Vault secrets engine path prefix <p>There are multiple authentication mechanisms supported by HashiCorp Vault. Knox supports pluggable authentication mechanisms. The authentication type is configured by setting <code>gateway.remote.alias.service.config.hashicorp.vault.authentication.type</code> in gateway-site.xml.</p> <p>Token Authentication</p> <p>Token authentication takes a single setting <code>gateway.remote.alias.service.config.hashicorp.vault.authentication.token</code> and takes either the value of the authentication token or a local alias configured with <code>${ALIAS=token_name}</code>.</p> <p>Kubernetes Authentication</p> <p>Kubernetes authentication takes a single setting <code>gateway.remote.alias.service.config.hashicorp.vault.authentication.kubernetes.role</code> which defines the role to use when connecting to Vault. The Kubernetes authentication mechanism uses the secrets prepopulated into a K8S pod to authenticate to Vault. Knox can then use the secrets from Vault after being authenticated.</p>"},{"location":"config/#remote-alias-service-zookeeper","title":"Remote Alias Service - Zookeeper","text":"<p>The Zookeeper remote alias service is designed to store aliases into Apache Zookeeper. It supports monitoring for remote aliases that are added, deleted or updated. The Zookeeper remote alias service is configured by turning the Remote Configuration Monitor on and setting <code>gateway.remote.alias.service.config.type</code> to <code>zookeeper</code> in gateway-site.xml. Knox needs to be restarted for this change to take effect. </p>"},{"location":"config/#logging","title":"Logging","text":"<p>If necessary you can enable additional logging by editing the <code>log4j2.xml</code> file in the <code>conf</code> directory. Changing the <code>rootLogger</code> value from <code>ERROR</code> to <code>DEBUG</code> will generate a large amount of debug logging. A number of useful, more fine loggers are also provided in the file.</p> <p>With the 2.0 release, Knox uses Log4j 2 instead of Log4j 1. Apache Log4j 2 is the successor of Log4j 1, but it is incompatible with its predecessor. The main differene is that from now on Knox uses XML file format for configuring logging properties instead of <code>.property</code> files.</p> <p>If you have an existing deployment with customized Log4j 1 <code>.property</code> files you will need to convert them to the new Log4j 2 format.</p>"},{"location":"config/#launcher-changes","title":"Launcher changes","text":"<p>The JVM property name that specifies the location of the Log4j configuration file, was changed to <code>log4j.configurationFile</code>.</p> <p>For example</p> <pre><code>-Dlog4j.configurationFile=gateway-log4j2.xml\n</code></pre> <p>Instead of</p> <pre><code>-Dlog4j.configuration=conf/gateway-log4j.properties\n</code></pre>"},{"location":"config/#configuration-file-changes","title":"Configuration file changes","text":"<p>Log4j 2 uses a different configuration file format than Log4j 1.</p> <pre><code>app.log.dir=logs\napp.log.file=${launcher.name}.log\nlog4j.rootLogger=ERROR, drfa\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.drfa=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.drfa.File=${app.log.dir}/${app.log.file}\nlog4j.appender.drfa.DatePattern=.yyyy-MM-dd\nlog4j.appender.drfa.layout=org.apache.log4j.PatternLayout\nlog4j.appender.drfa.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\nlog4j.logger.org.apache.http.impl.conn=INFO\nlog4j.logger.org.apache.http.impl.client=INFO\nlog4j.logger.org.apache.http.client=INFO\n</code></pre> <p>For example the equivalent of the above Log4j 1 config file looks like this:</p> <pre><code>&lt;Configuration&gt;\n    &lt;Properties&gt;\n        &lt;Property name=\"app.log.dir\"&gt;logs&lt;/Property&gt;\n        &lt;Property name=\"app.log.file\"&gt;${sys:launcher.name}.log&lt;/Property&gt;\n    &lt;/Properties&gt;\n    &lt;Appenders&gt;\n        &lt;Console name=\"stdout\" target=\"SYSTEM_OUT\"&gt;\n            &lt;PatternLayout pattern=\"%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\" /&gt;\n        &lt;/Console&gt;\n        &lt;RollingFile name=\"drfa\" fileName=\"${app.log.dir}/${app.log.file}\" filePattern=\"${app.log.dir}/${app.log.file}.%d{yyyy-MM-dd}\"&gt;\n            &lt;PatternLayout pattern=\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\" /&gt;\n            &lt;TimeBasedTriggeringPolicy /&gt;\n        &lt;/RollingFile&gt;\n    &lt;/Appenders&gt;\n    &lt;Loggers&gt;\n        &lt;Logger name=\"org.apache.http.impl.client\" level=\"INFO\" /&gt;\n        &lt;Logger name=\"org.apache.http.client\" level=\"INFO\" /&gt;\n        &lt;Logger name=\"org.apache.http.impl.conn\" level=\"INFO\" /&gt;\n        &lt;Root level=\"ERROR\"&gt;\n            &lt;AppenderRef ref=\"drfa\" /&gt;\n        &lt;/Root&gt;\n    &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre> <p>Log levels can be set on individual Java packages similarly as before. The Loggers inherit properties like logging level and appender types from their ancestors.</p>"},{"location":"config/#log4j-migration","title":"Log4j Migration","text":"<p>If you have an existing Knox installation which uses the default Logging settings, with no customizations, then you can simply upgrade to the new version and overwrite the <code>*-log4j.property</code> files with the <code>*-log4j2.xml</code> files.</p> <p>Old Log4j 1.x property files:</p> <pre><code>-rw-r--r--  1 user  group  3880 Sep 10 10:49 gateway-log4j.properties\n-rw-r--r--  1 user  group  1481 Sep 10 10:49 knoxcli-log4j.properties\n-rw-r--r--  1 user  group  1493 Sep 10 10:49 ldap-log4j.properties\n-rw-r--r--  1 user  group  1436 Sep 10 10:49 shell-log4j.properties\n</code></pre> <p>The new Log4j 2 XML configuration files:</p> <pre><code>-rw-r--r--  1 user  group  4619 Jan 22  2020 gateway-log4j2.xml\n-rw-r--r--  1 user  group  1684 Jan 22  2020 knoxcli-log4j2.xml\n-rw-r--r--  1 user  group  1765 Jan 22  2020 ldap-log4j2.xml\n-rw-r--r--  1 user  group  1621 Jan 22  2020 shell-log4j2.xml\n</code></pre> <p>If you have a lot of customizations in place, you will need to convert the property files to XML file format.</p> <p>There is a third party script that helps you with the conversion:</p> <pre><code>https://github.com/mulesoft-labs/log4j2-migrator\n</code></pre> <p>The final result is not always 100% correct, you might need to do some manual adjustments.</p> <p>Usage:</p> <pre><code>$ groovy log4j2migrator.groovy log4j.properties &gt; log4j2.xml\n</code></pre> <p>The scripts uses \u200b\u200bAsyncLoggers by default which requires the com.lmax:disruptor library which is not distributed with Knox by default. </p> <p>To avoid having this dependency you can replace all the <code>AsyncLogger</code> tags to <code>Logger</code>s.</p> <pre><code> $ groovy log4j2migrator.groovy log4j.properties &gt; log4j2.xml | sed 's/AsyncLogger/Logger/g'\n</code></pre> <p>Pay attention to the custom date time patterns in the configuration file. The script doesn't always convert them properly.</p> <p>You can find more information about the Log4j 2 configuration file format at here: https://logging.apache.org/log4j/2.x/manual/configuration.html#XML</p>"},{"location":"config/#custom-appenders-and-layouts","title":"Custom Appenders and Layouts","text":"<p>Custom Appenders or Layouts based on the Log4j 1 API, are not going to work with Log4j 2. Those need to be rewritten using the new API.</p> <p>The <code>AppenderSkeleton</code> class does not exist in Log4j 2, you should extend from <code>AbstractAppender</code> instead.</p> <p>You can read more about extending Log4j 2 at: https://logging.apache.org/log4j/2.x/manual/extending.html</p>"},{"location":"config/#java-vm-options","title":"Java VM Options","text":"<p>TODO - Java VM options doc.</p>"},{"location":"config/#persisting-the-master-secret","title":"Persisting the Master Secret","text":"<p>The master secret is required to start the server. This secret is used to access secured artifacts by the gateway instance. By default, the keystores, trust stores, and credential stores are all protected with the master secret. However, if a custom keystore is set, it and the contained keys may have different passwords. </p> <p>You may persist the master secret by supplying the -persist-master switch at startup. This will result in a warning indicating that persisting the secret is less secure than providing it at startup. We do make some provisions in order to protect the persisted password.</p> <p>It is encrypted with AES 128 bit encryption and where possible the file permissions are set to only be accessible by the user that the gateway is running as.</p> <p>After persisting the secret, ensure that the file at <code>data/security/master</code> has the appropriate permissions set for your environment. This is probably the most important layer of defense for master secret. Do not assume that the encryption is sufficient protection.</p> <p>A specific user should be created to run the gateway. This user will be the only user with permissions for the persisted master file.</p> <p>See the Knox CLI section for descriptions of the command line utilities related to the master secret.</p>"},{"location":"config/#management-of-security-artifacts","title":"Management of Security Artifacts","text":"<p>There are a number of artifacts that are used by the gateway in ensuring the security of wire level communications, access to protected resources and the encryption of sensitive data. These artifacts can be managed from outside of the gateway instances or generated and populated by the gateway instance itself.</p> <p>The following is a description of how this is coordinated with both standalone (development, demo, etc.) gateway instances and instances as part of a cluster of gateways in mind.</p> <p>Upon start of the gateway server:</p> <ol> <li>Look for a credential store at <code>data/security/keystores/__gateway-credentials.jceks</code>.    This credential store is used to store secrets/passwords that are used by the gateway.    For instance, this is where the passphrase for accessing the gateway's TLS certificate is kept.<ul> <li>If the credential store is not found, then one is created and encrypted with the provided master secret.</li> <li>If the credential store is found, then it is loaded using the provided master secret.</li> </ul> </li> <li>Look for an identity keystore at the configured location. If a configured location was not set, the     default location, <code>data/security/keystores/gateway.jks</code>, will be used. The identity keystore contains     the certificate and private key used to represent the identity of the server for TLS/SSL connections.   <ul> <li>If the identity keystore was not found, one is created in the configured or default location.   Then a self-signed certificate for use in standalone/demo mode is generated and stored with the    configured identity alias or the default alias of \"gateway-identity\".</li> <li>If the identity keystore is found, then it is loaded using either the password found in the    credential store under the configured alias name or the provided master secret. It is tested    to ensure that a certificate and private key are found using the configured alias name or   the default alias of \"gateway-identity\".</li> </ul> </li> <li>Look for a signing keystore at the configured location or the default location of <code>data/security/keystores/gateway.jks</code>.    The signing keystore contains the public and private key used for signature creation.<ul> <li>If the signing keystore was not found, the server will fail to start.  </li> <li>If the signing keystore is found, then it is loaded using either the password found in the    credential store under the configured alias name or the provided master secret. It is tested    to ensure that a public and private key are found using the configured alias name or   the default alias of \"gateway-identity\".</li> </ul> </li> </ol> <p>Upon deployment of a Hadoop cluster topology within the gateway we:</p> <ol> <li>Look for a credential store for the topology. For instance, we have a sample topology that gets deployed out of the box.  We look for <code>data/security/keystores/sandbox-credentials.jceks</code>. This topology specific credential store is used for storing secrets/passwords that are used for encrypting sensitive data with topology specific keys.<ul> <li>If no credential store is found for the topology being deployed then one is created for it.   Population of the aliases is delegated to the configured providers within the system that will require the use of a secret for a particular task.   They may programmatically set the value of the secret or choose to have the value for the specified alias generated through the AliasService.</li> <li>If a credential store is found then we ensure that it can be loaded with the provided master secret and the configured providers have the opportunity to ensure that the aliases are populated and if not to populate them.</li> </ul> </li> </ol> <p>By leveraging the algorithm described above we can provide a window of opportunity for management of these artifacts in a number of ways.</p> <ol> <li>Using a single gateway instance as a master instance the artifacts can be generated or placed into the expected location and then replicated across all of the slave instances before startup.</li> <li>Using an NFS mount as a central location for the artifacts would provide a single source of truth without the need to replicate them over the network. Of course, NFS mounts have their own challenges.</li> <li>Using the KnoxCLI to create and manage the security artifacts.</li> </ol> <p>See the Knox CLI section for descriptions of the command line utilities related to the security artifact management.</p>"},{"location":"config/#keystores","title":"Keystores","text":"<p>In order to provide your own certificate for use by the Gateway, you may either</p> <ul> <li>provide your own keystore </li> <li>import an existing key pair into the default Gateway keystore </li> <li>generate a self-signed cert using the Java keytool</li> </ul>"},{"location":"config/#providing-your-own-keystore","title":"Providing your own keystore","text":"<p>A keystore in one of the following formats may be specified: </p> <ul> <li>JKS: Java KeyStore file</li> <li>JCEKS: Java Cryptology Extension KeyStore file</li> <li>PKCS12: PKCS #12 file</li> </ul> <p>See <code>gateway.tls.keystore.password.alias</code>, <code>gateway.tls.keystore.path</code>, <code>gateway.tls.keystore.type</code>,  <code>gateway.tls.key.alias</code>, and <code>gateway.tls.key.passphrase.alias</code> under Gateway Server Configuration for information on configuring the Gateway to use this keystore.</p>"},{"location":"config/#importing-a-key-pair-into-a-java-keystore","title":"Importing a key pair into a Java keystore","text":"<p>One way to accomplish this is to start with a PKCS12 store for your key pair and then convert it to a Java keystore or JKS.</p> <p>The following example uses OpenSSL to create a PKCS12 encoded store from your provided certificate and private key that are in PEM format.</p> <pre><code>openssl pkcs12 -export -in cert.pem -inkey key.pem &gt; server.p12\n</code></pre> <p>The next example converts the PKCS12 store into a Java keystore (JKS). It should prompt you for the  keystore and key passwords for the destination keystore. You must use either the master-secret for the  keystore password and key passphrase or choose you own.  If you choose your own passwords, you must  use the Knox CLI utility to provide them to the Gateway.  </p> <pre><code>keytool -importkeystore -srckeystore server.p12 -destkeystore gateway.jks -srcstoretype pkcs12\n</code></pre> <p>While using this approach a couple of important things to be aware of:</p> <ol> <li> <p>The alias MUST be properly set. If it is not the default value (\"gateway-identity\"), it must be     set in the configuration using <code>gateway.tls.key.alias</code>. You may need to change it using keytool     after the import of the PKCS12 store. You can use keytool to do this - for example:</p> <pre><code>keytool -changealias -alias \"1\" -destalias \"gateway-identity\" -keystore gateway.jks -storepass {knoxpw}\n</code></pre> </li> <li> <p>The path to the identity keystore for the gateway MUST be the default path     (<code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code>) or MUST be specified in the configuration     using <code>gateway.tls.keystore.path</code> and <code>gateway.tls.keystore.type</code> </p> </li> <li>The passwords for the keystore and the imported key may both be set to the master secret for the     gateway install.  If a custom password is used, the passwords must be imported into the credential     store using the Knox CLI <code>create-alias</code> command.  The aliases for the passwords must then be set     in the configuration using <code>gateway.tls.keystore.password.alias</code> and <code>gateway.tls.key.passphrase.alias</code>.    If both the keystore password and the key passphrase are the same, only <code>gateway.tls.keystore.password.alias</code>     needs to be set.  You can change the key passphrase after import using keytool. You may need to     do this in order to provision the password in the credential store as described later in this     section. For example:<pre><code>keytool -keypasswd -alias gateway-identity -keystore gateway.jks\n</code></pre> </li> </ol> <p>The following will allow you to provision the password for the keystore or the passphrase for the  private key that was set during keystore creation above - it will prompt you for the actual  password/passphrase.</p> <pre><code>bin/knoxcli.sh create-alias &lt;alias name&gt;\n</code></pre> <p>The default alias for the keystore password is <code>gateway-identity-keystore-password</code>, to use a different alias, set <code>gateway.tls.keystore.password.alias</code> in the configuration. The default alias for the key  passphrase is <code>gateway-identity-keystore-password</code>, to use a different alias, set  <code>gateway.tls.key.passphrase.alias</code> in the configuration.</p>"},{"location":"config/#generating-a-self-signed-cert-for-use-in-testing-or-development-environments","title":"Generating a self-signed cert for use in testing or development environments","text":"<pre><code>keytool -genkey -keyalg RSA -alias gateway-identity -keystore gateway.jks \\\n    -storepass {master-secret} -validity 360 -keysize 2048\n</code></pre> <p>Keytool will prompt you for a number of elements used will comprise the distinguished name (DN) within your certificate. </p> <p>NOTE: When it prompts you for your First and Last name be sure to type in the hostname of the machine that your gateway instance will be running on. This is used by clients during hostname verification to ensure that the presented certificate matches the hostname that was used in the URL for the connection - so they need to match.</p> <p>NOTE: When it prompts for the key password just press enter to ensure that it is the same as the keystore password. Which, as was described earlier, must match the master secret for the gateway instance. Alternatively, you can set it to another passphrase - take note of it and set the gateway-identity-passphrase alias to that passphrase using the Knox CLI.</p> <p>See the Knox CLI section for descriptions of the command line utilities related to the management of the keystores.</p>"},{"location":"config/#using-a-ca-signed-key-pair","title":"Using a CA Signed Key Pair","text":"<p>For certain deployments a certificate key pair that is signed by a trusted certificate authority is required. There are a number of different ways in which these certificates are acquired and can be converted and imported into the Apache Knox keystore.</p>"},{"location":"config/#option-1","title":"Option #1","text":"<p>One way to do this is to install the certificate and keys in the default identity keystore and the  master secret.  The following steps have been used to do this and are provided here for guidance in  your installation.  You may have to adjust according to your environment.</p> <ol> <li> <p>Stop Knox gateway and back up all files in <code>{GATEWAY_HOME}/data/security/keystores</code></p> <pre><code>gateway.sh stop\n</code></pre> </li> <li> <p>Create a new master key for Knox and persist it. The master key will be referred to in following steps as <code>$master-key</code></p> <pre><code>knoxcli.sh create-master -force\n</code></pre> </li> <li> <p>Create identity keystore gateway.jks. cert in alias gateway-identity  </p> <pre><code>cd {GATEWAY_HOME}/data/security/keystore  \nkeytool -genkeypair -alias gateway-identity -keyalg RSA -keysize 1024 -dname \"CN=$fqdn_knox,OU=hdp,O=sdge\" -keypass $keypass -keystore gateway.jks -storepass $master-key -validity 300\n</code></pre> <p>NOTE: <code>$fqdn_knox</code> is the hostname of the Knox host. Some may choose <code>$keypass</code> to be the same as <code>$master-key</code>.</p> </li> <li> <p>Create credential store to store the <code>$keypass</code> in step 3. This creates <code>__gateway-credentials.jceks</code> file</p> <pre><code>knoxcli.sh create-alias gateway-identity-passphrase --value $keypass\n</code></pre> </li> <li> <p>Generate a certificate signing request from the gateway.jks</p> <pre><code>keytool -keystore gateway.jks -storepass $master-key -alias gateway-identity -certreq -file knox.csr\n</code></pre> </li> <li> <p>Send the <code>knox.csr</code> file to the CA authority and get back the signed certificate (<code>knox.signed</code>). You also need the CA certificate, which normally can be requested through an openssl command or web browser or from the CA.</p> </li> <li> <p>Import both the CA authority certificate (referred as <code>corporateCA.cer</code>) and the signed Knox certificate back into <code>gateway.jks</code></p> <pre><code>keytool -keystore gateway.jks -storepass $master-key -alias $hwhq -import -file corporateCA.cer  \nkeytool -keystore gateway.jks -storepass $master-key -alias gateway-identity -import -file knox.signed\n</code></pre> <p>NOTE: Use any alias appropriate for the corporate CA.</p> </li> <li> <p>Restart Knox gateway. Check <code>gateway.log</code> to check whether the gateway started properly and clusters are deployed. You can check the timestamp on cluster deployment files</p> <pre><code>gateway.sh start\nls -alrt {GATEWAY_HOME}/data/deployment\n</code></pre> </li> <li> <p>Verify that clients can use the CA authority cert to access Knox (which is the goal of using public signed cert) using curl or a web browser which has the CA certificate installed</p> <pre><code>curl --cacert PATH_TO_CA_CERT -u tom:tom-password -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> </li> </ol>"},{"location":"config/#option-2","title":"Option #2","text":"<p>Another way to do this is to place the CA signed keypair in it's own keystore.  The creation of this keystore is out of scope for this document. However, once created, the following steps can be use to configure the Knox Gateway to use it. </p> <ol> <li> <p>Move the new keystore into a location that the Knox Gateway can access.</p> </li> <li> <p>Edit the gateway-site.xml file to set the configurations   </p> <ul> <li><code>gateway.tls.keystore.password.alias</code> - Alias of the password for the keystore</li> <li><code>gateway.tls.keystore.path</code> - Path to the keystore file</li> <li><code>gateway.tls.keystore.type</code> - Type of keystore file (JKS, JCEKS, PKCS12)</li> <li><code>gateway.tls.key.alias</code> - Alias for the certificate and key</li> <li><code>gateway.tls.key.passphrase.alias</code> - Alias of the passphrase for the key    (needed if the passphrase is different than the keystore password)</li> </ul> </li> <li> <p>Provision the relevant passwords using the Knox CLI</p> <pre><code>knoxcli.sh create-alias &lt;alias&gt; --value &lt;password/passphrase&gt;\n</code></pre> </li> <li> <p>Stop Knox gateway</p> <pre><code>gateway.sh stop\n</code></pre> </li> <li> <p>Restart Knox gateway. Check <code>gateway.log</code> to check whether the gateway started properly and     clusters are deployed. You can check the timestamp on cluster deployment files</p> <pre><code>gateway.sh start\nls -alrt {GATEWAY_HOME}/data/deployment\n</code></pre> </li> <li> <p>Verify that clients can use the CA authority cert to access Knox (which is the goal of using     public signed cert) using curl or a web browser which has the CA certificate installed</p> <pre><code>curl --cacert PATH_TO_CA_CERT -u tom:tom-password -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> </li> </ol>"},{"location":"config/#credential-store","title":"Credential Store","text":"<p>Whenever you provide your own keystore with either a self-signed cert or an issued certificate signed  by a trusted authority, you will need to set an alias for the keystore password and key passphrase.  This is necessary for the current release in order for the system to determine the correct passwords  to use for the keystore and the key.</p> <p>The credential stores in Knox use the JCEKS keystore type as it allows for the storage of general secrets in addition to certificates.</p> <p>Keytool may be used to create credential stores but the Knox CLI section details how to create aliases.  These aliases are managed within credential stores which are created by the CLI as needed. The  simplest approach is to create the relevant aliases with the Knox CLI. This will create the credential  store if it does not already exist and add the password or passphrase.</p> <p>See the Knox CLI section for descriptions of the command line utilities related to the management of the credential stores.</p>"},{"location":"config/#provisioning-of-keystores","title":"Provisioning of Keystores","text":"<p>Once you have created these keystores you must move them into place for the gateway to discover them and use them to represent its identity for SSL connections. This is done by copying the keystores to the <code>{GATEWAY_HOME}/data/security/keystores</code> directory for your gateway install.</p>"},{"location":"config/#summary-of-secrets-to-be-managed","title":"Summary of Secrets to be Managed","text":"<ol> <li>Master secret - the same for all gateway instances in a cluster of gateways</li> <li>All security related artifacts are protected with the master secret</li> <li>Secrets used by the gateway itself are stored within the gateway credential store and are the same across all gateway instances in the cluster of gateways</li> <li>Secrets used by providers within cluster topologies are stored in topology specific credential stores and are the same for the same topology across the cluster of gateway instances.    However, they are specific to the topology - so secrets for one Hadoop cluster are different from those of another.    This allows for fail-over from one gateway instance to another even when encryption is being used while not allowing the compromise of one encryption key to expose the data for all clusters.</li> </ol> <p>NOTE: the SSL certificate will need special consideration depending on the type of certificate. Wildcard certs may be able to be shared across all gateway instances in a cluster. When certs are dedicated to specific machines the gateway identity store will not be able to be blindly replicated as host name verification problems will ensue. Obviously, trust-stores will need to be taken into account as well.</p>"},{"location":"config_advanced_ldap/","title":"Advanced LDAP","text":""},{"location":"config_advanced_ldap/#advanced-ldap-authentication","title":"Advanced LDAP Authentication","text":"<p>The default configuration computes the bind DN for incoming user based on userDnTemplate. This does not work in enterprises where users could belong to multiple branches of LDAP tree. You could instead enable advanced configuration that would compute bind DN of incoming user with an LDAP search.</p>"},{"location":"config_advanced_ldap/#problem-with-userdntemplate-based-authentication","title":"Problem with userDnTemplate based Authentication","text":"<p>UserDnTemplate based authentication uses the configuration parameter <code>ldapRealm.userDnTemplate</code>. Typical values of userDNTemplate would look like <code>uid={0},ou=people,dc=hadoop,dc=apache,dc=org</code>.</p> <p>To compute bind DN of the client, we swap the place holder <code>{0}</code> with the login id provided by the client. For example, if the login id provided by the client is  \"guest', the computed bind DN would be <code>uid=guest,ou=people,dc=hadoop,dc=apache,dc=org</code>.</p> <p>This keeps configuration simple.</p> <p>However, this does not work if users belong to different branches of LDAP DIT. For example, if there are some users under <code>ou=people,dc=hadoop,dc=apache,dc=org</code>  and some users under <code>ou=contractors,dc=hadoop,dc=apache,dc=org</code>, we cannot come up with userDnTemplate that would work for all the users.</p>"},{"location":"config_advanced_ldap/#using-advanced-ldap-authentication","title":"Using advanced LDAP Authentication","text":"<p>With advanced LDAP authentication, we find the bind DN of the user by searching the LDAP directory instead of interpolating the bind DN from userDNTemplate. </p>"},{"location":"config_advanced_ldap/#example-search-filter-to-find-the-client-bind-dn","title":"Example search filter to find the client bind DN","text":"<p>Assuming</p> <ul> <li>ldapRealm.userSearchAttributeName=uid</li> <li>ldapRealm.userObjectClass=person</li> <li>client specified login id = \"guest\"</li> </ul> <p>The LDAP Filter for doing a search to find the bind DN would be</p> <pre><code>(&amp;(uid=guest)(objectclass=person))\n</code></pre> <p>This could find the bind DN to be </p> <pre><code>uid=guest,ou=people,dc=hadoop,dc=apache,dc=org\n</code></pre> <p>Please note that the <code>userSearchAttributeName</code> need not be part of bindDN.</p> <p>For example, you could use </p> <ul> <li>ldapRealm.userSearchAttributeName=email</li> <li>ldapRealm.userObjectClass=person</li> <li>client specified login id =  \"bill.clinton@gmail.com\"</li> </ul> <p>The LDAP Filter for doing a search to find the bind DN would be</p> <pre><code>(&amp;(email=bill.clinton@gmail.com)(objectclass=person))\n</code></pre> <p>This could find the bind DN to be</p> <pre><code>uid=billc,ou=contractors,dc=hadoop,dc=apache,dc=org\n</code></pre>"},{"location":"config_advanced_ldap/#advanced-ldap-configuration-parameters","title":"Advanced LDAP configuration parameters","text":"<p>The table below provides a brief description and sample of the available advanced bind and search configuration parameters.</p> Parameter Description Default Sample principalRegex Parses the principal for insertion into templates via regex. (.*) (.*?)\\\\(.*) (e.g. match US\\tom: {0}=US\\tom, {1}=US, {2}=tom) userDnTemplate Direct user bind DN template. {0} cn={2},dc={1},dc=qa,dc=company,dc=com userSearchBase Search based template. Used with config below. none dc={1},dc=qa,dc=company,dc=com userSearchAttributeName Attribute name for simplified search filter. none sAMAccountName userSearchAttributeTemplate Attribute template for simplified search filter. {0} {2} userSearchFilter Advanced search filter template. Note \\&amp; is \\&amp; in XML. none (\\&amp;(objectclass=person)(sAMAccountName={2})) userSearchScope Search scope: subtree, onelevel, object. subtree onelevel"},{"location":"config_advanced_ldap/#advanced-ldap-configuration-combinations","title":"Advanced LDAP configuration combinations","text":"<p>There are also only certain valid combinations of advanced LDAP configuration parameters.</p> <ul> <li>User DN Template<ul> <li>userDnTemplate (Required)</li> <li>principalRegex (Optional)</li> </ul> </li> <li>User Search by Attribute<ul> <li>userSearchBase (Required)</li> <li>userAttributeName (Required)</li> <li>userAttributeTemplate (Optional)</li> <li>userSearchScope (Optional)</li> <li>principalRegex (Optional)</li> </ul> </li> <li>User Search by Filter<ul> <li>userSearchBase (Required)</li> <li>userSearchFilter (Required)</li> <li>userSearchScope (Optional)</li> <li>principalRegex (Optional)</li> </ul> </li> </ul>"},{"location":"config_advanced_ldap/#advanced-ldap-configuration-precedence","title":"Advanced LDAP configuration precedence","text":"<p>The presence of multiple configuration combinations should be avoided. The rules below clarify which combinations take precedence when present.</p> <ol> <li>userSearchBase takes precedence over userDnTemplate</li> <li>userSearchFilter takes precedence over userSearchAttributeName</li> </ol>"},{"location":"config_advanced_ldap/#example-provider-configuration-to-use-advanced-ldap-authentication","title":"Example provider configuration to use advanced LDAP authentication","text":"<p>The example configuration appears verbose due to the presence of liberal comments  and illustration of optional parameters and default values. The configuration that you would use could be much shorter if you rely on default values.</p> <pre><code>&lt;provider&gt;\n\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n        &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- update the value based on your ldap directory protocol, host and port --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n        &lt;value&gt;ldap://hdp.example.com:389&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: simple\n         Update the value based on mechanisms supported by your ldap directory --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n        &lt;value&gt;simple&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: {0}\n         update the value based on your ldap DIT(directory information tree).\n         ignored if value is defined for main.ldapRealm.userSearchAttributeName --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: null\n         If you specify a value for this attribute, useDnTemplate \n         specified above would be ignored and user bind DN would be computed using\n         ldap search\n         update the value based on your ldap DIT(directory information layout)\n         value of search attribute should identity the user uniquely --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userSearchAttributeName&lt;/name&gt;\n        &lt;value&gt;uid&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: false  \n         If the value is true, groups in which user is a member are looked up \n         from LDAP and made available  for service level authorization checks --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.authorizationEnabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- bind DN used to search for groups and user bind DN.  \n         Required if a value is defined for main.ldapRealm.userSearchAttributeName\n         or if the value of main.ldapRealm.authorizationEnabled is true --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemUsername&lt;/name&gt;\n        &lt;value&gt;uid=guest,ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- password for systemUserName.\n         Required if a value is defined for main.ldapRealm.userSearchAttributeName\n         or if the value of main.ldapRealm.authorizationEnabled is true --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemPassword&lt;/name&gt;\n        &lt;value&gt;${ALIAS=ldcSystemPassword}&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: simple\n         Update the value based on mechanisms supported by your ldap directory --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemAuthenticationMechanism&lt;/name&gt;\n        &lt;value&gt;simple&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: person\n         Objectclass to identify user entries in ldap, used to build search \n         filter to search for user bind DN --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userObjectClass&lt;/name&gt;\n        &lt;value&gt;person&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- search base used to search for user bind DN and groups --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.searchBase&lt;/name&gt;\n        &lt;value&gt;dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- search base used to search for user bind DN.\n         Defaults to the value of main.ldapRealm.searchBase. \n         If main.ldapRealm.userSearchAttributeName is defined, \n         value for main.ldapRealm.searchBase  or main.ldapRealm.userSearchBase \n         should be defined --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userSearchBase&lt;/name&gt;\n        &lt;value&gt;dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- search base used to search for groups.\n         Defaults to the value of main.ldapRealm.searchBase.\n         If value of main.ldapRealm.authorizationEnabled is true,\n         value for main.ldapRealm.searchBase  or main.ldapRealm.groupSearchBase should be defined --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.groupSearchBase&lt;/name&gt;\n        &lt;value&gt;dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: groupOfNames\n         Objectclass to identify group entries in ldap, used to build search \n         filter to search for group entries --&gt; \n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.groupObjectClass&lt;/name&gt;\n        &lt;value&gt;groupOfNames&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: member\n         If value is memberUrl, we treat found groups as dynamic groups --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttribute&lt;/name&gt;\n        &lt;value&gt;member&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: uid={0}\n         Ignored if value is defined for main.ldapRealm.userSearchAttributeName --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttributeValueTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: cn --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.groupIdAttribute&lt;/name&gt;\n        &lt;value&gt;cn&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;urls./**&lt;/name&gt;\n        &lt;value&gt;authcBasic&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;!-- optional, default value: 30min --&gt;\n    &lt;param&gt;\n        &lt;name&gt;sessionTimeout&lt;/name&gt;\n        &lt;value&gt;30&lt;/value&gt;\n    &lt;/param&gt;\n\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_advanced_ldap/#special-note-on-parameter-mainldaprealmcontextfactorysystempassword","title":"Special note on parameter main.ldapRealm.contextFactory.systemPassword","text":"<p>The value for this could have one of the following 2 formats</p> <ul> <li>plaintextpassword</li> <li>${ALIAS=ldcSystemPassword}</li> </ul> <p>The first format specifies the password in plain text in the provider configuration. Use of this format should be limited for testing and troubleshooting.</p> <p>We strongly recommend using the second format <code>${ALIAS=ldcSystemPassword}</code> in production. This format uses an alias for the password stored in credential store. In the example <code>${ALIAS=ldcSystemPassword}</code>,  ldcSystemPassword is the alias for the password stored in credential store.</p> <p>Assuming the plain text password is \"hadoop\", and your topology file name is \"hdp.xml\", you would use following command to create the right password alias in credential store.</p> <pre><code>{GATEWAY_HOME}/bin/knoxcli.sh  create-alias ldcSystemPassword --cluster hdp --value hadoop\n</code></pre>"},{"location":"config_apikey/","title":"API Key","text":""},{"location":"config_apikey/#api-key-api","title":"API Key API","text":""},{"location":"config_apikey/#introduction","title":"Introduction","text":"<p>The APIKEY API is an extension of the KnoxToken API that defaults certain configuration and translates the responses in a way that supports the specific API Key use cases. This API is used to issue API Keys for use with services like AI Inferencing APIs such as OpenAI compatible APIs where an Authorization Bearer Token is expected and it is not a JWT with expectations around expiry and cryptographic verification of the credentials.</p> <p>The only difference from the KnoxToken API in the configuration are the parameter names. They must be prefixed with \"apikey.\" this is done to disambiguate the config from that of KnoxToken itself when they are colocated in the same topology.</p> <p>In addition, the default behavior differs in that the time-to-live or TTL defaults to \"-1\" which means that by default the API Keys do not expire. It also differs in that the returned APIKeys are Passcode tokens and as such are by definition server managed. Therefore, we default the server managed configuration to true for convenience and to reduce errors in deployment.</p> <p>API Key - The example below shows the interaction with the APIKey API via curl and the response with default behavior.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;APIKEY&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>In this deployment example the TTL is -1 by default which means it never expires and is not included in the response.</p> <pre><code>$ curl -ivku guest:guest-password -X POST \"https://localhost:8443/gateway/sandbox/apikey/api/v1/auth/key\"\n{\"key_id\":\"9c2d22fb-e28d-4495-aaae-d4103dada8d1\",\"api_key\":\"T1dNeVpESXlabUl0WlRJNFpDMDBORGsxTFdGaFlX....R1F4OjpNMlV5WXpFeE56a3RZbVJtTXkwME1HTTJMVGxoTmpVdE9HWXdNbUZrTTJWa016UXo=\"}\n</code></pre> <p>API Key - The example below shows the interaction with the APIKey API via curl and the response.</p> <p>In this deployment example the TTL is set to 74000 ms which is translated to seconds in the response.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;APIKEY&lt;/role&gt;\n    &lt;param&gt;\n        &lt;name&gt;apikey.knox.token.ttl&lt;/name&gt;\n        &lt;value&gt;74000&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/service&gt;\n\n$ curl -ivku guest:guest-password -X POST \"https://localhost:8443/gateway/sandbox/apikey/api/v1/auth/key\"\n{\"key_id\":\"9c2d22fb-e28d-4495-aaae-d4103dada8d1\",\"api_key\":\"T1dNeVpESXlabUl0WlRJNFpDMDBORGsxTFdGaFlX....R1F4OjpNMlV5WXpFeE56a3RZbVJtTXkwME1HTTJMVGxoTmpVdE9HWXdNbUZrTTJWa016UXo=\",\"expires_in\":74}\n</code></pre> <p>Note that in both of the above response that there is a key_id as well as the api_key. The api_key is intended to be used as the API Key via Authorization Bearer Token in the invocations of APIs.</p> <p>The key_id may be used in management operations of the API Key lifecycle by those with appropriate permissions to do so.</p>"},{"location":"config_audit/","title":"Audit","text":""},{"location":"config_audit/#audit","title":"Audit","text":"<p>The Audit facility within the Knox Gateway introduces functionality for tracking actions that are executed by Knox per user's request or that are produced by Knox internal events like topology deploy, etc. The Knox Audit module is based on Apache log4j.</p>"},{"location":"config_audit/#configuration-needed","title":"Configuration needed","text":"<p>Out of the box, the Knox Gateway includes preconfigured auditing capabilities. To change its configuration please read the following sections.</p>"},{"location":"config_audit/#where-audit-logs-go","title":"Where audit logs go","text":"<p>The Audit module is preconfigured to write audit records to the log file <code>{GATEWAY_HOME}/log/gateway-audit.log</code>.</p> <p>This behavior can be changed in the <code>{GATEWAY_HOME}/conf/gateway-log4j.properties</code> file. <code>app.audit.file</code> can be used to change the location. The <code>log4j.appender.auditfile.*</code> properties can be used for further customization. For detailed information read the Apache log4j documentation.</p>"},{"location":"config_audit/#audit-format","title":"Audit format","text":"<p>Out of the box, the audit record format is defined by <code>org.apache.knox.gateway.audit.log4j.layout.AuditLayout</code>. Its structure is as follows:</p> <pre><code>EVENT_PUBLISHING_TIME ROOT_REQUEST_ID|PARENT_REQUEST_ID|REQUEST_ID|LOGGER_NAME|TARGET_SERVICE_NAME|USER_NAME|PROXY_USER_NAME|SYSTEM_USER_NAME|ACTION|RESOURCE_TYPE|RESOURCE_NAME|OUTCOME|LOGGING_MESSAGE\n</code></pre> <p>The audit record format can be changed by setting <code>log4j.appender.auditfile.layout</code> property in <code>{GATEWAY_HOME}/conf/gateway-log4j.properties</code> to another class that extends <code>org.apache.log4j.Layout</code> or its subclasses.</p> <p>For detailed information read Apache log4j.</p>"},{"location":"config_audit/#how-to-interpret-audit-log","title":"How to interpret audit log","text":"Component Description EVENT_PUBLISHING_TIME Time when audit record was published. ROOT_REQUEST_ID The root request ID if this is a sub-request. Currently it is empty. PARENT_REQUEST_ID The parent request ID if this is a sub-request. Currently it is empty. REQUEST_ID A unique value representing the current, active request. If the current request id value is different from the current parent request id value then the current request id value is moved to the parent request id before it is replaced by the provided request id. If the root request id is not set it will be set with the first non-null value of either the parent request id or the passed request id. LOGGER_NAME The name of the logger TARGET_SERVICE_NAME Name of Hadoop service. Can be empty if audit record is not linked to any Hadoop service, for example, audit record for topology deployment. USER_NAME Name of user that initiated session with Knox PROXY_USER_NAME Mapped user name. For detailed information read #[Identity Assertion]. SYSTEM_USER_NAME Currently is empty. ACTION Type of action that was executed. Following actions are defined: authentication, authorization, redeploy, deploy, undeploy, identity-mapping, dispatch, access. RESOURCE_TYPE Type of resource for which action was executed. Following resource types are defined: uri, topology, principal. RESOURCE_NAME Name of resource. For resource of type topology it is name of topology. For resource of type uri it is inbound or dispatch request path. For resource of type principal it is a name of mapped user. OUTCOME Action result type. Following outcomes are defined: success, failure, unavailable. LOGGING_MESSAGE Logging message. Contains additional tracking information."},{"location":"config_audit/#audit-log-rotation","title":"Audit log rotation","text":"<p>Audit logging is preconfigured with <code>org.apache.log4j.DailyRollingFileAppender</code>. Apache log4j contains information about other Appenders.</p>"},{"location":"config_audit/#how-to-change-the-audit-level-or-disable-it","title":"How to change the audit level or disable it","text":"<p>All audit messages are logged at <code>INFO</code> level and this behavior can't be changed.</p> <p>Disabling auditing can be done by decreasing the log level for the Audit appender or setting it to <code>OFF</code>.</p>"},{"location":"config_authn/","title":"Overview","text":""},{"location":"config_authn/#authentication","title":"Authentication","text":"<p>There are two types of providers supported in Knox for establishing a user's identity:</p> <ol> <li>Authentication Providers</li> <li>Federation Providers</li> </ol> <p>Authentication providers directly accept a user's credentials and validates them against some particular user store. Federation providers, on the other hand, validate a token that has been issued for the user by a trusted Identity Provider (IdP).</p> <p>The current release of Knox ships with an authentication provider based on the Apache Shiro project and is initially configured for BASIC authentication against an LDAP store. This has been specifically tested against Apache Directory Server and Active Directory.</p> <p>This section will cover the general approach to leveraging Shiro within the bundled provider including:</p> <ol> <li>General mapping of provider config to <code>shiro.ini</code> config</li> <li>Specific configuration for the bundled BASIC/LDAP configuration</li> <li>Some tips into what may need to be customized for your environment</li> <li>How to setup the use of LDAP over SSL or LDAPS</li> </ol>"},{"location":"config_authn/#general-configuration-for-shiro-provider","title":"General Configuration for Shiro Provider","text":"<p>As is described in the configuration section of this document, providers have a name-value based configuration - as is the common pattern in the rest of Hadoop.</p> <p>The following example shows the format of the configuration for a given provider:</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;{name}&lt;/name&gt;\n        &lt;value&gt;{value}&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>Conversely, the Shiro provider currently expects a <code>shiro.ini</code> file in the <code>WEB-INF</code> directory of the cluster specific web application.</p> <p>The following example illustrates a configuration of the bundled BASIC/LDAP authentication config in a <code>shiro.ini</code> file:</p> <pre><code>[urls]\n/**=authcBasic\n[main]\nldapRealm=org.apache.shiro.realm.ldap.JndiLdapRealm\nldapRealm.contextFactory.authenticationMechanism=simple\nldapRealm.contextFactory.url=ldap://localhost:33389\nldapRealm.userDnTemplate=uid={0},ou=people,dc=hadoop,dc=apache,dc=org\n</code></pre> <p>In order to fit into the context of an INI file format, at deployment time we interrogate the parameters provided in the provider configuration and parse the INI section out of the parameter names. The following provider config illustrates this approach. Notice that the section names in the above shiro.ini match the beginning of the parameter names that are in the following config:</p> <pre><code>&lt;gateway&gt;\n    &lt;provider&gt;\n        &lt;role&gt;authentication&lt;/role&gt;\n        &lt;name&gt;ShiroProvider&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm&lt;/name&gt;\n            &lt;value&gt;org.apache.shiro.realm.ldap.JndiLdapRealm&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n            &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n            &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n            &lt;value&gt;simple&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;urls./**&lt;/name&gt;\n            &lt;value&gt;authcBasic&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre> <p>This happens to be the way that we are currently configuring Shiro for BASIC/LDAP authentication. This same config approach may be used to achieve other authentication mechanisms or variations on this one. We however have not tested additional uses for it for this release.</p>"},{"location":"config_authn/#ldap-configuration","title":"LDAP Configuration","text":"<p>This section discusses the LDAP configuration used above for the Shiro Provider. Some of these configuration elements will need to be customized to reflect your deployment environment.</p> <p>main.ldapRealm - this element indicates the fully qualified class name of the Shiro realm to be used in authenticating the user. The class name provided by default in the sample is the <code>org.apache.shiro.realm.ldap.JndiLdapRealm</code> this implementation provides us with the ability to authenticate but by default has authorization disabled. In order to provide authorization - which is seen by Shiro as dependent on an LDAP schema that is specific to each organization - an extension of JndiLdapRealm is generally used to override and implement the doGetAuthorizationInfo method. In this particular release we are providing a simple authorization provider that can be used along with the Shiro authentication provider.</p> <p>main.ldapRealm.userDnTemplate - in order to bind a simple username to an LDAP server that generally requires a full distinguished name (DN), we must provide the template into which the simple username will be inserted. This template allows for the creation of a DN by injecting the simple username into the common name (CN) portion of the DN. This element will need to be customized to reflect your deployment environment. The template provided in the sample is only an example and is valid only within the LDAP schema distributed with Knox and is represented by the <code>users.ldif</code> file in the <code>{GATEWAY_HOME}/conf</code> directory.</p> <p>main.ldapRealm.contextFactory.url - this element is the URL that represents the host and port of the LDAP server. It also includes the scheme of the protocol to use. This may be either <code>ldap</code> or <code>ldaps</code> depending on whether you are communicating with the LDAP over SSL (highly recommended). This element will need to be customized to reflect your deployment environment..</p> <p>main.ldapRealm.contextFactory.authenticationMechanism - this element indicates the type of authentication that should be performed against the LDAP server. The current default value is <code>simple</code> which indicates a simple bind operation. This element should not need to be modified and no mechanism other than a simple bind has been tested for this particular release.</p> <p>urls./** - this element represents a single URL_Ant_Path_Expression and the value the Shiro filter chain to apply to it. This particular sample indicates that all paths into the application have the same Shiro filter chain applied. The paths are relative to the application context path. The use of the value <code>authcBasic</code> here indicates that BASIC authentication is expected for every path into the application. Adding an additional Shiro filter to that chain for validating that the request isSecure() and over SSL can be achieved by changing the value to <code>ssl, authcBasic</code>. This parameter can be used to exclude endpoints from authentication, this is important in case of jwks endpoints which need not require authentication. We have support for unauthenticated paths in other authenitcation providers and this support can be extended here using the <code>urls</code> parameter. Following is an example of how <code>/knoxtoken/api/v1/jwks.json</code> endpoint can be excluded from authentication in shiro configuration.</p> <pre><code>    &lt;param&gt;\n        &lt;name&gt;urls./knoxtoken/api/v1/jwks.json&lt;/name&gt;\n        &lt;value&gt;anon&lt;/value&gt;\n    &lt;/param&gt;\n</code></pre>"},{"location":"config_authn/#active-directory-special-note","title":"Active Directory - Special Note","text":"<p>You would use LDAP configuration as documented above to authenticate against Active Directory as well.</p> <p>Some Active Directory specific things to keep in mind:</p> <p>Typical AD <code>main.ldapRealm.userDnTemplate</code> value looks slightly different, such as</p> <pre><code>cn={0},cn=users,DC=lab,DC=sample,dc=com\n</code></pre> <p>Please compare this with a typical Apache DS <code>main.ldapRealm.userDnTemplate</code> value and make note of the difference:</p> <pre><code>`uid={0},ou=people,dc=hadoop,dc=apache,dc=org`\n</code></pre> <p>If your AD is configured to authenticate based on just the cn and password and does not require user DN, you do not have to specify value for <code>main.ldapRealm.userDnTemplate</code>.</p>"},{"location":"config_authn/#ldap-over-ssl-ldaps-configuration","title":"LDAP over SSL (LDAPS) Configuration","text":"<p>In order to communicate with your LDAP server over SSL (again, highly recommended), you will need to modify the topology file in a couple ways and possibly provision some keying material.</p> <ol> <li>main.ldapRealm.contextFactory.url must be changed to have the <code>ldaps</code> protocol scheme and the port must be the SSL listener port on your LDAP server.</li> <li>Identity certificate (keypair) provisioned to LDAP server - your LDAP server specific documentation should indicate what is required for providing a cert or keypair to represent the LDAP server identity to connecting clients.</li> <li>Trusting the LDAP Server's public key - if the LDAP Server's identity certificate is issued by a well known and trusted certificate authority and is already represented in the JRE's cacerts truststore then you don't need to do anything for trusting the LDAP server's cert. If, however, the cert is self-signed or issued by an untrusted authority you will need to either add it to the cacerts keystore or to another truststore that you may direct Knox to utilize through a system property (<code>javax.net.ssl.trustStore</code> and <code>javax.net.ssl.trustStorePassword</code>).</li> </ol>"},{"location":"config_authn/#session-configuration","title":"Session Configuration","text":"<p>Knox maps each cluster topology to a web application and leverages standard JavaEE session management.</p> <p>To configure session idle timeout for the topology, please specify value of parameter sessionTimeout for ShiroProvider in your topology file. If you do not specify the value for this parameter, it defaults to 30 minutes.</p> <p>The definition would look like the following in the topology file:</p> <pre><code>...\n&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;!--\n        Session timeout in minutes. This is really idle timeout.\n        Defaults to 30 minutes, if the property value is not defined.\n        Current client authentication will expire if client idles\n        continuously for more than this value\n        --&gt;\n        &lt;name&gt;sessionTimeout&lt;/name&gt;\n        &lt;value&gt;30&lt;/value&gt;\n    &lt;/param&gt;\n&lt;provider&gt;\n...\n</code></pre> <p>At present, ShiroProvider in Knox leverages JavaEE session to maintain authentication state for a user across requests using JSESSIONID cookie. So, a client that authenticated with Knox could pass the JSESSIONID cookie with repeated requests as long as the session has not timed out instead of submitting userid/password with every request. Presenting a valid session cookie in place of userid/password would also perform better as additional credential store lookups are avoided.</p>"},{"location":"config_authz/","title":"Authorization","text":""},{"location":"config_authz/#authorization","title":"Authorization","text":""},{"location":"config_authz/#service-level-authorization","title":"Service Level Authorization","text":"<p>The Knox Gateway has an out-of-the-box authorization provider that allows administrators to restrict access to the individual services within a Hadoop cluster.</p> <p>This provider utilizes a simple and familiar pattern of using ACLs to protect Hadoop resources by specifying users, groups and ip addresses that are permitted access.</p> <p>Note: This feature will not work as expected if 'anonymous' authentication is used. </p>"},{"location":"config_authz/#configuration","title":"Configuration","text":"<p>ACLs are bound to services within the topology descriptors by introducing the authorization provider with configuration like:</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;authorization&lt;/role&gt;\n    &lt;name&gt;AclsAuthz&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/provider&gt;\n</code></pre> <p>The above configuration enables the authorization provider but does not indicate any ACLs yet and therefore there is no restriction to accessing the Hadoop services. In order to indicate the resources to be protected and the specific users, groups or ip's to grant access, we need to provide parameters like the following:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;username[,*|username...];group[,*|group...];ipaddr[,*|ipaddr...]&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>where <code>{serviceName}</code> would need to be the name of a configured Hadoop service within the topology.</p> <p>NOTE: ipaddr is unique among the parts of the ACL in that you are able to specify a wildcard within an ipaddr to indicate that the remote address must being with the String prior to the asterisk within the ipaddr ACL. For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;*;*;192.168.*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>This indicates that the request must come from an IP address that begins with '192.168.' in order to be granted access.</p> <p>Note also that configuration without any ACLs defined is equivalent to:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;*;*;*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>meaning: all users, groups and IPs have access. Each of the elements of the ACL parameter support multiple values via comma separated list and the <code>*</code> wildcard to match any.</p> <p>For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;webhdfs.acl&lt;/name&gt;\n    &lt;value&gt;hdfs;admin;127.0.0.2,127.0.0.3&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>this configuration indicates that ALL of the following have to be satisfied to be granted access:</p> <ol> <li>The user name must be \"hdfs\" AND</li> <li>the user must be in the group \"admin\" AND</li> <li>the user must come from either 127.0.0.2 or 127.0.0.3</li> </ol> <p>This allows us to craft policy that restricts the members of a large group to a subset that should have access. The user being removed from the group will allow access to be denied even though their username may have been in the ACL.</p> <p>An additional configuration element may be used to alter the processing of the ACL to be OR instead of the default AND behavior:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>this processing behavior requires that the effective user satisfy one of the parts of the ACL definition in order to be granted access. For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;webhdfs.acl&lt;/name&gt;\n    &lt;value&gt;hdfs,guest;admin;127.0.0.2,127.0.0.3&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>You may also set the ACL processing mode at the top level for the topology. This essentially sets the default for the managed cluster. It may then be overridden at the service level as well.</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>this configuration indicates that ONE of the following must be satisfied to be granted access:</p> <ol> <li>The user is \"hdfs\" or \"guest\" OR</li> <li>the user is in \"admin\" group OR</li> <li>the request is coming from 127.0.0.2 or 127.0.0.3</li> </ol> <p>Following are a few concrete examples on how to use this feature.</p> <p>Note: In the examples below <code>{serviceName}</code> represents a real service name (e.g. WEBHDFS) and would be replaced with these values in an actual configuration.</p>"},{"location":"config_authz/#usecases","title":"Usecases","text":""},{"location":"config_authz/#usecase-1-restrict-access-to-specific-hadoop-services-to-specific-users","title":"USECASE-1: Restrict access to specific Hadoop services to specific Users","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;*;*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-2-restrict-access-to-specific-hadoop-services-to-specific-groups","title":"USECASE-2: Restrict access to specific Hadoop services to specific Groups","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acls&lt;/name&gt;\n    &lt;value&gt;*;admins;*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-3-restrict-access-to-specific-hadoop-services-to-specific-remote-ips","title":"USECASE-3: Restrict access to specific Hadoop services to specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;*;*;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-4-restrict-access-to-specific-hadoop-services-to-specific-users-or-users-within-specific-groups","title":"USECASE-4: Restrict access to specific Hadoop services to specific Users OR users within specific Groups","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;admin;*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-5-restrict-access-to-specific-hadoop-services-to-specific-users-or-users-from-specific-remote-ips","title":"USECASE-5: Restrict access to specific Hadoop services to specific Users OR users from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;*;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-6-restrict-access-to-specific-hadoop-services-to-users-within-specific-groups-or-from-specific-remote-ips","title":"USECASE-6: Restrict access to specific Hadoop services to users within specific Groups OR from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;*;admin;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-7-restrict-access-to-specific-hadoop-services-to-specific-users-or-users-within-specific-groups-or-from-specific-remote-ips","title":"USECASE-7: Restrict access to specific Hadoop services to specific Users OR users within specific Groups OR from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl.mode&lt;/name&gt;\n    &lt;value&gt;OR&lt;/value&gt;\n&lt;/param&gt;\n&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;admin;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-8-restrict-access-to-specific-hadoop-services-to-specific-users-and-users-within-specific-groups","title":"USECASE-8: Restrict access to specific Hadoop services to specific Users AND users within specific Groups","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;admin;*&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-9-restrict-access-to-specific-hadoop-services-to-specific-users-and-users-from-specific-remote-ips","title":"USECASE-9: Restrict access to specific Hadoop services to specific Users AND users from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;*;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-10-restrict-access-to-specific-hadoop-services-to-users-within-specific-groups-and-from-specific-remote-ips","title":"USECASE-10: Restrict access to specific Hadoop services to users within specific Groups AND from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;*;admins;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-11-restrict-access-to-specific-hadoop-services-to-specific-users-and-users-within-specific-groups-and-from-specific-remote-ips","title":"USECASE-11: Restrict access to specific Hadoop services to specific Users AND users within specific Groups AND from specific Remote IPs","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;{serviceName}.acl&lt;/name&gt;\n    &lt;value&gt;guest;admins;127.0.0.1&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_authz/#usecase-12-full-example-including-identity-assertionprincipal-mapping","title":"USECASE-12: Full example including identity assertion/principal mapping","text":"<p>The principal mapping aspect of the identity assertion provider is important to understand in order to fully utilize the authorization features of this provider.</p> <p>This feature allows us to map the authenticated principal to a runAs or impersonated principal to be asserted to the Hadoop services in the backend. It is fully documented in the Identity Assertion section of this guide.</p> <p>These additional mapping capabilities are used together with the authorization ACL policy. An example of a full topology that illustrates these together is below.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        &lt;provider&gt;\n            &lt;role&gt;authentication&lt;/role&gt;\n            &lt;name&gt;ShiroProvider&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm&lt;/name&gt;\n                &lt;value&gt;org.apache.shiro.realm.ldap.JndiLdapRealm&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n                &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n                &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n                &lt;value&gt;simple&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;urls./**&lt;/name&gt;\n                &lt;value&gt;authcBasic&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;identity-assertion&lt;/role&gt;\n            &lt;name&gt;Default&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;principal.mapping&lt;/name&gt;\n                &lt;value&gt;guest=hdfs;&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;group.principal.mapping&lt;/name&gt;\n                &lt;value&gt;*=users;hdfs=admin&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;authorization&lt;/role&gt;\n            &lt;name&gt;AclsAuthz&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;acl.mode&lt;/name&gt;\n                &lt;value&gt;OR&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;webhdfs.acl.mode&lt;/name&gt;\n                &lt;value&gt;AND&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;webhdfs.acl&lt;/name&gt;\n                &lt;value&gt;hdfs;admin;127.0.0.2,127.0.0.3&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;webhcat.acl&lt;/name&gt;\n                &lt;value&gt;hdfs;admin;127.0.0.2,127.0.0.3&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n        &lt;provider&gt;\n            &lt;role&gt;hostmap&lt;/role&gt;\n            &lt;name&gt;static&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;localhost&lt;/name&gt;\n                &lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n    &lt;/gateway&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;JOBTRACKER&lt;/role&gt;\n        &lt;url&gt;rpc://localhost:8050&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;WEBHDFS&lt;/role&gt;\n        &lt;url&gt;http://localhost:50070/webhdfs&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;WEBHCAT&lt;/role&gt;\n        &lt;url&gt;http://localhost:50111/templeton&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;OOZIE&lt;/role&gt;\n        &lt;url&gt;http://localhost:11000/oozie&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;WEBHBASE&lt;/role&gt;\n        &lt;url&gt;http://localhost:8080&lt;/url&gt;\n    &lt;/service&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;HIVE&lt;/role&gt;\n        &lt;url&gt;http://localhost:10001/cliservice&lt;/url&gt;\n    &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre>"},{"location":"config_authz/#composite-authorization-provider","title":"Composite Authorization Provider","text":"<p>By providing a composite authz provider, we are able to configure multiple authz providers in a single topology.  This allows the use of both the AclsAuthz provider and something like the Ranger Knox plugin where available.</p> <p>All authorization providers used within the CompositeAuthz provider will need to grant access for the request processing to continue to the protected resource. This is a logical AND across all the providers.</p> <p>The following is an example of what configuration of the CompositeAuthz provider is like.</p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;CompositeAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;composite.provider.names&lt;/name&gt;\n            &lt;value&gt;AclsAuthz,SomeOther&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;AclsAuthz.webhdfs.acl&lt;/name&gt;\n            &lt;value&gt;admin;*;*&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;SomeOther.provider.specific.param&lt;/name&gt;\n            &lt;value&gt;provider.specific-value&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre> <p>Note the comma separated list of provider names in composite.provider.names param.</p> <p>Also Note the use of those names as prefixes to the params to be set on the respective providers.</p> <p>The prefixes are removed and the expected param names are set on the actual providers as appropriate.</p>"},{"location":"config_authz/#path-based-authorization","title":"Path Based Authorization","text":"<p>Path based authorization (<code>PathAclsAuthz</code>) enforces Acls authorization on a configured path.  The semantics of Path based authorization are similar to Acls authz. Authorization is done based on path matching similar to rewrite rules. </p> <p>Format is very similar to AclsAuthz provider with an addition of path argument. The format is <code>{path};{users};{groups}:{ips}</code>. For details on the format please see #[Service Level Authorization]. One important thing to note here is that the path is not plural, there has to be one and only one path defined.</p> <p>In case one wants multiple paths they can define multiple rules with rule name as a parameter e.g. <code>KNOXTOKEN.{rule_name}.path.acl</code></p> <p>Following are special cases for rule names:</p>"},{"location":"config_authz/#this-rule-will-be-applied-to-all-services-defined-in-the-topology","title":"This rule will be applied to ALL services defined in the topology","text":"<p>This rule be applied to all services in the topology. Which means any service that has <code>api</code>  as a context path needs the user to be <code>admin</code> for successful authorization. </p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_authz/#this-rule-will-be-applied-to-only-the-service","title":"This rule will be applied to only the service","text":"<p>This rule be applied to only <code>{service_name}</code> services in the topology. Any request for <code>{service_name}</code> that has <code>api</code>  as a context path needs the user to be <code>admin</code> for successful authorization. </p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;{service_name}.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_authz/#all-of-these-rules-will-be-applied-to-service","title":"ALL of these rules will be applied to service","text":"<p>NOTE: {rule_1} and {rule_2} should be any unique names.  Similar to previous cases for a service <code>{service_name}</code>, for any  request to be successful with <code>api</code> and <code>api2</code> as context paths, it needs to have user <code>admin</code>. </p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;{service_name}.{rule_1}.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;{service_name}.{rule_2}.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/api2/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_authz/#examples","title":"Examples","text":"<p>Following are concrete examples of the the above rules:</p>"},{"location":"config_authz/#this-rule-will-be-applied-to-all-services-defined-in-the-topology_1","title":"This rule will be applied to ALL services defined in the topology","text":"<pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/knoxtoken/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_authz/#this-rule-will-be-applied-to-only-to-knoxtoken-service","title":"This rule will be applied to only to KNOXTOKEN service","text":"<pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;KNOXTOKEN.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/knoxtoken/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_authz/#all-of-these-rules-will-be-applied-to-only-to-knoxtoken-service","title":"All of these rules will be applied to only to KNOXTOKEN service","text":"<pre><code>    &lt;provider&gt;\n        &lt;role&gt;authorization&lt;/role&gt;\n        &lt;name&gt;PathAclsAuthz&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;KNOXTOKEN.rule_1.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/knoxtoken/api/**;admin;*;*&lt;/value&gt; \n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;KNOXTOKEN.rule_2.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/knoxtoken/foo/**;knox;*;*&lt;/value&gt; \n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;KNOXTOKEN.rule_3.path.acl&lt;/name&gt;\n            &lt;value&gt;https://*:*/**/knoxtoken/bar/**;sam;admin;*&lt;/value&gt; \n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_client_credentials/","title":"Client Credentials","text":""},{"location":"config_client_credentials/#client-credentials-api","title":"Client Credentials API","text":""},{"location":"config_client_credentials/#introduction","title":"Introduction","text":"<p>The CLIENTID API is an extension of the KnoxToken API that defaults certain configuration and translates the responses in a way that supports the specific OAuth Client Credentials Flow use cases. This API is used to issue API Keys for use with services like the Iceberg REST Catalog API.</p> <p>The only difference from the KnoxToken API in the configuration are the parameter names. They must be prefixed with \"clientid.\" this is done to disambiguate the config from that of KnoxToken itself when they are colocated in the same topology.</p> <p>In addition, the default behavior differs in that the time-to-live or TTL defaults to \"-1\" which means that by default the API Keys do not expire. It also differs in that the returned APIKeys are Passcode tokens and as such are by definition server managed. Therefore, we default the server managed configuration to true for convenience and to reduce errors in deployment.</p> <p>Client Credentials - The example below shows the interaction with the APIKey API via curl and the response with default behavior.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;CLIENTID&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>In this deployment example the TTL is -1 by default which means it never expires and is not included in the response.</p> <pre><code>$ curl -ivku guest:guest-password -X POST \"https://localhost:8443/gateway/sandbox/clientid/api/v1/oauth/credentials\"\n{\"client_secret\":\"WXpOa1l6SmxPRFF0TmpOalpTMDBPREZpTFRobE5qY3RO....jpOems1T1RabU5qSXROREl4T1MwMFlUVTBMV0UyWlRVdFptTXlNek0xTjJWaVl6SXg=\",\"client_id\":\"c3dc2e84-63ce-481b-8e67-75f754894f87\"}\n</code></pre> <p>Client Credentials - The example below shows the interaction with the APIKey API via curl and the response.</p> <p>In this deployment example the TTL is set to 74000 ms which is translated to seconds in the response.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;CLIENTID&lt;/role&gt;\n    &lt;param&gt;\n        &lt;name&gt;clientid.knox.token.ttl&lt;/name&gt;\n        &lt;value&gt;74000&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/service&gt;\n\n$ curl -ivku guest:guest-password -X POST \"https://localhost:8443/gateway/sandbox/clientid/api/v1/oauth/credentials\"\n{\"client_secret\":\"WXpKaE1qRmlOR0V0TkRBMk5DMDBNelZsTFdFek16RXR....WTVaVFprOjpZelJsTlRJMFlXVXROMlEwTXkwME5EQTVMV0k1WWpJdFlqZ3pOR00xTmpsa01qUXg=\",\"expires_in\":74,\"client_id\":\"c2a21b4a-4064-435e-a331-6d6858ef9e6d\"}\n</code></pre> <p>Note that in both of the above responses that there is a client_id and the client_secret. The key_id may be used in management operations of the API Key lifecycle by those with appropriate permissions to do so.</p>"},{"location":"config_ha/","title":"High Availability","text":""},{"location":"config_ha/#high-availability","title":"High Availability","text":"<p>This describes how Knox itself can be made highly available.</p> <p>All Knox instances must be configured to use the same topology credential keystores. These files are located under <code>{GATEWAY_HOME}/data/security/keystores/{TOPOLOGY_NAME}-credentials.jceks</code>. They are generated after the first topology deployment.</p> <p>In addition to these topology-specific credentials, gateway credentials and topologies must also be kept in-sync for Knox to operate in an HA manner.</p>"},{"location":"config_ha/#manually-synchronize-knox-instances","title":"Manually Synchronize Knox Instances","text":"<p>Here are the steps to manually sync topology credential keystores:</p> <ol> <li>Choose a Knox instance that will be the source for topology credential keystores. Let's call it keystores master</li> <li>Replace the topology credential keystores in the other Knox instances with topology credential keystores from the keystores master</li> <li>Restart Knox instances</li> </ol> <p>Manually synchronizing the gateway credentials and topologies involves using ssh/scp to copy the topology-related files to all the participating Knox instances, and running the Knox CLI on each participating instance to define the gateway credential aliases.</p> <p>This manual process can be tedious and error-prone. As such, ZooKeeper-based HA is recommended to simplify the management of these deployments.</p>"},{"location":"config_ha/#high-availability-with-apache-zookeeper","title":"High Availability with Apache ZooKeeper","text":"<p>Rather than manually keeping Knox HA instances in sync (in terms of credentials and topology), Knox can get it's state from Apache ZooKeeper. By configuring all the Knox instances to monitor the same ZooKeeper ensemble, they can be kept in-sync by modifying the topology-related configuration and/or credential aliases at only one of the instances (using the Admin UI, Admin API, or Knox CLI).</p>"},{"location":"config_ha/#what-is-automatically-synchronized-across-instances","title":"What is Automatically Synchronized Across Instances?","text":"<ul> <li>Provider Configurations</li> <li>Descriptors</li> <li>Topologies (generated only)</li> <li>Credential Aliases</li> </ul> <p>When a provider configuration or descriptor is added or updated to the ZooKeeper ensemble, all of the participating Knox instances will get the change, and the affected topologies will be [re]generated and [re]deployed. Similarly, if one of these is deleted, the affected topologies will be deleted and undeployed.</p> <p>When provider configurations and descriptors are added, modified or removed using the Admin UI or API (when the Knox instance is configured to monitor a ZooKeeper ensemble), then those changes will be automatically reflected in the associated ZooKeeper ensemble. Those changes will subsequently be consumed by all the other Knox instances monitoring that ensemble. By using the Admin UI or API, ssh/scp access to the Knox hosts can be avoided completely for the purpose of effecting topology changes.</p> <p>Similarly, when the Knox CLI is used to create or delete a gateway alias (when the Knox instance is configured to monitor a ZooKeeper ensemble), that alias change is reflected in the ZooKeeper ensemble, and all other Knox instances montoring that ensemble will apply the change.</p>"},{"location":"config_ha/#what-is-not-automatically-synchronized-across-instances","title":"What is NOT Automatically Synchronized Across Instances?","text":"<ul> <li>Topologies (XML)</li> <li>Gateway config (e.g., gateway-site, gateway-logging, etc...)</li> </ul> <p>If you're creating/modifying topology XML files directly, then there is no automated support for keeping these in sync across Knox HA instances.</p> <p>However, if the Knox instances are running in an Apache Ambari-managed cluster, there is limited support for keeping topology XML files and gateway configuration synchronized across those instances.</p> <p></p>"},{"location":"config_ha/#high-availability-with-apache-http-server-mod_proxy-mod_proxy_balancer","title":"High Availability with Apache HTTP Server + mod_proxy + mod_proxy_balancer","text":""},{"location":"config_ha/#1-requirements","title":"1 - Requirements","text":""},{"location":"config_ha/#openssl-devel","title":"openssl-devel","text":"<p>openssl-devel is required for Apache Module mod_ssl.</p> <pre><code>sudo yum install openssl-devel\n</code></pre>"},{"location":"config_ha/#apache-http-server","title":"Apache HTTP Server","text":"<p>Apache HTTP Server 2.4.6 or later is required. See this document for installing and setting up Apache HTTP Server: http://httpd.apache.org/docs/2.4/install.html</p> <p>Hint: pass <code>--enable-ssl</code> to the <code>./configure</code> command to enable the generation of the Apache Module mod_ssl.</p>"},{"location":"config_ha/#apache-module-mod_proxy","title":"Apache Module mod_proxy","text":"<p>See this document for setting up Apache Module mod_proxy: http://httpd.apache.org/docs/2.4/mod/mod_proxy.html</p>"},{"location":"config_ha/#apache-module-mod_proxy_balancer","title":"Apache Module mod_proxy_balancer","text":"<p>See this document for setting up Apache Module mod_proxy_balancer: http://httpd.apache.org/docs/2.4/mod/mod_proxy_balancer.html</p>"},{"location":"config_ha/#apache-module-mod_ssl","title":"Apache Module mod_ssl","text":"<p>See this document for setting up Apache Module mod_ssl: http://httpd.apache.org/docs/2.4/mod/mod_ssl.html</p>"},{"location":"config_ha/#2-configuration-example","title":"2 - Configuration example","text":""},{"location":"config_ha/#generate-certificate-for-apache-http-server","title":"Generate certificate for Apache HTTP Server","text":"<p>See this document for an example: http://www.akadia.com/services/ssh_test_certificate.html</p> <p>By convention, Apache HTTP Server and Knox certificates are put into the <code>/etc/apache2/ssl/</code> folder.</p>"},{"location":"config_ha/#update-apache-http-server-configuration-file","title":"Update Apache HTTP Server configuration file","text":"<p>This file is located under {APACHE_HOME}/conf/httpd.conf.</p> <p>Following directives have to be added or uncommented in the configuration file:</p> <ul> <li>LoadModule proxy_module modules/mod_proxy.so</li> <li>LoadModule proxy_http_module modules/mod_proxy_http.so</li> <li>LoadModule proxy_balancer_module modules/mod_proxy_balancer.so</li> <li>LoadModule ssl_module modules/mod_ssl.so</li> <li>LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so</li> <li>LoadModule lbmethod_bytraffic_module modules/mod_lbmethod_bytraffic.so</li> <li>LoadModule lbmethod_bybusyness_module modules/mod_lbmethod_bybusyness.so</li> <li>LoadModule lbmethod_heartbeat_module modules/mod_lbmethod_heartbeat.so</li> <li>LoadModule slotmem_shm_module modules/mod_slotmem_shm.so</li> </ul> <p>Also following lines have to be added to file. Replace placeholders (${...}) with real data:</p> <pre><code>Listen 443\n&lt;VirtualHost *:443&gt;\n   SSLEngine On\n   SSLProxyEngine On\n   SSLCertificateFile ${PATH_TO_CERTIFICATE_FILE}\n   SSLCertificateKeyFile ${PATH_TO_CERTIFICATE_KEY_FILE}\n   SSLProxyCACertificateFile ${PATH_TO_PROXY_CA_CERTIFICATE_FILE}\n\n   ProxyRequests Off\n   ProxyPreserveHost Off\n\n   RequestHeader set X-Forwarded-Port \"443\"\n   Header add Set-Cookie \"ROUTEID=.%{BALANCER_WORKER_ROUTE}e; path=/\" env=BALANCER_ROUTE_CHANGED\n   &lt;Proxy balancer://mycluster&gt;\n     BalancerMember ${HOST_#1} route=1\n     BalancerMember ${HOST_#2} route=2\n     ...\n     BalancerMember ${HOST_#N} route=N\n\n     ProxySet failontimeout=On lbmethod=${LB_METHOD} stickysession=ROUTEID \n   &lt;/Proxy&gt;\n\n   ProxyPass / balancer://mycluster/\n   ProxyPassReverse / balancer://mycluster/\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note:</p> <ul> <li>SSLProxyEngine enables SSL between Apache HTTP Server and Knox instances;</li> <li>SSLCertificateFile and SSLCertificateKeyFile have to point to certificate data of Apache HTTP Server. User will use this certificate for communications with Apache HTTP Server;</li> <li>SSLProxyCACertificateFile has to point to Knox certificates.</li> </ul>"},{"location":"config_ha/#startstop-apache-http-server","title":"Start/stop Apache HTTP Server","text":"<pre><code>APACHE_HOME/bin/apachectl -k start\nAPACHE_HOME/bin/apachectl -k stop\n</code></pre>"},{"location":"config_ha/#verify","title":"Verify","text":"<p>Use Knox samples.</p>"},{"location":"config_hadoop_auth_provider/","title":"Hadoop Auth","text":""},{"location":"config_hadoop_auth_provider/#hadoopauth-authentication-provider","title":"HadoopAuth Authentication Provider","text":"<p>The HadoopAuth authentication provider for Knox integrates the use of the Apache Hadoop module for SPNEGO and delegation token based authentication. This introduces the same authentication pattern used across much of the Hadoop ecosystem to Apache Knox and allows clients to using the strong authentication and SSO capabilities of Kerberos.</p>"},{"location":"config_hadoop_auth_provider/#configuration","title":"Configuration","text":""},{"location":"config_hadoop_auth_provider/#overview","title":"Overview","text":"<p>As with all providers in the Knox gateway, the HadoopAuth provider is configured through provider parameters. The configuration parameters are the same parameters used within Apache Hadoop for the same capabilities. In this section, we provide an example configuration and description of each of the parameters. We do encourage the reader to refer to the Hadoop documentation for this as well. (see http://hadoop.apache.org/docs/current/hadoop-auth/Configuration.html)</p> <p>One of the interesting things to note about this configuration is the use of the <code>config.prefix</code> parameter. In Hadoop there may be multiple components with their own specific configuration values for these parameters and since they may get mixed into the same Configuration object - there needs to be a way to identify the component specific values. The <code>config.prefix</code> parameter is used for this and is prepended to each of the configuration parameters for this provider. Below, you see an example configuration where the value for config.prefix happens to be <code>hadoop.auth.config</code>. You will also notice that this same value is prepended to the name of the rest of the configuration parameters.</p> <pre><code>&lt;provider&gt;\n  &lt;role&gt;authentication&lt;/role&gt;\n  &lt;name&gt;HadoopAuth&lt;/name&gt;\n  &lt;enabled&gt;true&lt;/enabled&gt;\n  &lt;param&gt;\n    &lt;name&gt;config.prefix&lt;/name&gt;\n    &lt;value&gt;hadoop.auth.config&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.signature.secret&lt;/name&gt;\n    &lt;value&gt;knox-signature-secret&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.type&lt;/name&gt;\n    &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.simple.anonymous.allowed&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.token.validity&lt;/name&gt;\n    &lt;value&gt;1800&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.cookie.domain&lt;/name&gt;\n    &lt;value&gt;novalocal&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.cookie.path&lt;/name&gt;\n    &lt;value&gt;gateway/default&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;HTTP/lmccay-knoxft-24m-r6-sec-160422-1327-2.novalocal@EXAMPLE.COM&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/security/keytabs/spnego.service.keytab&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;hadoop.auth.config.kerberos.name.rules&lt;/name&gt;\n    &lt;value&gt;DEFAULT&lt;/value&gt;\n  &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_hadoop_auth_provider/#descriptions","title":"Descriptions","text":"<p>The following tables describes the configuration parameters for the HadoopAuth provider:</p>"},{"location":"config_hadoop_auth_provider/#config","title":"Config","text":"Name Description Default config.prefix If specified, all other configuration parameter names must start with the prefix. none signature.secret This is the secret used to sign the delegation token in the hadoop.auth cookie. This same secret needs to be used across all instances of the Knox gateway in a given cluster. Otherwise, the delegation token will fail validation and authentication will be repeated each request. A simple random number type This parameter needs to be set to <code>kerberos</code> none, would throw exception simple.anonymous.allowed This should always be false for a secure deployment. true token.validity The validity -in seconds- of the generated authentication token. This is also used for the rollover interval when <code>signer.secret.provider</code> is set to random or ZooKeeper. 36000 seconds cookie.domain Domain to use for the HTTP cookie that stores the authentication token null cookie.path Path to use for the HTTP cookie that stores the authentication token null kerberos.principal The web-application Kerberos principal name. The Kerberos principal name must start with HTTP/.... For example: <code>HTTP/localhost@LOCALHOST</code> null kerberos.keytab The path to the keytab file containing the credentials for the kerberos principal. For example: <code>/Users/lmccay/lmccay.keytab</code> null kerberos.name.rules The name of the ruleset for extracting the username from the kerberos principal. DEFAULT"},{"location":"config_hadoop_auth_provider/#rest-invocation","title":"REST Invocation","text":"<p>Once a user logs in with kinit then their Kerberos session may be used across client requests with things like curl. The following curl command can be used to request a directory listing from HDFS while authenticating with SPNEGO via the <code>--negotiate</code> flag</p> <pre><code>curl -k -i --negotiate -u : https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre>"},{"location":"config_id_assertion/","title":"Identity Assertion","text":""},{"location":"config_id_assertion/#identity-assertion","title":"Identity Assertion","text":"<p>The identity assertion provider within Knox plays the critical role of communicating the identity principal to be used within the Hadoop cluster to represent the identity that has been authenticated at the gateway.</p> <p>The general responsibilities of the identity assertion provider is to interrogate the current Java Subject that has been established by the authentication or federation provider and:</p> <ol> <li>determine whether it matches any principal mapping rules and apply them appropriately</li> <li>determine whether it matches any group principal mapping rules and apply them</li> <li>if it is determined that the principal will be impersonating another through a principal mapping rule then a Subject.doAS is required so providers farther downstream can determine the appropriate effective principal name and groups for the user</li> </ol>"},{"location":"config_id_assertion/#default-identity-assertion-provider","title":"Default Identity Assertion Provider","text":"<p>The following configuration is required for asserting the users identity to the Hadoop cluster using Pseudo or Simple \"authentication\" and for using Kerberos/SPNEGO for secure clusters.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Default&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/provider&gt;\n</code></pre> <p>This particular configuration indicates that the Default identity assertion provider is enabled and that there are no principal mapping rules to apply to identities flowing from the authentication in the gateway to the backend Hadoop cluster services. The primary principal of the current subject will therefore be asserted via a query parameter or as a form parameter - ie. <code>?user.name={primaryPrincipal}</code></p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Default&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;principal.mapping&lt;/name&gt;\n        &lt;value&gt;guest=hdfs;&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;group.principal.mapping&lt;/name&gt;\n        &lt;value&gt;*=users;hdfs=admin&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n       &lt;name&gt;hadoop.proxyuser.impersonation.enabled&lt;/name&gt;\n       &lt;value&gt;false&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;hadoop.proxyuser.admin.users&lt;/name&gt;\n       &lt;value&gt;*&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;hadoop.proxyuser.admin.groups&lt;/name&gt;\n       &lt;value&gt;*&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;hadoop.proxyuser.admin.hosts&lt;/name&gt;\n       &lt;value&gt;*&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>This configuration identifies the same identity assertion provider but does provide principal and group mapping rules. In this case, when a user is authenticated as \"guest\" his identity is actually asserted to the Hadoop cluster as \"hdfs\". In addition, since there are group principal mappings defined, he will also be considered as a member of the groups \"users\" and \"admin\". In this particular example the wildcard \"*\" is used to indicate that all authenticated users need to be considered members of the \"users\" group and that only the user \"hdfs\" is mapped to be a member of the \"admin\" group.</p> <p>NOTE: These group memberships are currently only meaningful for Service Level Authorization using the AclsAuthorization provider. The groups are not currently asserted to the Hadoop cluster at this time. See the Authorization section within this guide to see how this is used.</p> <p>The principal mapping aspect of the identity assertion provider is important to understand in order to fully utilize the authorization features of this provider.</p> <p>This feature allows us to map the authenticated principal to a runAs or impersonated principal to be asserted to the Hadoop services in the backend.</p> <p>When a principal mapping is defined that results in an impersonated principal, this impersonated principal is then the effective principal.</p> <p>If there is no mapping to another principal then the authenticated or primary principal is the effective principal.</p> <p>Another way to impersonate principals is to apply Hadoop Proxyuser-based impersonations as described in the next section.</p>"},{"location":"config_id_assertion/#hadoop-proxyuser-impersonation","title":"Hadoop Proxyuser impersonation","text":"<p>From v2.0.0, an authenticated user can impersonate other user(s) leveraging Hadoop's proxuyser configuration mechanism. This feature was implemented in KNOX-2839 and requires the following configuration to work:</p> <ul> <li><code>hadoop.proxyuser.impersonation.enabled</code> - a <code>boolean</code> flag indicates if token impersonation is enabled. Defaults to <code>true</code></li> <li><code>hadoop.proxyuser.$username.users</code>  - indicates the list of users for whom <code>$username</code> is allowed to impersonate. It is possible to set this to a 1-element list using the <code>*</code> wildcard which means <code>$username</code> can impersonate everyone. Defaults to an empty list that is equivalent to  <code>$username</code> is not allowed to impersonate anyone.</li> <li><code>hadoop.proxyuser.$username.groups</code>  - indicates the list of group names for whose members <code>$username</code> is allowed to impersonate. It is possible to set this to a 1-element list using the <code>*</code> wildcard which means <code>$username</code> can impersonate members of any group. Defaults to an empty list that is equivalent to <code>$username</code> is not allowed to impersonate members from any group.</li> <li><code>hadoop.proxyuser.$username.hosts</code>  - indicates a list of hostnames from where the requests are allowed to be accepted in case the <code>doAs</code> parameter is used when impersonating requests. It is possible to set this to a 1-element list using the <code>*</code> wildcard which means <code>$username</code> can impersonate incoming requests from any host. Defaults to an empty list that is equivalent to <code>$username</code> is not allowed to impersonate requests from any host.</li> </ul> <p>Please note this configuration is applied iff the <code>doAs</code> query parameter is present in the incoming request and impersonation is enabled in the affected topology.</p> <p>Important note: this new-type impersonation support on the identity assertion layer is ignored if the topology uses the <code>HadoopAuth</code> authentication provider because the <code>doAs</code> support is working OOTB there, therefore a second authorization is useless going forward.</p> <p>It's also worth articulating that Hadoop Proxyuser-based impersonation works together with the already existing principal mapping (see below). At first, Knox applies the Hadoop Proxyuser impersonation, then it proceeds with principal mappings (if any). Let see a sample:</p> <ul> <li><code>hadoop.proxyuser.admin.users</code> is set to <code>bob</code> (<code>admin</code> is allowed to impersonate <code>bob</code>)</li> <li><code>principal.mapping</code> is set to <code>bob=tom</code> (<code>bob</code> is mapped as <code>tom</code> )</li> </ul> <p>The <code>admin</code> user sends the following request:</p> <pre><code>curl https://KNOX_HOST:8443/gateway/sandbox/service/path?doAs=bob\n</code></pre> <p>In the request processing flow, after the identity assertion phase is completed, <code>tom</code> will be the effective user. As you can see, the rules were applied transitively.</p> <p>For other use cases you may want to check out GitHub Pull Request #681.</p>"},{"location":"config_id_assertion/#principal-mapping","title":"Principal Mapping","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;principal.mapping&lt;/name&gt;\n    &lt;value&gt;{primaryPrincipal}[,...]={impersonatedPrincipal}[;...]&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;principal.mapping&lt;/name&gt;\n    &lt;value&gt;guest=hdfs&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>For multiple mappings:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;principal.mapping&lt;/name&gt;\n    &lt;value&gt;guest,alice=hdfs;mary=alice2&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_id_assertion/#expression-based-principal-mapping","title":"Expression-Based Principal Mapping","text":"<p>Alternatively, you can use an expression language to define principal mappings.</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;expression.principal.mapping&lt;/name&gt;\n  &lt;!-- expression that returns the new principal --&gt;\n  &lt;value&gt;...&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>The value of <code>expression.principal.mapping</code> must be a valid expression that evaluates to a string. </p> <p>For example, the following expression will map all users to one constant user, 'bob'.</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;expression.principal.mapping&lt;/name&gt;\n  &lt;value&gt;'bob'&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>By adding a conditional you can selectively apply the mapping to specific users.</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;expression.principal.mapping&lt;/name&gt;\n  &lt;!-- Only map sam/tom to bob --&gt;\n  &lt;value&gt;\n    (if (or (= username 'sam') \n            (= username 'tom')) \n        'bob')\n  &lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>The <code>if</code> expression expects ether 2 or 3 parameters. The first one is always a conditional that should return a boolean value. The second parameter is the consequent branch that is only evaluated if the conditional is true. The third, optional part is the alternative branch that is evaluated if the conditional is false.</p> <pre><code>(if (&lt; (strlen username) 5)\n    (concat username '_suffix') \n    (concat 'prefix_' username))\n</code></pre> <p>Here the user <code>admin</code> will be mapped to <code>prefix_admin</code>, while <code>sam</code> will be mapped to <code>sam_suffix</code>.</p> <p>In an XML topology, the less than and greater than operators should be either encoded as <code>&amp;lt;</code> <code>&amp;gt;</code>, or the expression should be put inside a CDATA section.</p> <pre><code>(&amp;lt; (strlen username) 5)\n</code></pre> <p>The following expression capitalizes the principal:</p> <pre><code>(concat\n  (uppercase (substr username 0 1))\n  (lowercase (substr username 1)))\n</code></pre> <p>The functionality of the Regex-based identity assertion provider is exposed via the <code>regex-template</code> function.</p> <pre><code>(regex-template  username '(.*)@(.*?)\\..*' '{1}_{[2]}' (hash 'us' 'USA' 'ca' 'CANADA') true)\n</code></pre> <p>The above expression turns <code>nobody@us.imaginary.tld</code> to <code>nobody_USA</code>.</p> <p>See KNOX-2983 for the complete list of functions.</p>"},{"location":"config_id_assertion/#group-principal-mapping","title":"Group Principal Mapping","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;group.principal.mapping&lt;/name&gt;\n    &lt;value&gt;{userName[,*|userName...]}={groupName[,groupName...]}[,...]&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;group.principal.mapping&lt;/name&gt;\n    &lt;value&gt;*=users;hdfs=admin&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>this configuration indicates that all (*) authenticated users are members of the \"users\" group and that user \"hdfs\" is a member of the admin group. Group principal mapping has been added along with the authorization provider described in this document.</p>"},{"location":"config_id_assertion/#group-mapping-based-on-predicates","title":"Group Mapping Based on Predicates","text":"<pre><code>&lt;param&gt;\n    &lt;name&gt;group.mapping.{mappedGroupName}&lt;/name&gt;\n    &lt;value&gt;{predicate}&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>The predicate-based group mapping offers more flexibility by allowing the use of arbitrary logical expressions to define the mappings. The predicate expression must evaluate to a boolean result, true or false. The syntax is inspired by the Lisp language, but it is limited to boolean expressions and comparisons.</p> <p>For instance:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;group.mapping.admin&lt;/name&gt;\n    &lt;value&gt;(or (username 'guest') (member 'analyst'))&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>This configuration maps the user to the \"admin\" group if either the authenticated user is \"guest\" or the authenticated user is a member of the \"analyst\" group.</p> <p>The syntax of the language is based on parenthesized prefix notation, meaning that the operators precede their operands.</p> <p>The language is made up of two basic building blocks: atoms (boolean, string, number, symbol) and lists. A list is written with its elements separated by whitespace, and surrounded by parentheses. A list can contain other lists or atoms.</p> <p>A function call or an operator is written as a list with the function or operator's name first, and the arguments following. For instance, a function f that takes three arguments would be called as (f arg1 arg2 arg3).</p> <p>Lists are of arbitrary length and can be nested to express more complex conditionals.</p> <pre><code>(or \n    (and\n        (member 'admin')\n        (member 'scientist'))\n    (or\n        (username 'tom')\n        (username 'sam')))\n</code></pre> <p>Returns:</p> <ol> <li>True if the user is either 'tom' or 'sam'</li> <li>True if the user is both in the 'admin' and the 'scientist' group.</li> <li>False otherwise.</li> </ol> <p>The following predicate checks if the user is a member of any group:</p> <pre><code>(!= (size groups) 0)\n\n(not (empty groups))\n\n(match groups '.*')\n</code></pre> <p>This predicate checks if the username is either \"tom\" or \"sam\":</p> <pre><code>(match username 'tom|sam')\n</code></pre> <p>This checks the username in a case insensitive manner:</p> <pre><code>(= (lowercase username) 'bob')\n</code></pre>"},{"location":"config_id_assertion/#supported-functions-and-operators","title":"Supported functions and operators","text":""},{"location":"config_id_assertion/#or","title":"or","text":"<p>Evaluates true if one or more of its operands is true. Supports short-circuit evaluation and variable number of arguments.</p> <p>Number of arguments: 1..N</p> <pre><code>(or bool1 bool2 ... boolN)\n</code></pre> <p>Example</p> <pre><code>(or true false true)\n</code></pre>"},{"location":"config_id_assertion/#and","title":"and","text":"<p>Evaluates true if all of its operands are true. Supports short-circuit evaluation and variable number of arguments.</p> <p>Number of arguments: 1..N</p> <pre><code>(and bool1 bool2 ... boolN)\n</code></pre> <p>Example</p> <pre><code> (and true false true)\n</code></pre>"},{"location":"config_id_assertion/#not","title":"not","text":"<p>Negates the operand.</p> <p>Number of arguments: 1</p> <pre><code> (not aBool)\n</code></pre> <p>Example</p> <pre><code> (not true)\n</code></pre>"},{"location":"config_id_assertion/#_1","title":"=","text":"<p>Evaluates true if the two operands are equal.</p> <p>Number of arguments: 2</p> <pre><code> (= op1 op2)\n</code></pre> <p>Example</p> <pre><code> (= 'apple' 'orange')\n</code></pre>"},{"location":"config_id_assertion/#_2","title":"!=","text":"<p>Evaluates true if the two operands are not equal.</p> <p>Number of arguments: 2</p> <pre><code> (!= op1 op2)\n</code></pre> <p>Example</p> <pre><code>  (!= 'apple' 'orange')\n</code></pre>"},{"location":"config_id_assertion/#member","title":"member","text":"<p>Evaluates true if the current user is a member of the given group</p> <p>Number of arguments: 1</p> <pre><code> (member aString)\n</code></pre> <p>Example</p> <pre><code> (member 'analyst')\n</code></pre>"},{"location":"config_id_assertion/#username","title":"username","text":"<p>Evaluates true if the current user has the given username</p> <p>Number of arguments: 1</p> <pre><code>(username aString)\n</code></pre> <p>Example</p> <pre><code>(username 'admin')\n</code></pre> <p>This is a shorter version of (= username 'admin')</p>"},{"location":"config_id_assertion/#size","title":"size","text":"<p>Gets the size of a list</p> <p>Number of arguments: 1</p> <pre><code> (size alist)\n</code></pre> <p>Example</p> <pre><code> (size groups)\n</code></pre>"},{"location":"config_id_assertion/#empty","title":"empty","text":"<p>Evaluates to true if the given list is empty</p> <p>Number of arguments: 1</p> <pre><code> (empty alist)\n</code></pre> <p>Example</p> <pre><code> (empty groups)\n</code></pre>"},{"location":"config_id_assertion/#match","title":"match","text":"<p>Evaluates true if the given string matches to the given regexp. Or any items of the given list matches the given regexp.</p> <p>Number of arguments: 2</p> <pre><code>(match aString aRegExpString)\n\n(match aList aRegExpString)\n</code></pre> <p>Example</p> <pre><code>(match username 'tom|sam')\n</code></pre> <p>This function can also take a list as a first argument. In this case it will return true if the regexp matches to any of the items in the list.</p> <pre><code>(match groups 'analyst|scientist')\n</code></pre> <p>This returns true if the user is either in the 'analyst' group or in the 'scientist' group. The same can be expressed by combining 2 member functions with an or expression.</p>"},{"location":"config_id_assertion/#request-header","title":"request-header","text":"<p>Returns the value of the specified request header as a String. If the given key doesn't exist empty string is returned.</p> <p>Number of arguments: 1</p> <pre><code>(request-header aString)\n</code></pre> <p>Example</p> <pre><code>(request-header 'User-Agent')\n</code></pre>"},{"location":"config_id_assertion/#request-attribute","title":"request-attribute","text":"<p>Returns the value of the specified request attribute as a String. If the given key doesn't exist empty string is returned.</p> <p>Number of arguments: 1</p> <pre><code>(request-attribute aString)\n</code></pre> <p>Example</p> <pre><code>(request-attribute 'sourceRequestUrl')\n</code></pre>"},{"location":"config_id_assertion/#session","title":"session","text":"<p>Returns the value of the specified session attribute as a String. If the given key doesn't exist empty string is returned.</p> <p>Number of arguments: 1</p> <pre><code> (session aString)\n</code></pre> <p>Example</p> <pre><code> (session 'subject.userRoles')\n</code></pre>"},{"location":"config_id_assertion/#lowercase","title":"lowercase","text":"<p>Converts the given string to lowercase.</p> <p>Number of arguments: 1</p> <pre><code> (lowercase aString)\n</code></pre> <p>Example</p> <pre><code> (lowercase 'KNOX')\n</code></pre>"},{"location":"config_id_assertion/#uppercase","title":"uppercase","text":"<p>Converts the given string to uppercase.</p> <p>Number of arguments: 1</p> <pre><code> (uppercase aString)\n</code></pre> <p>Example</p> <pre><code>(uppercase 'knox')\n</code></pre>"},{"location":"config_id_assertion/#constants","title":"Constants","text":"<p>The following constants are populated automatically from the current security context.</p>"},{"location":"config_id_assertion/#username_1","title":"username","text":"<p>The username (principal) of the current user, derived from javax.security.auth.Subject.</p>"},{"location":"config_id_assertion/#groups","title":"groups","text":"<p>The groups of the current user (as determined by the authentication provider), derived from subject.getPrincipals(GroupPrincipal.class).</p>"},{"location":"config_id_assertion/#concat-identity-assertion-provider","title":"Concat Identity Assertion Provider","text":"<p>The Concat identity assertion provider allows for composition of a new user principal through the concatenation of optionally configured prefix and/or suffix provider parameters. This is a useful assertion provider for converting an incoming identity into a disambiguated identity within the Hadoop cluster based on what topology is used to access Hadoop.</p> <p>The following configuration would convert the user principal into a value that represents a domain specific identity where the identities used inside the Hadoop cluster represent this same separation.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Concat&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;concat.suffix&lt;/name&gt;\n        &lt;value&gt;_domain1&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The above configuration will result in all user interactions through that topology to have their principal communicated to the Hadoop cluster with a domain designator concatenated to the username. Possibly useful for multi-tenant deployment scenarios.</p> <p>In addition to the concat.suffix parameter, the provider supports the setting of a prefix through a <code>concat.prefix</code> parameter.</p>"},{"location":"config_id_assertion/#switchcase-identity-assertion-provider","title":"SwitchCase Identity Assertion Provider","text":"<p>The SwitchCase identity assertion provider solves issues where down stream ecosystem components require user and group principal names to be a specific case. An example of how this provider is enabled and configured within the <code>&lt;gateway&gt;</code> section of a topology file is shown below. This particular example will switch user principals names to lower case and group principal names to upper case.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;SwitchCase&lt;/name&gt;\n    &lt;param&gt;\n        &lt;name&gt;principal.case&lt;/name&gt;\n        &lt;value&gt;lower&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;group.principal.case&lt;/name&gt;\n        &lt;value&gt;upper&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/provider&gt;\n</code></pre> <p>These are the configuration parameters used to control the behavior of the provider.</p> Param Description principal.case The case mapping of user principal names. Choices are: lower, upper, none.  Defaults to lower. group.principal.case The case mapping of group principal names. Choices are: lower, upper, none. Defaults to setting of principal.case. <p>If no parameters are provided the full defaults will results in both user and group principal names being switched to lower case. A setting of \"none\" or anything other than \"upper\" or \"lower\" leaves the case of the principal name unchanged.</p>"},{"location":"config_id_assertion/#regular-expression-identity-assertion-provider","title":"Regular Expression Identity Assertion Provider","text":"<p>The regular expression identity assertion provider allows incoming identities to be translated using a regular expression, template and lookup table. This will probably be most useful in conjunction with the HeaderPreAuth federation provider.</p> <p>There are three configuration parameters used to control the behavior of the provider.</p> Param Description input This is a regular expression that will be applied to the incoming identity. The most critical part of the regular expression is the group notation within the expression. In regular expressions, groups are expressed within parenthesis. For example in the regular expression \"<code>(.*)@(.*?)\\..*</code>\" there are two groups. When this regular expression is applied to \"nobody@us.imaginary.tld\" group 1 matches \"nobody\" and group 2 matches \"us\". output This is a template that assembles the result identity. The result is assembled from the static text and the matched groups from the input regular expression. In addition, the matched group values can be looked up in the lookup table. An output value of \"<code>{1}_{2}</code>\" of will result in \"nobody_us\". lookup This lookup table provides a simple (albeit limited) way to translate text in the incoming identities. This configuration takes the form of \"=\" separated name values pairs separated by \";\". For example a lookup setting is \"us=USA;ca=CANADA\". The lookup is invoked in the output setting by surrounding the desired group number in square brackets (i.e. []). Putting it all together, output setting of \"<code>{1}_[{2}]</code>\" combined with input of \"<code>(.*)@(.*?)\\..*</code>\" and lookup of \"us=USA;ca=CANADA\" will turn \"nobody@us.imaginary.tld\" into \"nobody@USA\". use.original.on.lookup.failure (Optional) Default value is false. If set to true, it will preserve the original string if there is no match. e.g. In the above lookup case for email nobody@uk.imaginary.tld, it will be transformed to nobody@ , if this property is set to true it will be transformed to  nobody@uk. <p>Within the topology file the provider configuration might look like this.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Regex&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;input&lt;/name&gt;\n        &lt;value&gt;(.*)@(.*?)\\..*&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;output&lt;/name&gt;\n        &lt;value&gt;{1}_{[2]}&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;lookup&lt;/name&gt;\n        &lt;value&gt;us=USA;ca=CANADA&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>Using curl with this type of configuration might produce the following results. </p> <pre><code>curl -k --header \"SM_USER: nobody@us.imaginary.tld\" 'https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY'\n\n{\"Path\":\"/user/member_USA\"}\n\nurl -k --header \"SM_USER: nobody@ca.imaginary.tld\" 'https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY'\n\n{\"Path\":\"/user/member_CANADA\"}\n</code></pre>"},{"location":"config_id_assertion/#hadoop-group-lookup-provider","title":"Hadoop Group Lookup Provider","text":"<p>An identity assertion provider that looks up user's 'group membership' for authenticated users using Hadoop's group mapping service (GroupMappingServiceProvider).</p> <p>This allows existing investments in the Hadoop to be leveraged within Knox and used within the access control policy enforcement at the perimeter.</p> <p>The 'role' for this provider is 'identity-assertion' and name is 'HadoopGroupProvider'.</p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;identity-assertion&lt;/role&gt;\n        &lt;name&gt;HadoopGroupProvider&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;&lt;param&gt; ... &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre>"},{"location":"config_id_assertion/#configuration","title":"Configuration","text":"<p>All the configuration for 'HadoopGroupProvider' resides in the provider section in a gateway topology file. The 'hadoop.security.group.mapping' property determines the implementation. This configuration may be centralized within the gateway-site.xml through the use of a special param to this provider called CENTRAL_GROUP_CONFIG_PREFIX. This indicates to the provider that the required configuration can be found within the gateway-site.xml file with the provided prefix.</p> <pre><code>     &lt;param&gt;\n        &lt;name&gt;CENTRAL_GROUP_CONFIG_PREFIX&lt;/name&gt;\n        &lt;value&gt;gateway.group.config.&lt;/value&gt;\n     &lt;/param&gt;\n</code></pre> <p>Some of the valid implementations are as follows: </p>"},{"location":"config_id_assertion/#orgapachehadoopsecurityjnibasedunixgroupsmappingwithfallback","title":"org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback","text":"<p>This is the default implementation and will be picked up if 'hadoop.security.group.mapping' is not specified. This implementation will determine if the Java Native Interface (JNI) is available. If JNI is available, the implementation will use the API within Hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, <code>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</code>, is used, which shells out with the <code>bash -c id -gn &lt;user&gt; ; id -Gn &lt;user&gt;</code> command (for a Linux/Unix environment) or the <code>groups -F &lt;user&gt;</code> command (for a Windows environment) to resolve a list of groups for a user.</p>"},{"location":"config_id_assertion/#orgapachehadoopsecurityjnibasedunixgroupsnetgroupmappingwithfallback","title":"org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback","text":"<p>As above, if JNI is available then we get the netgroup membership using Hadoop native API, else fallback on ShellBasedUnixGroupsNetgroupMapping to resolve list of groups for a user.</p>"},{"location":"config_id_assertion/#orgapachehadoopsecurityshellbasedunixgroupsmapping","title":"org.apache.hadoop.security.ShellBasedUnixGroupsMapping","text":"<p>Uses the <code>bash -c id -gn &lt;user&gt; ; id -Gn &lt;user&gt;</code> command (for a Linux/Unix environment) or the <code>groups -F &lt;user&gt;</code> command (for a Windows environment) to resolve list of groups for a user.</p>"},{"location":"config_id_assertion/#orgapachehadoopsecurityshellbasedunixgroupsnetgroupmapping","title":"org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping","text":"<p>Similar to <code>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</code> except it uses <code>getent netgroup</code> command to get netgroup membership.</p>"},{"location":"config_id_assertion/#orgapachehadoopsecurityldapgroupsmapping","title":"org.apache.hadoop.security.LdapGroupsMapping","text":"<p>This implementation connects directly to an LDAP server to resolve the list of groups. However, this should only be used if the required groups reside exclusively in LDAP, and are not materialized on the Unix servers.</p>"},{"location":"config_id_assertion/#orgapachehadoopsecuritycompositegroupsmapping","title":"org.apache.hadoop.security.CompositeGroupsMapping","text":"<p>This implementation asks multiple other group mapping providers for determining group membership, see Composite Groups Mapping for more details.</p> <p>For more information on the implementation and properties refer to Hadoop Group Mapping.</p>"},{"location":"config_id_assertion/#example","title":"Example","text":"<p>The following example snippet works with the demo ldap server that ships with Apache Knox. Replace the existing 'Default' identity-assertion provider with the one below (HadoopGroupProvider).</p> <pre><code>    &lt;provider&gt;\n        &lt;role&gt;identity-assertion&lt;/role&gt;\n        &lt;name&gt;HadoopGroupProvider&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;\n            &lt;value&gt;org.apache.hadoop.security.LdapGroupsMapping&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.bind.user&lt;/name&gt;\n            &lt;value&gt;uid=tom,ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.bind.password&lt;/name&gt;\n            &lt;value&gt;tom-password&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.url&lt;/name&gt;\n            &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.base&lt;/name&gt;\n            &lt;value&gt;&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.search.filter.user&lt;/name&gt;\n            &lt;value&gt;(&amp;amp;(|(objectclass=person)(objectclass=applicationProcess))(cn={0}))&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.search.filter.group&lt;/name&gt;\n            &lt;value&gt;(objectclass=groupOfNames)&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.search.attr.member&lt;/name&gt;\n            &lt;value&gt;member&lt;/value&gt;\n        &lt;/param&gt;\n        &lt;param&gt;\n            &lt;name&gt;hadoop.security.group.mapping.ldap.search.attr.group.name&lt;/name&gt;\n            &lt;value&gt;cn&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n</code></pre> <p>Here, we are working with the demo LDAP server running at 'ldap://localhost:33389' which populates some dummy users for testing that we will use in this example. This example uses the user 'tom' for LDAP binding. If you have different LDAP/AD settings, you will have to update the properties accordingly. </p> <p>Let's test our setup using the following command (assuming the gateway is started and listening on localhost:8443). Note that we are using credentials for the user 'sam' along with the command. </p> <pre><code>    curl -i -k -u sam:sam-password -X GET 'https://localhost:8443/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS'\n</code></pre> <p>The command should be executed successfully and you should see the groups 'scientist' and 'analyst' to which user 'sam' belongs to in gateway-audit.log i.e.</p> <pre><code>    ||a99aa0ab-fc06-48f2-8df3-36e6fe37c230|audit|WEBHDFS|sam|||identity-mapping|principal|sam|success|Groups: [scientist, analyst]\n</code></pre>"},{"location":"config_kerberos/","title":"Knox Security Config","text":""},{"location":"config_kerberos/#secure-clusters","title":"Secure Clusters","text":"<p>See the Hadoop documentation for setting up a secure Hadoop cluster http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html</p> <p>Once you have a Hadoop cluster that is using Kerberos for authentication, you have to do the following to configure Knox to work with that cluster.</p>"},{"location":"config_kerberos/#create-unix-account-for-knox-on-hadoop-master-nodes","title":"Create Unix account for Knox on Hadoop master nodes","text":"<pre><code>useradd -g hadoop knox\n</code></pre>"},{"location":"config_kerberos/#create-kerberos-principal-keytab-for-knox","title":"Create Kerberos principal, keytab for Knox","text":"<p>One way of doing this, assuming your KDC realm is EXAMPLE.COM, is to ssh into your host running KDC and execute <code>kadmin.local</code> That will result in an interactive session in which you can execute commands.</p> <p>ssh into your host running KDC</p> <pre><code>kadmin.local\nadd_principal -randkey knox/knox@EXAMPLE.COM\nktadd -k knox.service.keytab -norandkey knox/knox@EXAMPLE.COM\nexit\n</code></pre>"},{"location":"config_kerberos/#copy-knox-keytab-to-knox-host","title":"Copy knox keytab to Knox host","text":"<p>Add unix account for the knox user on Knox host</p> <pre><code>useradd -g hadoop knox\n</code></pre> <p>Copy knox.service.keytab created on KDC host on to your Knox host <code>{GATEWAY_HOME}/conf/knox.service.keytab</code></p> <pre><code>chown knox knox.service.keytab\nchmod 400 knox.service.keytab\n</code></pre>"},{"location":"config_kerberos/#update-krb5conf-at-gateway_homeconfkrb5conf-on-knox-host","title":"Update <code>krb5.conf</code> at <code>{GATEWAY_HOME}/conf/krb5.conf</code> on Knox host","text":"<p>You could copy the <code>{GATEWAY_HOME}/templates/krb5.conf</code> file provided in the Knox binary download and customize it to suit your cluster.</p>"},{"location":"config_kerberos/#update-krb5jaasloginconf-at-etcknoxconfkrb5jaasloginconf-on-knox-host","title":"Update <code>krb5JAASLogin.conf</code> at <code>/etc/knox/conf/krb5JAASLogin.conf</code> on Knox host","text":"<p>You could copy the <code>{GATEWAY_HOME}/templates/krb5JAASLogin.conf</code> file provided in the Knox binary download and customize it to suit your cluster.</p>"},{"location":"config_kerberos/#update-gateway-sitexml-on-knox-host","title":"Update <code>gateway-site.xml</code> on Knox host","text":"<p>Update <code>conf/gateway-site.xml</code> in your Knox installation and set the value of <code>gateway.hadoop.kerberos.secured</code> to true.</p>"},{"location":"config_kerberos/#restart-knox","title":"Restart Knox","text":"<p>After you do the above configurations and restart Knox, Knox would use SPNEGO to authenticate with Hadoop services and Oozie. There is no change in the way you make calls to Knox whether you use curl or Knox DSL.</p>"},{"location":"config_knox_sso/","title":"Knox SSO","text":""},{"location":"config_knox_sso/#knoxsso-setup-and-configuration","title":"KnoxSSO Setup and Configuration","text":""},{"location":"config_knox_sso/#introduction","title":"Introduction","text":"<p>Authentication of the Hadoop component UIs, and those of the overall ecosystem, is usually limited to Kerberos (which requires SPNEGO to be configured for the user's browser) and simple/pseudo. This often results in the UIs not being secured - even in secured clusters. This is where KnoxSSO provides value by providing WebSSO capabilities to the Hadoop cluster.</p> <p>By leveraging the hadoop-auth module in Hadoop common, we have introduced the ability to consume a common SSO cookie for web UIs while retaining the non-web browser authentication through kerberos/SPNEGO. We do this by extending the AltKerberosAuthenticationHandler class which provides the useragent based multiplexing. </p> <p>We also provide integration guidance within the developers guide for other applications to be able to participate in these SSO capabilities.</p> <p>The flexibility of the Apache Knox authentication and federation providers allows KnoxSSO to provide a normalization of authentication events through token exchange resulting in a common JWT (JSON WebToken) based token.</p> <p>KnoxSSO provides an abstraction for integrating any number of authentication systems and SSO solutions and enables participating web applications to scale to those solutions more easily. Without the token exchange capabilities offered by KnoxSSO each component UI would need to integrate with each desired solution on its own. With KnoxSSO they only need to integrate with the single solution and common token.</p> <p>In addition, KnoxSSO comes with its own form-based IdP. This allows for easily integrating a form-based login with the enterprise AD/LDAP server.</p> <p>This document describes the overall setup requirements for KnoxSSO and participating applications.</p> <p>In v2.0.0, the Knox team implemented an extension to the KnoxSSO feature that controls the number of concurrent UI sessions the users can have. For more informstion please check out the Concurrent Session Verification section below.</p>"},{"location":"config_knox_sso/#form-based-idp-setup","title":"Form-based IdP Setup","text":"<p>By default the <code>knoxsso.xml</code> topology contains an application element for the knoxauth login application. This is a simple single page application for providing a login page and authenticating the user with HTTP basic auth against AD/LDAP.</p> <pre><code>&lt;application&gt;\n    &lt;name&gt;knoxauth&lt;/name&gt;\n&lt;/application&gt;\n</code></pre> <p>The Shiro Provider has specialized configuration beyond the typical HTTP Basic authentication requirements for REST APIs or other non-knoxauth applications. You will notice below that there are a couple additional elements - namely, redirectToUrl and restrictedCookies with WWW-Authenticate. These are used to short-circuit the browser's HTTP basic dialog challenge so that we can use a form instead.</p> <pre><code>&lt;provider&gt;\n   &lt;role&gt;authentication&lt;/role&gt;\n   &lt;name&gt;ShiroProvider&lt;/name&gt;\n   &lt;enabled&gt;true&lt;/enabled&gt;\n   &lt;param&gt;\n      &lt;name&gt;sessionTimeout&lt;/name&gt;\n      &lt;value&gt;30&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;redirectToUrl&lt;/name&gt;\n      &lt;value&gt;/gateway/knoxsso/knoxauth/login.html&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;restrictedCookies&lt;/name&gt;\n      &lt;value&gt;rememberme,WWW-Authenticate&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm&lt;/name&gt;\n      &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n      &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n      &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n      &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n      &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm.authenticationCachingEnabled&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n      &lt;value&gt;simple&lt;/value&gt;\n   &lt;/param&gt;\n   &lt;param&gt;\n      &lt;name&gt;urls./**&lt;/name&gt;\n      &lt;value&gt;authcBasic&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_knox_sso/#knoxsso-service-setup","title":"KnoxSSO Service Setup","text":""},{"location":"config_knox_sso/#knoxssoxml-topology","title":"knoxsso.xml Topology","text":"<p>To enable KnoxSSO, we use the KnoxSSO topology for exposing an API that can be used to abstract the use of any number of enterprise or customer IdPs. By default, the <code>knoxsso.xml</code> file is configured for using the simple KnoxAuth application for form-based authentication against LDAP/AD. By swapping the Shiro authentication provider that is there out-of-the-box with another authentication or federation provider, an admin may leverage many of the existing providers for SSO for the UI components that participate in KnoxSSO.</p> <p>Just as with any Knox service, the KNOXSSO service is protected by the gateway providers defined above it. In this case, the ShiroProvider is taking care of HTTP Basic Auth against LDAP for us. Once the user authenticates the request processing continues to the KNOXSSO service that will create the required cookie and do the necessary redirects.</p> <p>The knoxsso.xml topology will result in a KnoxSSO URL that looks something like:</p> <pre><code>https://{gateway_host}:{gateway_port}/gateway/knoxsso/api/v1/websso\n</code></pre> <p>This URL is needed when configuring applications that participate in KnoxSSO for a given deployment. We will refer to this as the Provider URL.</p>"},{"location":"config_knox_sso/#knoxsso-configuration-parameters","title":"KnoxSSO Configuration Parameters","text":"Parameter Description Default knoxsso.cookie.name This optional setting allows the admin to set the name of the sso cookie to use to represent a successful authentication event. hadoop-jwt knoxsso.cookie.secure.only This determines whether the browser is allowed to send the cookie over unsecured channels. This should always be set to true in production systems. If during development a relying party is not running SSL then you can turn this off. Running with it off exposes the cookie and underlying token for capture and replay by others. The value of the gateway-site property named <code>ssl.enabled</code> (which defaults to <code>true</code>). knoxsso.cookie.max.age optional: This indicates that a cookie can only live for a specified amount of time - in seconds. This should probably be left to the default which makes it a session cookie. Session cookies are discarded once the browser session is closed. session knoxsso.cookie.domain.suffix optional: This indicates the portion of the request hostname that represents the domain to be used for the cookie domain. For single host development scenarios, the default behavior should be fine. For production deployments, the expected domain should be set and all configured URLs that are related to SSO should use this domain. Otherwise, the cookie will not be presented by the browser to mismatched URLs. Default cookie domain or a domain derived from a hostname that includes more than 2 dots. knoxsso.token.ttl This indicates the lifespan of the token within the cookie. Once it expires a new cookie must be acquired from KnoxSSO. This is in milliseconds. The 36000000 in the topology above gives you 10 hrs. 30000 That is 30 seconds. knoxsso.token.audiences This is a comma separated list of audiences to add to the JWT token. This is used to ensure that a token received by a participating application knows that the token was intended for use with that application. It is optional. In the event that an application has expected audiences and they are not present the token must be rejected. In the event where the token has audiences and the application has none expected then the token is accepted. empty knoxsso.redirect.whitelist.regex A semicolon-delimited list of regular expressions. The incoming originalUrl must match one of the expressions in order for KnoxSSO to redirect to it after authentication. Note that cookie use is still constrained to redirect destinations in the same domain as the KnoxSSO service - regardless of the expressions specified here. The value of the gateway-site property named gateway.dispatch.whitelist. If that is not defined, the default allows only relative paths, localhost or destinations in the same domain as the Knox host (with or without SSL). This may need to be opened up for production use and actual participating applications. knoxsso.expected.params Optional: Comma separated list of query parameters that are expected and consumed by KnoxSSO and will not be passed on to originalUrl empty knoxsso.signingkey.keystore.name Optional: name of a JKS keystore in gateway security directory that has required signing key certificate empty knoxsso.signingkey.keystore.alias Optional: alias of the signing key certificate in the <code>knoxsso.signingkey.keystore.name</code> keystore empty knoxsso.signingkey.keystore.passphrase.alias Optional: passphrase alias for the signing key certificate empty"},{"location":"config_knox_sso/#participating-application-configuration","title":"Participating Application Configuration","text":""},{"location":"config_knox_sso/#hadoop-configuration-example","title":"Hadoop Configuration Example","text":"<p>The following is used as the KnoxSSO configuration in the Hadoop JWTRedirectAuthenticationHandler implementation. Any participating application will need similar configuration. Since JWTRedirectAuthenticationHandler extends the AltKerberosAuthenticationHandler, the typical Kerberos configuration parameters for authentication are also required.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.type&lt;/name\n    &lt;value&gt;org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>This is the handler classname in Hadoop auth for JWT token (KnoxSSO) support.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.authentication.provider.url&lt;/name&gt;\n    &lt;value&gt;https://c6401.ambari.apache.org:8443/gateway/knoxsso/api/v1/websso&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The above property is the SSO provider URL that points to the knoxsso endpoint.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.authentication.public.key.pem&lt;/name&gt;\n    &lt;value&gt;MIICVjCCAb+gAwIBAgIJAPPvOtuTxFeiMA0GCSqGSIb3DQEBBQUAMG0xCzAJBgNV\n  BAYTAlVTMQ0wCwYDVQQIEwRUZXN0MQ0wCwYDVQQHEwRUZXN0MQ8wDQYDVQQKEwZI\n  YWRvb3AxDTALBgNVBAsTBFRlc3QxIDAeBgNVBAMTF2M2NDAxLmFtYmFyaS5hcGFj\n  aGUub3JnMB4XDTE1MDcxNjE4NDcyM1oXDTE2MDcxNTE4NDcyM1owbTELMAkGA1UE\n  BhMCVVMxDTALBgNVBAgTBFRlc3QxDTALBgNVBAcTBFRlc3QxDzANBgNVBAoTBkhh\n  ZG9vcDENMAsGA1UECxMEVGVzdDEgMB4GA1UEAxMXYzY0MDEuYW1iYXJpLmFwYWNo\n  ZS5vcmcwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAMFs/rymbiNvg8lDhsdA\n  qvh5uHP6iMtfv9IYpDleShjkS1C+IqId6bwGIEO8yhIS5BnfUR/fcnHi2ZNrXX7x\n  QUtQe7M9tDIKu48w//InnZ6VpAqjGShWxcSzR6UB/YoGe5ytHS6MrXaormfBg3VW\n  tDoy2MS83W8pweS6p5JnK7S5AgMBAAEwDQYJKoZIhvcNAQEFBQADgYEANyVg6EzE\n  2q84gq7wQfLt9t047nYFkxcRfzhNVL3LB8p6IkM4RUrzWq4kLA+z+bpY2OdpkTOe\n  wUpEdVKzOQd4V7vRxpdANxtbG/XXrJAAcY/S+eMy1eDK73cmaVPnxPUGWmMnQXUi\n  TLab+w8tBQhNbq6BOQ42aOrLxA8k/M4cV1A=&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The above property holds the KnoxSSO server's public key for signature verification. Adding it directly to the config like this is convenient and is easily done through Ambari to existing config files that take custom properties. Config is generally protected as root access only as well - so it is a pretty good solution.</p> <p>Individual UIs within the Hadoop ecosystem will have similar configuration for participating in the KnoxSSO websso capabilities.</p> <p>Blogs will be provided on the Apache Knox project site for these usecases as they become available.</p>"},{"location":"config_knox_sso/#knoxsso-cookie-invalidation","title":"KnoxSSO Cookie Invalidation","text":"<p>This feature was implemented in the scope of KNOX-2691.</p> <p>The user story is that there is a need for a new feature that would allow a pre-configured superuser to invalidate previously issued Knox SSO tokens for (a) particular user(s) in case there is a malicious attack in terms of one (or more) of those users' SSO tokens got compromised.</p> <p>To be able to achieve this goal, the <code>KNOXSSO</code> service is modified in a way such that it saves the generated SSO cookie using Knox's token state service capabilities in case token management is enabled in KNOXSSO's configuration (using the well-known <code>knox.token.exp.server-managed=true</code> parameter, by default this is set to <code>false</code> in the relevant topologies).</p> <p>This is only the SSO cookie generation side of the feature. The verification side also needs to be configured the same way: the <code>SSOCookieProvider</code> configuration must have the same parameter to enable this new feature.</p> <p>It is very important to highlight, that turning this feature on will make previously initiated <code>KNOX SSO sessions</code> invalid, therefore the browsers must be closed, and/or the cookies have to be removed. This will ensure new user logins which will be captured by the enabled token state service.</p> <p>There is another essential configuration when <code>KNOXSSO</code> is configured to use the Pac4J federation filter. In this case, the <code>knox.global.logout.page.url</code> configuration is a must-have parameter in <code>gateway-site.xml</code> which usually points to the logout endpoint of the pre-configured SAML/OIDC callback.</p> <p>Together with the new Token Management UI, pre-configured \"superusers\" can disable (invalidate) SSO cookies. This will result in forcing the users to log in again, which, for obvious reasons, the malicious user(s) cannot do.</p>"},{"location":"config_knox_sso/#concurrent-session-verification","title":"Concurrent Session Verification","text":""},{"location":"config_knox_sso/#overview","title":"Overview","text":"<p>This feature allows end-users limiting the number of concurrent UI sessions the users can have.  In order to reach this goal the users can be sorted out into three groups: non-privileged, privileged, unlimited.</p> <p>The non-privileged and privileged groups each have a configurable limit, which the members of the group can not exceed. The members of the unlimited group are able to create unlimited  number of concurrent sessions.</p> <p>All of the users, who are not configured in neither the privileged nor in the unlimited group, shall become automatically the member of the non-privileged group.</p>"},{"location":"config_knox_sso/#configuration","title":"Configuration","text":"<p>The following table shows the relevant gateway-level parameters that are essential for this feature to work. Please note these parameters are not listed above in the Gateway Server Configuration table.</p> Parameter Description Default gateway.service.concurrentsessionverifier.impl To enable the session verification feature, end-users should set this parameter to <code>org.apache.knox.gateway.session.control.InMemoryConcurrentSessionVerifier</code> <code>org.apache.knox.gateway.session.control.EmptyConcurrentSessionVerifier</code> gateway.session.verification.privileged.users Indicates a list of users that are qualified 'privileged'. Empty list gateway.session.verification.unlimited.users Indicates a list of (super) users that can have as many UI sessions as they want. Empty list gateway.session.verification.privileged.user.limit The number of UI sessions a 'privileged' user can have 3 gateway.session.verification.non.privileged.user.limit The number of UI sessions a 'non-privileged' user can have 2 gateway.session.verification.expired.tokens.cleaning.period The time period (seconds) about the expired session verification data is removed from the background storage 1800"},{"location":"config_knox_sso/#how-this-works","title":"How this works","text":"<p>If the verifier is disabled it will not do anything even if the other parameters are configured.</p> <p>When the verifier is enabled all of the users are considered as a non-privileged user by default and they will not be able to create more  concurrent sessions than the non-privileged limit. The same is true after you added someone in the privileged user group: that user will not be able to create more UI sessions than the configured privileged user limit. Whereas the members of the unlimited users group are able to create unlimited number of concurrent sessions even if they are configured in the privileged group as well.</p> <p>The underlying verifier stores verification data (included JWTs) provided by KnoxSSO and used for login. In order to save resources the expired tokens are deleted periodically. The cleaning period can also be updated thru the <code>gateway.session.verification.expired.tokens.cleaning.period</code> parameter.</p>"},{"location":"config_knox_token/","title":"Knox Token","text":""},{"location":"config_knox_token/#knoxtoken-configuration","title":"KnoxToken Configuration","text":""},{"location":"config_knox_token/#introduction","title":"Introduction","text":"<p>The Knox Token Service enables the ability for clients to acquire the same JWT token that is used for KnoxSSO with WebSSO flows for UIs to be used for accessing REST APIs. By acquiring the token and setting it as a Bearer token on a request, a client is able to access REST APIs that are protected with the JWTProvider federation provider.</p> <p>This section describes the overall setup requirements and options for KnoxToken service.</p>"},{"location":"config_knox_token/#knoxtoken-service","title":"KnoxToken service","text":"<p>The Knox Token Service configuration can be configured in any descriptor/topology, tailored to issue tokens to authenticated users, and constrain the usage of the tokens in a number of ways.</p> <pre><code>\"services\": [\n  {\n    \"name\": \"KNOXTOKEN\",\n    \"params\": {\n      \"knox.token.ttl\": \"36000000\",\n      \"knox.token.audiences\": \"tokenbased\",\n      \"knox.token.target.url\": \"https://localhost:8443/gateway/tokenbased\",\n      \"knox.token.exp.server-managed\": \"false\",\n      \"knox.token.renewer.whitelist\": \"admin\",\n      \"knox.token.exp.renew-interval\": \"86400000\",\n      \"knox.token.exp.max-lifetime\": \"604800000\",\n      \"knox.token.type\": \"JWT\"\n    }\n  }\n]\n</code></pre>"},{"location":"config_knox_token/#knoxtoken-configuration-parameters","title":"KnoxToken Configuration Parameters","text":"Parameter Description Default knox.token.ttl This indicates the lifespan (milliseconds) of the token. Once it expires a new token must be acquired from KnoxToken service. The 36000000 in the topology above gives you 10 hrs. 30000 (30 seconds) knox.token.audiences This is a comma-separated list of audiences to add to the JWT token. This is used to ensure that a token received by a participating application knows that the token was intended for use with that application. It is optional. In the event that an endpoint has expected audiences and they are not present the token must be rejected. In the event where the token has audiences and the endpoint has none expected then the token is accepted. empty knox.token.target.url This is an optional configuration parameter to indicate the intended endpoint for which the token may be used. The KnoxShell token credential collector can pull this URL from a knoxtokencache file to be used in scripts. This eliminates the need to prompt for or hardcode endpoints in your scripts. n/a knox.token.exp.server-managed This is an optional configuration parameter to enable/disable server-managed token state, to support the associated token renewal and revocation APIs. false knox.token.renewer.whitelist This is an optional configuration parameter to authorize the comma-separated list of users to invoke the associated token renewal and revocation APIs. knox.token.exp.renew-interval This is an optional configuration parameter to specify the amount of time (milliseconds) to be added to a token's TTL when a renewal request is approved. 86400000 (24 hours) knox.token.exp.max-lifetime This is an optional configuration parameter to specify the maximum allowed lifetime (milliseconds) of a token, after which renewal will not be permitted. 604800000 (7 days) knox.token.type If this is configured the generated JWT's header will have this value as the <code>typ</code> property knox.token.issuer This is an optional configuration parameter to specify the issuer of a token. KNOXSSO <p>Note that server-managed token state can be configured for all KnoxToken service deployments in gateway-site (see gateway.knox.token.exp.server-managed). If it is configured at the gateway level, then the associated service parameter, if configured, will override the gateway configuration.</p> <p>Adding the KnoxToken configuration shown above to a topology that is protected with the ShrioProvider is a very simple and effective way to expose an endpoint from which a Knox token can be requested. Once it is acquired it may be used to access resources at intended endpoints until it expires.</p> <p>The following curl command can be used to acquire a token from the Knox Token service as configured in the sandbox topology:</p> <pre><code>curl -ivku guest:guest-password https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token\n</code></pre> <p>Resulting in a JSON response that contains the token, the expiration and the optional target endpoint:</p> <pre><code>      `{\"access_token\":\"eyJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJndWVzdCIsImF1ZCI6InRva2VuYmFzZWQiLCJpc3MiOiJLTk9YU1NPIiwiZXhwIjoxNDg5OTQyMTg4fQ.bcqSK7zMnABEM_HVsm3oWNDrQ_ei7PcMI4AtZEERY9LaPo9dzugOg3PA5JH2BRF-lXM3tuEYuZPaZVf8PenzjtBbuQsCg9VVImuu2r1YNVJlcTQ7OV-eW50L6OTI0uZfyrFwX6C7jVhf7d7YR1NNxs4eVbXpS1TZ5fDIRSfU3MU\",\"target_url\":\"https://localhost:8443/gateway/tokenbased\",\"token_type\":\"Bearer \",\"expires_in\":1489942188233}`\n</code></pre> <p>The following curl example shows how to add a bearer token to an Authorization header:</p> <pre><code>curl -ivk -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJndWVzdCIsImF1ZCI6InRva2VuYmFzZWQiLCJpc3MiOiJLTk9YU1NPIiwiZXhwIjoxNDg5OTQyMTg4fQ.bcqSK7zMnABEM_HVsm3oWNDrQ_ei7PcMI4AtZEERY9LaPo9dzugOg3PA5JH2BRF-lXM3tuEYuZPaZVf8PenzjtBbuQsCg9VVImuu2r1YNVJlcTQ7OV-eW50L6OTI0uZfyrFwX6C7jVhf7d7YR1NNxs4eVbXpS1TZ5fDIRSfU3MU\" https://localhost:8443/gateway/tokenbased/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre> <p>If you want tokens to include group membership informations, add a <code>knox.token.include.groups</code> query parameter to the URL.</p> <pre><code>curl -u admin:admin-password -k \"https://localhost:8443/gateway/homepage/knoxtoken/api/v1/token?knox.token.include.groups=true\"\n</code></pre> <p>The response contains the token with the group information:</p> <pre><code>{\n      \"sub\": \"admin\",\n      \"jku\": \"https://localhost:8443/gateway/homepage/knoxtoken/api/v1/jwks.json\",\n      \"kid\": \"oigA7mZCwA2d7oimQyUaB0oDAfhI-1Bjq9y1n-Mw_OU\",\n      \"iss\": \"KNOXSSO\",\n      \"exp\": 1649777837,\n      \"knox.groups\": [\n        \"admin-group2\",\n        \"admin-group1\"\n      ],\n      \"managed.token\": \"true\",\n      \"knox.id\": \"dfeb8979-7f00-4938-bbff-1bc7574bb53d\"\n}\n</code></pre> <p>This feature is enabled by default. If you want to disable it, add the following configuration to the KNOXTOKEN service.</p> <pre><code>     &lt;param&gt;\n        &lt;name&gt;knox.token.include.groups.allowed&lt;/name&gt; &lt;!-- default = true --&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/param&gt;\n</code></pre>"},{"location":"config_knox_token/#knoxtoken-renewal-revocation-and-enabledisable-actions","title":"KnoxToken Renewal, Revocation and Enable/Disable actions","text":"<p>The KnoxToken service supports the renewal and explicit revocation of tokens it has issued. Support for both requires server-managed token state to be enabled with at least one renewer white-listed.</p>"},{"location":"config_knox_token/#renewal","title":"Renewal","text":"<pre><code>curl -ivku admin:admin-password -X PUT -d $TOKEN 'https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/renew'\n</code></pre> <p>The JSON responses include a flag indicating success or failure.</p> <p>A successful result includes the updated expiration time.</p> <pre><code>{\n  \"renewed\": \"true\",\n  \"expires\": \"1584278311658\"\n}\n</code></pre> <p>Error results include a message describing the reason for failure.</p> <p>Invalid token</p> <pre><code>{\n  \"renewed\": \"false\",\n  \"error\": \"Unknown token: 9caf743e-1e0d-4708-a9ac-a684a576067c\"\n}\n</code></pre> <p>Unauthorized caller</p> <pre><code>{\n  \"renewed\": \"false\",\n  \"error\": \"Caller (guest) not authorized to renew tokens.\"\n}\n</code></pre>"},{"location":"config_knox_token/#revocation","title":"Revocation","text":"<pre><code>curl -ivku admin:admin-password -X DELETE -d $TOKEN 'https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/revoke'\n</code></pre> <p>The JSON responses include a flag indicating success or failure.</p> <pre><code>{\n  \"revoked\": \"true\"\n}\n</code></pre> <p>Error results include a message describing the reason for the failure.</p> <p>Invalid token</p> <pre><code>{\n  \"revoked\": \"false\",\n  \"error\": \"Unknown token: 9caf743e-1e0d-4708-a9ac-a684a576067c\"\n}\n</code></pre> <p>Unauthorized caller</p> <pre><code>{\n  \"revoked\": \"false\",\n  \"error\": \"Caller (guest) not authorized to revoke tokens.\"\n}\n</code></pre> <p>KnoxSSO Cookies must not be revoked</p> <pre><code>$ curl -iku admin:admin-password  -H \"Content-Type: application/json\" -d 'c236d20c-4a05-4cfa-b35e-2ba6dc451de0' -X DELETE https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/revoke\nHTTP/1.1 403 Forbidden\nDate: Fri, 09 Oct 2023 08:55:25 GMT\nSet-Cookie: KNOXSESSIONID=node03e9y0cy8giy31rh00xc1mrcfx0.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Thu, 05-Oct-2023 08:55:25 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 113\n\n{\n  \"revoked\": \"false\",\n  \"error\": \"SSO cookie (c236d20c...2ba6dc451de0) cannot not be revoked.\",\n  \"code\": 20\n}\n</code></pre> <p>Revoke multiple tokens in one batch:</p> <pre><code>$ curl -iku admin:admin-password -H \"Content-Type: application/json\" -d '[\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"5735f5ae-bddd-4ed1-9383-47a839b9ae2b\"]' -X DELETE https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/revokeTokens\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:20:39 GMT\n...\n\n{\n  \"revoked\": \"true\"\n}\n</code></pre> <p>When revoking multiple tokens, current token state check is executed one by one. This means, if there was at least failed token revocation, the HTTP response will indicate that despite the fact that the rest of the token revocation actions succeeded.</p>"},{"location":"config_knox_token/#enable","title":"Enable","text":"<p>This endpoint added in the scope of KNOX-2602.</p> <pre><code>$ curl -ku admin:admin-password -d \"1e2f286e-9df1-4123-8d41-e6af523d6923\" -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/enable\n\n{\n  \"setEnabledFlag\": \"true\",\n  \"isEnabled\": \"true\"\n}\n</code></pre> <p>The JSON responses include a flag (<code>setEnabledFlag</code>) indicating success or failure along with the token state after the action is executed (<code>isEnabled</code>).</p> <p>Trying to enable an already enabled token:</p> <pre><code>$ curl -ku admin:admin-password -d \"1e2f286e-9df1-4123-8d41-e6af523d6923\" -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/enable\n{\n  \"setEnabledFlag\": \"false\",\n  \"error\": \"Token is already enabled\"\n}\n</code></pre> <p>Disabled KnoxSSO Cookies must not be (re-)enabled:</p> <pre><code>$ curl -iku admin:admin-password  -H \"Content-Type: application/json\" -d '107824ab-c54d-4db3-b3b5-5c964892ad05' -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/enable\nHTTP/1.1 400 Bad Request\nDate: Fri, 06 Oct 2023 08:57:58 GMT\nSet-Cookie: KNOXSESSIONID=node011ejmvgcjnlpl13mchqmqjtdjc1.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Thu, 05-Oct-2023 08:57:58 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 107\n\n{\n  \"setEnabledFlag\": \"false\",\n  \"error\": \"Disabled KnoxSSO Cookies cannot not be enabled\",\n  \"code\": 80\n}\n</code></pre> <p>Enable multiple tokens in one batch:</p> <pre><code>$ curl -iku admin:admin-password -H \"Content-Type: application/json\" -d '[\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"5735f5ae-bddd-4ed1-9383-47a839b9ae2b\"]' -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/enableTokens\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:19:23 GMT\n...\n\n{\n  \"setEnabledFlag\": \"true\",\n  \"isEnabled\": \"true\"\n}\n</code></pre> <p>When enabling multiple tokens, current token state check is not executed. This means, if you are enabling tokens that were already enabled before the batch operation, they remain enabled.</p>"},{"location":"config_knox_token/#disable","title":"Disable","text":"<p>This endpoint added in the scope of KNOX-2602.</p> <pre><code>$ curl -ku admin:admin-password -d \"1e2f286e-9df1-4123-8d41-e6af523d6923\" -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/disable\n{\n  \"setEnabledFlag\": \"true\",\n  \"isEnabled\": \"false\"\n}\n</code></pre> <p>The JSON responses include a flag (<code>setEnabledFlag</code>) indicating success or failure along with the token state after the action is executed (<code>isEnabled</code>).</p> <p>Trying to enable an already enabled token:</p> <pre><code>$ curl -ku admin:admin-password -d \"1e2f286e-9df1-4123-8d41-e6af523d6923\" -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/disable\n{\n  \"setEnabledFlag\": \"false\",\n  \"error\": \"Token is already disabled\"\n}\n</code></pre> <p>Disable multiple tokens in one batch:</p> <pre><code>$ curl -iku admin:admin-password -H \"Content-Type: application/json\" -d '[\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"5735f5ae-bddd-4ed1-9383-47a839b9ae2b\"]' -X PUT https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/disableTokens\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:16:14 GMT\n...\n\n{\n  \"setEnabledFlag\": \"true\",\n  \"isEnabled\": \"false\"\n}\n</code></pre> <p>When disabling multiple tokens, current token state check is not executed. This means, if you are disabling tokens that were already disabled before the batch operation, they remain disabled.</p>"},{"location":"config_knox_token/#fetching-tokens-for-users","title":"Fetching tokens for users","text":"<p>Fetching tokens by <code>userName</code>:</p> <pre><code>$ curl -iku admin:admin-password -X GET https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/getUserTokens?userName=admin\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:02:32 GMT\nSet-Cookie: KNOXSESSIONID=node01vfrmf5kpjt0ku6mt9765wwx64.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Mon, 09-Oct-2023 07:02:32 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 822\n\n{\"tokens\":[{\"tokenId\":\"5244358f-19a3-4834-b16f-aa7ddb2e7fe1\",\"issueTime\":\"2023-10-10T09:02:03.904+0200\",\"expiration\":\"2023-10-11T09:02:03.000+0200\",\"maxLifetime\":\"2023-10-17T09:02:03.904+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":true,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":null},\"issueTimeLong\":1696921323904,\"expirationLong\":1697007723000,\"maxLifetimeLong\":1697526123904},{\"tokenId\":\"9b37e838-4aa2-43fd-b2f1-b35660b33778\",\"issueTime\":\"2023-10-10T09:02:14.271+0200\",\"expiration\":\"2023-10-10T10:02:14.242+0200\",\"maxLifetime\":\"2023-10-17T09:02:14.271+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":\"admin token 1\"},\"issueTimeLong\":1696921334271,\"expirationLong\":1696924934242,\"maxLifetimeLong\":1697526134271}]}\n</code></pre> <p>Fetching tokens by <code>createdBy</code>:</p> <pre><code>$ curl -iku admin:admin-password -X GET https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/getUserTokens?createdBy=admin\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:05:45 GMT\nSet-Cookie: KNOXSESSIONID=node047nn0zjkauc41qzexnjmlhj2j6.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Mon, 09-Oct-2023 07:05:46 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 436\n\n{\"tokens\":[{\"tokenId\":\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"issueTime\":\"2023-10-10T09:02:29.146+0200\",\"expiration\":\"2023-10-10T10:02:29.127+0200\",\"maxLifetime\":\"2023-10-17T09:02:29.146+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":\"admin\",\"userName\":\"guest\",\"enabled\":true,\"comment\":\"admin token 1 for guest\"},\"issueTimeLong\":1696921349146,\"expirationLong\":1696924949127,\"maxLifetimeLong\":1697526149146}]}\n</code></pre> <p>Fetching tokens by <code>userNameOrCreatedBy</code>:</p> <pre><code>$ curl -iku admin:admin-password -X GET https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/getUserTokens?userNameOrCreatedBy=admin\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:07:02 GMT\nSet-Cookie: KNOXSESSIONID=node0rt50pq4getaj1s1owcj3pvgfm7.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Mon, 09-Oct-2023 07:07:02 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 1246\n\n{\"tokens\":[{\"tokenId\":\"5244358f-19a3-4834-b16f-aa7ddb2e7fe1\",\"issueTime\":\"2023-10-10T09:02:03.904+0200\",\"expiration\":\"2023-10-11T09:02:03.000+0200\",\"maxLifetime\":\"2023-10-17T09:02:03.904+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":true,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":null},\"issueTimeLong\":1696921323904,\"expirationLong\":1697007723000,\"maxLifetimeLong\":1697526123904},{\"tokenId\":\"9b37e838-4aa2-43fd-b2f1-b35660b33778\",\"issueTime\":\"2023-10-10T09:02:14.271+0200\",\"expiration\":\"2023-10-10T10:02:14.242+0200\",\"maxLifetime\":\"2023-10-17T09:02:14.271+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":\"admin token 1\"},\"issueTimeLong\":1696921334271,\"expirationLong\":1696924934242,\"maxLifetimeLong\":1697526134271},{\"tokenId\":\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"issueTime\":\"2023-10-10T09:02:29.146+0200\",\"expiration\":\"2023-10-10T10:02:29.127+0200\",\"maxLifetime\":\"2023-10-17T09:02:29.146+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":\"admin\",\"userName\":\"guest\",\"enabled\":true,\"comment\":\"admin token 1 for guest\"},\"issueTimeLong\":1696921349146,\"expirationLong\":1696924949127,\"maxLifetimeLong\":1697526149146}]}\n</code></pre> <p>Fetching <code>all</code> tokens:</p> <pre><code>$ curl -iku admin:admin-password -X GET https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/getUserTokens?allTokens=true\nHTTP/1.1 200 OK\nDate: Tue, 10 Oct 2023 07:08:08 GMT\nSet-Cookie: KNOXSESSIONID=node0fctcnhp9fm3w1gq1mc2z993109.node0; Path=/gateway/sandbox; Secure; HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/sandbox; Max-Age=0; Expires=Mon, 09-Oct-2023 07:08:08 GMT; SameSite=lax\nContent-Type: application/json\nContent-Length: 2048\n\n{\"tokens\":[{\"tokenId\":\"5244358f-19a3-4834-b16f-aa7ddb2e7fe1\",\"issueTime\":\"2023-10-10T09:02:03.904+0200\",\"expiration\":\"2023-10-11T09:02:03.000+0200\",\"maxLifetime\":\"2023-10-17T09:02:03.904+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":true,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":null},\"issueTimeLong\":1696921323904,\"expirationLong\":1697007723000,\"maxLifetimeLong\":1697526123904},{\"tokenId\":\"9b37e838-4aa2-43fd-b2f1-b35660b33778\",\"issueTime\":\"2023-10-10T09:02:14.271+0200\",\"expiration\":\"2023-10-10T10:02:14.242+0200\",\"maxLifetime\":\"2023-10-17T09:02:14.271+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":null,\"userName\":\"admin\",\"enabled\":true,\"comment\":\"admin token 1\"},\"issueTimeLong\":1696921334271,\"expirationLong\":1696924934242,\"maxLifetimeLong\":1697526134271},{\"tokenId\":\"3c043de7-f9e9-4c1a-b32f-abfbc3dcbcb2\",\"issueTime\":\"2023-10-10T09:02:29.146+0200\",\"expiration\":\"2023-10-10T10:02:29.127+0200\",\"maxLifetime\":\"2023-10-17T09:02:29.146+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":\"admin\",\"userName\":\"guest\",\"enabled\":true,\"comment\":\"admin token 1 for guest\"},\"issueTimeLong\":1696921349146,\"expirationLong\":1696924949127,\"maxLifetimeLong\":1697526149146},{\"tokenId\":\"75f1b921-680d-433d-976f-270a100a1cf9\",\"issueTime\":\"2023-10-10T09:07:50.871+0200\",\"expiration\":\"2023-10-11T09:07:50.000+0200\",\"maxLifetime\":\"2023-10-17T09:07:50.871+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":true,\"createdBy\":null,\"userName\":\"sam\",\"enabled\":true,\"comment\":null},\"issueTimeLong\":1696921670871,\"expirationLong\":1697008070000,\"maxLifetimeLong\":1697526470871},{\"tokenId\":\"5735f5ae-bddd-4ed1-9383-47a839b9ae2b\",\"issueTime\":\"2023-10-10T09:07:55.293+0200\",\"expiration\":\"2023-10-10T10:07:55.276+0200\",\"maxLifetime\":\"2023-10-17T09:07:55.293+0200\",\"metadata\":{\"customMetadataMap\":{},\"knoxSsoCookie\":false,\"createdBy\":null,\"userName\":\"sam\",\"enabled\":true,\"comment\":\"sam token\"},\"issueTimeLong\":1696921675293,\"expirationLong\":1696925275276,\"maxLifetimeLong\":1697526475293}]}\n</code></pre>"},{"location":"config_knox_token/#token-generationmanagement-uis","title":"Token Generation/Management UIs","text":""},{"location":"config_knox_token/#overview","title":"Overview","text":"<p>In Apache Knox v2.0.0 the team added two new UIs that are directly accessible from the Knox Home page:</p> <ul> <li>Token Generation</li> <li>Token Management</li> </ul> <p>By default, the <code>homepage</code> topology comes with the <code>KNOXTOKEN</code> service enabled with the following attributes:</p> <ul> <li>token TTL is set to 120 days</li> <li>token service is enabled (default to keystore-based token state service)</li> <li>the admin user is allowed to renew/revoke tokens</li> </ul> <p>In this topology, homepage, two new applications were added in order to display the above-listed UIs:</p> <ul> <li><code>tokengen</code>: this is an old-style JSP UI, with a relatively simple JS code included. The source is located in the gateway-applications Maven sub-module.</li> <li><code>token-management</code>: this is an Angular UI. The source is located in its own knox-token-management-ui Maven sub-module.</li> </ul> <p>On the Knox Home page, you will see a new town in the General Proxy Information table like this:</p> <p></p> <p>However, the Integration Token links are disabled by default, because token integration requires a gateway-level alias - called <code>knox.token.hash.key</code> - being created and without that alias, it does not make sense to show those links.</p>"},{"location":"config_knox_token/#creating-the-token-hash-key","title":"Creating the token hash key","text":"<p>As explained, if you would like to use Knox's token generation features, you will have to create a gateway-level alias with a 256, 384, or 512-bit length JWK. You can do it in - at least - two different ways:</p> <ol> <li>You generate your own MAC (using this online tool for instance) and save it as an alias using Knox CLI.</li> <li>You do it running the following Knox CLI command: <code>generate-jwk --saveAlias knox.token.hash.key</code></li> </ol> <p>The second option involves a newly created Knox CLI command called <code>generate-jwk</code>:</p>"},{"location":"config_knox_token/#token-state-service-implementations","title":"Token state service implementations","text":"<p>There was an important step the Knox team made to provide more flexibility for our end-users: there are some internal service implementations in Knox that were hard-coded in the Java source code. One of those services is the <code>Token State</code> service implementation which you can change in gateway-site.xml going forward by setting the <code>gateway.service.tokenstate.impl</code> property to any of:</p> <ol> <li><code>org.apache.knox.gateway.services.token.impl.DefaultTokenStateService</code> - keeps all token information in memory, therefore all of this information is lost when Knox is shut down</li> <li><code>org.apache.knox.gateway.services.token.impl.AliasBasedTokenStateService</code> - token information is stored in the gateway credential store. This is a durable option, but not suitable for HA deployments</li> <li><code>org.apache.knox.gateway.services.token.impl.JournalBasedTokenStateService</code> - token information is stored in plain files within <code>$KNOX_DATA_DIR/security/token-state</code> folder. This option also provides a durable persistence layer for tokens and it might be good for HA scenarios too (in case of KNOX_DATA_DIR is on a shared drive), but the token data is written out in plain text (i.e. not encrypted) so it's less secure.</li> <li><code>org.apache.knox.gateway.services.token.impl.ZookeeperTokenStateService</code> - this is an extension of the keystore-based approach. In this case, token information is stored in Zookeeper using Knox aliases. The token's alias name equals to its generated token ID.</li> <li><code>org.apache.knox.gateway.services.token.impl.JDBCTokenStateService</code> - stores token information in relational databases. It's not only durable, but it's perfectly fine with HA deployments. Currently, PostgreSQL and MySQL databases are supported.</li> </ol> <p>By default, the <code>AliasBasedTokenStateService</code> implementation is used.</p>"},{"location":"config_knox_token/#configuring-the-jdbc-token-state-service","title":"Configuring the JDBC token state service","text":"<p>If you want to use the newly implemented database token management, you\u2019ve to set <code>gateway.service.tokenstate.impl</code> in gateway-site.xml to <code>org.apache.knox.gateway.services.token.impl.JDBCTokenStateService</code>.</p> <p>Now, that you have configured your token state backend, you need to configure a valid database in gateway-site.xml. There are two ways to do that:</p> <ol> <li>You either declare database connection properties one-by-one: <code>gateway.database.type</code> - should be set to <code>postgresql</code> or <code>mysql</code> <code>gateway.database.host</code> - the host where your DB server is running <code>gateway.database.port</code> - the port that your DB server is listening on <code>gateway.database.name</code> - the name of the database you are connecting to</li> <li>Or you declare an all-in-one JDBC connection string called <code>gateway.database.connection.url</code>. The following value will show you how to connect to an SSL enabled PostgreSQL server: <code>jdbc:postgresql://$myPostgresServerHost:5432/postgres?user=postgres&amp;ssl=true&amp;sslmode=verify-full&amp;sslrootcert=/usr/local/var/postgresql@10/data/root.crt</code></li> </ol> <p>If your database requires user/password authentication, the following aliases must be saved into the Knox Gateway\u2019s credential store (__gateway-credentials.jceks):</p> <ul> <li><code>gateway_database_user</code> - the username</li> <li><code>gateway_database_password</code> - the password</li> </ul>"},{"location":"config_knox_token/#database-design","title":"Database design","text":"<p>As you can see, there are only 2 tables:</p> <ul> <li><code>KNOXTOKENS</code> contains basic information about the generated token</li> <li><code>KNOX_TOKEN_METADATA</code> contains an arbitrary number of metadata information for the generated token. At the time of this document being written the following metadata exist:<ul> <li><code>passcode</code> - this is the BASE-64 encoded value of the generated    passcode token MAC. That is, the BASE-64 decoded value is a generated    MAC.</li> <li><code>userName</code> - the logged-in user who generated the token</li> <li><code>enabled</code> - this is a boolean flag indicating that the given token is enabled or not (a disabled token cannot be used for    authentication purposes)</li> <li><code>comment</code> - this is optional metadata, saved only if the user enters something in the Comment input field on the Token    Generation page (see below)</li> <li><code>createdBy</code> - if the token is an impersonated token, this metadata holds the name if the user who generated the token (see Token impersonation below)</li> </ul> </li> </ul>"},{"location":"config_knox_token/#generating-a-token","title":"Generating a token","text":"<p>Once you configured the <code>knox.token.hash.key</code> alias and optionally customized your token state service, you are all set to generate Knox tokens using the new Token Generation UI:</p> <p></p> <p>The following sections are displayed on the page:</p> <ul> <li>status bar: here you can see an informative message on the configured Token State backend. There are 3 different statuses:<ul> <li>ERROR: shown in red. This indicates a problem with the service backend which makes the feature not work. Usually, this is visible when end-users configure JDBC token state service, but they make a mistake in their DB settings</li> <li>WARN: displayed in yellow (see above picture). This indicates that the feature is enabled and working, but there are some limitations</li> <li>INFO: displayed in green. This indicates when the token management backend is properly configured for HA and production deployments</li> </ul> </li> <li>there is an information label explaining the purpose of the token generation page</li> <li>comment: this is an optional input field that allows end-users to add meaningful comments (mnemonics) to their generated tokens. The maximum length is 255 characters.</li> <li>the <code>Configured maximum lifetime</code> informs the clients about the <code>knox.token.ttl</code> property set in the <code>homepage</code> topology (defaults to 120 days). If that property is not set (e.g. someone removes it from he homepage topology), Knox uses a hard-coded value of 30 seconds (aka. default Knox token TTL)</li> <li>Custom token lifetime can be set by adjusting the days/hours/minutes spinners. The default configuration will yield one hour.</li> <li>Token impersonation: an optional free text input field that makes it possible to generate a token for someone else.</li> <li>Clicking the Generate Token button will try to create a token for you.</li> </ul>"},{"location":"config_knox_token/#about-the-generated-token-ttl","title":"About the generated token TTL","text":"<p>Out of the box, Knox will display the custom lifetime spinners on the Token Generation page. However, they can be hidden by setting the <code>knox.token.lifespan.input.enabled</code> property to <code>false</code> in the <code>homepage</code> topology. Given that possibility and the configured maximum lifetime the generated token can have the following TTL value:</p> <ul> <li>there is no configured token TTL and lifespan inputs are disabled -&gt; the default TTL is used (30 seconds)</li> <li>there is configured TTL and lifespan inputs are disabled -&gt; the configured TTL is used</li> <li>there is configured TTL and lifespan inputs are enabled and lifespan inputs result in a value that is less than or equal to the configured TTL -&gt; the lifespan query param is used</li> <li>there is configured TTL and lifespan inputs are enabled and lifespan inputs result in a value that is greater than the configured TTL -&gt; the configured TTL is used</li> </ul>"},{"location":"config_knox_token/#successful-token-generation","title":"Successful token generation","text":"<p>On the resulting page there is two sensitive information that you can use in Knox to authenticate your request:</p> <ol> <li> <p>JWT token - this is the serialized JWT and is fully compatible with the old-style Bearer authorization method. Clicking the <code>JWT Token</code> label on the page will copy the value into the clipboard. You might want to use it as the \u2018Token\u2019 user:</p> <p><code>$ curl -ku Token:eyJqa3UiOiJodHRwczpcL1wvbG9jYWxob3N0Ojg0NDNcL2dhdGV3YXlcL2hvbWVwYWdlXC9rbm94dG9rZW5cL2FwaVwvdjFcL2p3a3MuanNvbiIsImtpZCI6IkdsOTZfYTM2MTJCZWFsS2tURFRaOTZfVkVsLVhNRVRFRmZuNTRMQ1A2UDQiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImprdSI6Imh0dHBzOlwvXC9sb2NhbGhvc3Q6ODQ0M1wvZ2F0ZXdheVwvaG9tZXBhZ2VcL2tub3h0b2tlblwvYXBpXC92MVwvandrcy5qc29uIiwia2lkIjoiR2w5Nl9hMzYxMkJlYWxLa1REVFo5Nl9WRWwtWE1FVEVGZm41NExDUDZQNCIsImlzcyI6IktOT1hTU08iLCJleHAiOjE2MzY2MjU3MTAsIm1hbmFnZWQudG9rZW4iOiJ0cnVlIiwia25veC5pZCI6ImQxNjFjYWMxLWY5M2UtNDIyOS1hMGRkLTNhNzdhYjkxNDg3MSJ9.e_BNPf_G1iBrU0m3hul5VmmSbpw0w1pUAXl3czOcuxFOQ0Tki-Gq76fCBFUNdKt4QwLpNXxM321cH1TeMG4IhL-92QORSIZgRxY4OUtUgERzcU7-27VNYOzJbaRCjrx-Vb4bSriRJJDwbbXyAoEw_bjiP8EzFFJTPmGcctEzrOLWFk57cLO-2QLd2nbrNd4qmrRR6sEfP81Jg8UL-Ptp66vH_xalJJWuoyoNgGRmH8IMdLVwBgeLeVHiI7NmokuhO-vbctoEwV3Rt4pMpA0VSWGFN0MI4WtU0crjXXHg8U9xSZyOeyT3fMZBXctvBomhGlWaAvuT5AxQGyMMP3VLGw https:/localhost:8443/gateway/sandbox/webhdfs/v1?op=LISTSTATUS</code> <code>{\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":0,\"blockSize\":0,\"childrenNum\":1,\"fileId\":16386,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1621238405734,\"owner\":\"hdfs\",\"pathSuffix\":\"tmp\",\"permission\":\"1777\",\"replication\":0,\"storagePolicy\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"childrenNum\":1,\"fileId\":16387,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1621238326078,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"755\",\"replication\":0,\"storagePolicy\":0,\"type\":\"DIRECTORY\"}]}}</code></p> </li> <li> <p>Passcode token - this is the serialized passcode token, which you can use as the \u2018Passcode\u2019 user (Clicking the <code>Passcode Token</code> label on the page will copy the value into the clipboard):</p> <p><code>$ curl -ku Passcode:WkRFMk1XTmhZekV0WmprelpTMDBNakk1TFdFd1pHUXRNMkUzTjJGaU9URTBPRGN4OjpPVEV5Tm1KbFltUXROVEUyWkMwME9HSTBMVGd4TTJZdE1HRmxaalJrWlRVNFpXRTA= https://localhost:8443/gateway/sandbox/webhdfs/v1?op=LISTSTATUS</code> <code>{\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":0,\"blockSize\":0,\"childrenNum\":1,\"fileId\":16386,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1621238405734,\"owner\":\"hdfs\",\"pathSuffix\":\"tmp\",\"permission\":\"1777\",\"replication\":0,\"storagePolicy\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"childrenNum\":1,\"fileId\":16387,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1621238326078,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"755\",\"replication\":0,\"storagePolicy\":0,\"type\":\"DIRECTORY\"}]}}</code></p> </li> </ol> <p>The reason, we needed to support the shorter <code>Passcode token</code>, is that there are 3rd party tools where the long JWT exceeds input fields limitations so we need to address this issue with shorter token values.</p> <p>The rest of the fields are complementary information such as the expiration date/time of the generated token or the user who created it.</p>"},{"location":"config_knox_token/#token-generation-failed","title":"Token generation failed","text":"<p>If there was an error during token generation, you will see a failure right under the input field boxes (above the Generate Token button):</p> <p></p> <p>The above error message indicates a failure that the admin user already generated more tokens than they are allowed to. This limitation is configurable in the <code>gateway-site.xml</code>:</p> <ul> <li><code>gateway.knox.token.limit.per.user</code> - indicates the maximum number of tokens a user can manage at the same time. <code>-1</code> means that users are allowed to create/manage as many tokens as they want. This configuration only applies when the server-managed token state is enabled either in <code>gateway-site</code> or at the <code>topology</code> level. Defaults to 10.</li> </ul> <p>This behavior can be changed by setting the following <code>KNOXTOKEN</code> service-level parameter called <code>knox.token.user.limit.exceeded.action</code>. This property may have the following values:</p> <ul> <li><code>REMOVE_OLDEST</code> - if that\u2019s configured, the oldest token of the user, who the token is being generated for, will be removed</li> <li><code>RETURN_ERROR</code> - if that\u2019s configured, Knox will return an error response with 403 error code (as it did in previous versions)</li> </ul> <p>The default value is <code>RETURN_ERROR</code>.</p>"},{"location":"config_knox_token/#token-management","title":"Token Management","text":"<p>In addition to the token generation UI, Knox comes with the Token Management UI where logged-in users can see all the active tokens that were generated before. That is, if a token got expired and was removed from the underlying token store, it won't be displayed here. Based on a configuration you can find below, users can see only their tokens or all of them.</p> <p></p> <p>On this page, you will a table with the following information:</p> <ol> <li>Each row starts with a selection checkbox for batch operations (except for disabled KnoxSSO cookies, as there is no point in doing anything with them)</li> <li>A unique token identifer. Disabled token's Token ID value is shown in orange</li> <li>Information on when the token was created and when it will expire<ol> <li>if the token is already expired, the expiration time is shown in red</li> <li>if the token is still valid, the expiration time is shown in green</li> </ol> </li> <li>Username indicates the user for whom the token is created for</li> <li>Impersonated is a boolean flag indicating if this is an impersonated token:<ol> <li>green check: yes, this is impersonated. You'll see the user who created the token under the icon</li> <li>red cross: no, this is not an impersonated token</li> </ol> </li> <li>KnoxSSO is another boolean flag that indicates if this token is created by the <code>KNOXSSO</code> service if the feature was enabled<ol> <li>green check: yes, this is KnoxSSO cookie (token)</li> <li>red cross: no, this is not a KnoxSSO cookie (it was created by a regular token API call or on the Token Generation page)</li> </ol> </li> <li>In the Actions column you will see<ol> <li>the enable/disable/revoke actions are visible for impersonated tokens too</li> <li>KnoxSSO cookies cannot be revoked nor re-enabled</li> </ol> </li> </ol> <p>In order to refresh the table, you can use the <code>Refresh icon</code> above the table (if you generated tokens on another tab for instance).</p> <p>Batch operations</p> <p>When at least one token is selected, the following buttons are shown under the table:</p> <ul> <li><code>Disable Selected Tokens</code>: when executed, all the selected tokens become disabled (if they were disabled originally, they will remain disabled)</li> <li><code>Enable Selected Tokens</code>: when executed, all the selected tokens become enabled (if they were enabled originally, they will remain enabled)</li> <li><code>Revoke Selected Tokens</code>: when executed, all the selected tokens will be revoked. Please note this option is shown only, if there is no KnoxSSO cookie (token) selected (i.e. batch revocation only works with regular tokens).</li> </ul> <p>Toggles</p> <ul> <li><code>Show Disabled KnoxSSO Cookies</code>: this is true by default. Since disabled KnoxSSO cookies remain in the underlying token state service until they expire, it may bother users to see them in the tokens table. Flipping this toggle button helps to hide them.</li> <li><code>Show My Tokens Only</code>: this toggle button is only visible to users, who can see all tokens. By default, this is false. Enabling it will filter the tokens table in a way such that it will contain tokens only that were generated for the logged in user (impersonated or not).</li> </ul> <p>Configuration</p> <p>By default, logged in users can see token that were generated by them or for them (in caase of token impersonation). However, you may want to edit the <code>gateway.knox.token.management.users.can.see.all.tokens</code> parameter in <code>gateway-site.xml</code> to allow other users than <code>admin</code> to become such a \"superuser\", who can see all tokens on the Token Management UI.</p> <pre><code> &lt;property&gt;\n    &lt;name&gt;gateway.knox.token.management.users.can.see.all.tokens&lt;/name&gt;\n    &lt;value&gt;admin&lt;/value&gt;\n    &lt;description&gt;A comma-separated list of user names who can see all tokens on the Token Management page&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"config_knox_token/#token-impersonation","title":"Token impersonation","text":"<p>On the token generation page end-users can generate tokens on behalf of other users by specifying the desired user name in the token <code>impersonation</code> field. The following screenshot sows a successful token generation for user <code>tom</code> (the logged in user is <code>admin</code>).</p> <p></p> <p>For this to work, the topology has to be configured with the HadoopAuth authentication provider, or an identity assertion provider where impersonation is enabled In both cases, <code>doAs</code> support will only work with a valid Hadoop proxyuser configuration (see Hadoop Proxyuser impersonation above)</p>"},{"location":"config_knox_token/#token-metadata","title":"Token metadata","text":"<p>As indicated above, the <code>KNOXTOKEN</code> service maintains some hard-coded token metadata out-of-the-box:</p> <ul> <li>userName</li> <li>comment</li> <li>enabled</li> <li>passcode</li> <li>createdBy (in case of impersonated tokens)</li> </ul> <p>In v2.0.0, the Knox team implemented a change in this service that allows end-users to add accept query parameters starting with the <code>md_</code> prefix and treat them as Knox Token Metadata.</p> <p>For instance:</p> <p><code>curl -iku admin:admin-password -X GET 'https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token?md_notebookName=accountantKnoxToken&amp;md_souldBeRemovedBy=31March2022&amp;md_otherMeaningfuMetadata=KnoxIsCool'</code> When such a token is created by Knox, we should save the following metadata too:</p> <ul> <li>notebookName=accountantKnoxToken</li> <li>shouldBeRemovedBy=31March2022</li> <li>otherMeaningfulMetadata=KnoxIsCool</li> </ul> <p>It's not only Knox can save these metadata, but the Knox's existing <code>getUserTokens API</code> endpoint is able to fetch basic token information using the supplied metadata name besides the user name information.</p> <p>It's important to note the following: the <code>getUserTokens</code> API returns tokens if any of the supplied metadata exists for the given token. Metadata values may or may not be matched: you can either use the <code>*</code> wildcard to match all metadata values with a given name or you can further filter the stored metadata information by specifying the desired value.</p> <p>For instance:</p> <p><code>curl -iku admin:admin-password -X GET 'https://localhost:8443/gateway/sandbox/knoxtoken/api/v1/token/getUserTokens?userName=admin&amp;md_notebookName=accountantKnoxToken&amp;md_name=*'</code></p> <p>will return all Knox tokens where metadata with <code>notebookName</code> exists and equals <code>accountantKnoxToken</code> OR metadata with <code>name</code> exists.</p> <p>Another sample:</p> <ol> <li>Create token1 with <code>md_Name=reina&amp;md_Score=50</code></li> <li>Create token2 with <code>md_Name=mary&amp;md_Score=100</code></li> <li>Create token3 with <code>md_Name=mary&amp;md_Score=20&amp;md_Grade=A</code></li> </ol> <p>The following table shows the returned token(s) in case metadata filtering is added in the <code>getUserTokens</code> API:</p> Metadata Token returned md_Name=reina token1 md_Name=mary token2 and token3 md_Score=100 token2 md_Name=mary&amp;md_Score=20 token2 and token3 md_Name=mary&amp;md_Name=reina token1, token2 and token3 md_Name=* token1, token2 and token3 md_Uknown=* Empty list <p>You may want to check out GitHub Pull Request #542 for sample <code>curl</code> commands.</p>"},{"location":"config_knoxauth_service/","title":"Knox Auth Service","text":""},{"location":"config_knoxauth_service/#knox-auth-service","title":"Knox Auth Service","text":""},{"location":"config_knoxauth_service/#introduction","title":"Introduction","text":"<p>With workloads moving to containers, it was necessary that Knox supports new ways of authentication needs of containers. As part of this effort, the Knox team developed a new internal service, called <code>KNOX-AUTH-SERVICE</code>. This service gathers a collection of public REST API endpoints that allows other developers to integrate Knox in their microservice/DEVOPS architectures using containers (such as docker or k9s).</p>"},{"location":"config_knoxauth_service/#configuration","title":"Configuration","text":"<p>This service can be added to any Knox topology as an internal service as follows:</p> <pre><code>&lt;service&gt;\n     &lt;role&gt;KNOX-AUTH-SERVICE&lt;/role&gt;\n     &lt;param&gt;\n       &lt;name&gt;preauth.auth.header.actor.id.name&lt;/name&gt;\n       &lt;value&gt;X-Knox-Actor-ID&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;preauth.auth.header.actor.groups.prefix&lt;/name&gt;\n       &lt;value&gt;X-Knox-Actor-Groups&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;preauth.group.filter.pattern&lt;/name&gt;\n       &lt;value&gt;.*&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n       &lt;name&gt;auth.bearer.token.env&lt;/name&gt;\n       &lt;value&gt;BEARER_AUTH_TOKEN&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"config_knoxauth_service/#available-rest-api-endpoints","title":"Available REST API endpoints","text":""},{"location":"config_knoxauth_service/#authapiv1pre","title":"auth/api/v1/pre","text":"<p>This REST API endpoint has a very simple job: if a valid principal is found in the incoming request, a header is added to the response (by default <code>X-Knox-Actor-ID</code>) with the principal name. In addition, if the authenticated subject has group(s), it (they) will be added as comma-separated entries in the header(s) of the default form of <code>X-Knox-Actor-Groups-#num</code>. Each group header has a character limit of 1000 to keep them reasonably sized. The header names can be customized via the <code>preauth.auth.header.actor.id.name</code> and <code>preauth.auth.header.actor.groups.prefix</code> service parameters.</p> <p>End users may filter user groups by setting the <code>preauth.group.filter.pattern</code> service parameter to a valid regular expression. By default, all the user gropus are added into the <code>X-Knox-Actor-Groups-#num</code> header.</p> <p>Sample <code>curl</code> commands are available in this GitHub Pull Request.</p>"},{"location":"config_knoxauth_service/#authapiv1bearer","title":"auth/api/v1/bearer","text":"<p>This REST API enpoint populates the HTTP \"Authorization\" header with the <code>Bearer Token</code> in the HTTP response object obtained from an environment variable.  The current implementation assumes that the token is not rotated as it never gets exposed to the end-user. By default, the <code>BEARER_AUTH_TOKEN</code> environment variable is expected to hold the Bearer token. This can be customized by configuring the <code>auth.bearer.token.env</code> service parameter to the desired value.</p> <p>Sample <code>curl</code> commands are available in this GitHub Pull Request.</p>"},{"location":"config_ldap_authc_cache/","title":"LDAP Authentication Cache","text":""},{"location":"config_ldap_authc_cache/#ldap-authentication-caching","title":"LDAP Authentication Caching","text":"<p>Knox can be configured to cache LDAP authentication information. Knox leverages Shiro's built in caching mechanisms and has been tested with Shiro's EhCache cache manager implementation.</p> <p>The following provider snippet demonstrates how to configure turning on the cache using the ShiroProvider. In addition to using <code>org.apache.knox.gateway.shirorealm.KnoxLdapRealm</code> in the Shiro configuration, and setting up the cache you must set the flag for enabling caching authentication to true. Please see the property, <code>main.ldapRealm.authenticationCachingEnabled</code> below. If caching is enabled on more than one topology, advanced caching configs with differing persistence directories have to be provided. The reason for this is separate topologies manage their own caches in different directories. Two cache has to be in different places otherwise ehcache won't be able to lock the directory and the topology deployment will fail.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapGroupContextFactory&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n        &lt;value&gt;$ldapGroupContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n        &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.authorizationEnabled&lt;/name&gt;\n        &lt;!-- defaults to: false --&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.searchBase&lt;/name&gt;\n        &lt;value&gt;ou=groups,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.cacheManager&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxCacheManager&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.securityManager.cacheManager&lt;/name&gt;\n        &lt;value&gt;$cacheManager&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.authenticationCachingEnabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttributeValueTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemUsername&lt;/name&gt;\n        &lt;value&gt;uid=guest,ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemPassword&lt;/name&gt;\n        &lt;value&gt;guest-password&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;urls./**&lt;/name&gt;\n        &lt;value&gt;authcBasic&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_ldap_authc_cache/#trying-out-caching","title":"Trying out caching","text":"<p>Knox bundles a template topology files that can be used to try out the caching functionality. The template file located under <code>{GATEWAY_HOME}/templates</code> is <code>sandbox.knoxrealm.ehcache.xml</code>.</p> <p>To try this out</p> <pre><code>cd {GATEWAY_HOME}\ncp templates/sandbox.knoxrealm.ehcache.xml conf/topologies/sandbox.xml\nbin/ldap.sh start\nbin/gateway.sh start\n</code></pre> <p>The following call to WebHDFS should report: <code>{\"Path\":\"/user/tom\"}</code></p> <pre><code>curl  -i -v  -k -u tom:tom-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> <p>In order to see the cache working, LDAP can now be shutdown and the user will still authenticate successfully.</p> <pre><code>bin/ldap.sh stop\n</code></pre> <p>and then the following should still return successfully like it did earlier.</p> <pre><code>curl  -i -v  -k -u tom:tom-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre>"},{"location":"config_ldap_authc_cache/#advanced-caching-config","title":"Advanced Caching Config","text":"<p>By default the EhCache support in Shiro contains the ehcache.xml in its classpath which is the following</p> <pre><code>&lt;config xmlns=\"http://www.ehcache.org/v3\"&gt;\n\n    &lt;persistence directory=\"${java.io.tmpdir}/shiro-ehcache\"/&gt;\n\n    &lt;cache alias=\"shiro-activeSessionCache\"&gt;\n        &lt;key-type serializer=\"org.ehcache.impl.serialization.CompactJavaSerializer\"&gt;\n            java.lang.Object\n        &lt;/key-type&gt;\n        &lt;value-type serializer=\"org.ehcache.impl.serialization.CompactJavaSerializer\"&gt;\n            java.lang.Object\n        &lt;/value-type&gt;\n\n        &lt;resources&gt;\n            &lt;heap unit=\"entries\"&gt;10000&lt;/heap&gt;\n            &lt;disk unit=\"GB\"&gt;1&lt;/disk&gt;\n        &lt;/resources&gt;\n    &lt;/cache&gt;\n\n    &lt;cache alias=\"org.apache.shiro.realm.text.PropertiesRealm-0-accounts\"&gt;\n        &lt;key-type serializer=\"org.ehcache.impl.serialization.CompactJavaSerializer\"&gt;\n            java.lang.Object\n        &lt;/key-type&gt;\n        &lt;value-type serializer=\"org.ehcache.impl.serialization.CompactJavaSerializer\"&gt;\n            java.lang.Object\n        &lt;/value-type&gt;\n\n        &lt;resources&gt;\n            &lt;heap unit=\"entries\"&gt;1000&lt;/heap&gt;\n            &lt;disk unit=\"GB\"&gt;1&lt;/disk&gt;\n        &lt;/resources&gt;\n    &lt;/cache&gt;\n\n    &lt;cache-template name=\"defaultCacheConfiguration\"&gt;\n        &lt;expiry&gt;\n            &lt;tti unit=\"seconds\"&gt;120&lt;/tti&gt;\n        &lt;/expiry&gt;\n        &lt;heap unit=\"entries\"&gt;10000&lt;/heap&gt;\n    &lt;/cache-template&gt;\n\n&lt;/config&gt;\n</code></pre> <p>A custom configuration file (ehcache.xml) can be used in place of this in order to set specific caching configuration.</p> <p>In order to set the ehcache.xml file to use for a particular topology, set the following parameter in the configuration for the ShiroProvider:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;main.cacheManager.cacheManagerConfigFile&lt;/name&gt;\n    &lt;value&gt;classpath:ehcache.xml&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>In the above example, place the ehcache.xml file under <code>{GATEWAY_HOME}/conf</code> and restart the gateway server.</p>"},{"location":"config_ldap_group_lookup/","title":"LDAP Group Lookup","text":""},{"location":"config_ldap_group_lookup/#ldap-group-lookup","title":"LDAP Group Lookup","text":"<p>Knox can be configured to look up LDAP groups that the authenticated user belong to. Knox can look up both Static LDAP Groups and Dynamic LDAP Groups. The looked up groups are populated as Principal(s) in the Java Subject of the authenticated user. Therefore service authorization rules can be defined in terms of LDAP groups looked up from a LDAP directory.</p> <p>To look up LDAP groups of authenticated user from LDAP, you have to use <code>org.apache.knox.gateway.shirorealm.KnoxLdapRealm</code> in Shiro configuration.</p> <p>Please see below a sample Shiro configuration snippet from a topology file that was tested looking LDAP groups.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ShiroProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;!-- \n    session timeout in minutes,  this is really idle timeout,\n    defaults to 30mins, if the property value is not defined,, \n    current client authentication would expire if client idles continuously for more than this value\n    --&gt;\n    &lt;!-- defaults to: 30 minutes\n    &lt;param&gt;\n        &lt;name&gt;sessionTimeout&lt;/name&gt;\n        &lt;value&gt;30&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n\n    &lt;!--\n      Use single KnoxLdapRealm to do authentication and ldap group look up\n    --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapGroupContextFactory&lt;/name&gt;\n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n        &lt;value&gt;$ldapGroupContextFactory&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;!-- defaults to: simple\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n        &lt;value&gt;simple&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n        &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.authorizationEnabled&lt;/name&gt;\n        &lt;!-- defaults to: false --&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;!-- defaults to: simple\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemAuthenticationMechanism&lt;/name&gt;\n        &lt;value&gt;simple&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.searchBase&lt;/name&gt;\n        &lt;value&gt;ou=groups,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;!-- defaults to: groupOfNames\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.groupObjectClass&lt;/name&gt;\n        &lt;value&gt;groupOfNames&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n    &lt;!-- defaults to: member\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttribute&lt;/name&gt;\n        &lt;value&gt;member&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n    &lt;param&gt;\n         &lt;name&gt;main.cacheManager&lt;/name&gt;\n         &lt;value&gt;org.apache.shiro.cache.MemoryConstrainedCacheManager&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.securityManager.cacheManager&lt;/name&gt;\n        &lt;value&gt;$cacheManager&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttributeValueTemplate&lt;/name&gt;\n        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;!-- the above element is the template for most ldap servers \n        for active directory use the following instead and\n        remove the above configuration.\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.memberAttributeValueTemplate&lt;/name&gt;\n        &lt;value&gt;cn={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    --&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemUsername&lt;/name&gt;\n        &lt;value&gt;uid=guest,ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n        &lt;name&gt;main.ldapRealm.contextFactory.systemPassword&lt;/name&gt;\n        &lt;value&gt;${ALIAS=ldcSystemPassword}&lt;/value&gt;\n    &lt;/param&gt;\n\n    &lt;param&gt;\n        &lt;name&gt;urls./**&lt;/name&gt; \n        &lt;value&gt;authcBasic&lt;/value&gt;\n    &lt;/param&gt;\n\n&lt;/provider&gt;\n</code></pre> <p>The configuration shown above would look up Static LDAP groups of the authenticated user and populate the group principals in the Java Subject corresponding to the authenticated user.</p> <p>If you want to look up Dynamic LDAP Groups instead of Static LDAP Groups, you would have to specify groupObjectClass and memberAttribute params as shown below:</p> <pre><code>&lt;param&gt;\n    &lt;name&gt;main.ldapRealm.groupObjectClass&lt;/name&gt;\n    &lt;value&gt;groupOfUrls&lt;/value&gt;\n&lt;/param&gt;\n&lt;param&gt;\n    &lt;name&gt;main.ldapRealm.memberAttribute&lt;/name&gt;\n    &lt;value&gt;memberUrl&lt;/value&gt;\n&lt;/param&gt;\n</code></pre>"},{"location":"config_ldap_group_lookup/#template-topology-files-and-ldif-files-to-try-out-ldap-group-look-up","title":"Template topology files and LDIF files to try out LDAP Group Look up","text":"<p>Knox bundles some template topology files and ldif files that you can use to try and test LDAP Group Lookup and associated authorization ACLs. All these template files are located under <code>{GATEWAY_HOME}/templates</code>.</p>"},{"location":"config_ldap_group_lookup/#ldap-static-group-lookup-templates-authentication-and-group-lookup-from-the-same-directory","title":"LDAP Static Group Lookup Templates, authentication and group lookup from the same directory","text":"<ul> <li>topology file: sandbox.knoxrealm1.xml</li> <li>ldif file: users.ldapgroups.ldif</li> </ul> <p>To try this out</p> <pre><code>cd {GATEWAY_HOME}\ncp templates/sandbox.knoxrealm1.xml conf/topologies/sandbox.xml\ncp templates/users.ldapgroups.ldif conf/users.ldif\njava -jar bin/ldap.jar conf\njava -Dsandbox.ldcSystemPassword=guest-password -jar bin/gateway.jar -persist-master\n</code></pre> <p>Following call to WebHDFS should report HTTP/1.1 401 Unauthorized As guest is not a member of group \"analyst\", authorization provider states user should be member of group \"analyst\"</p> <pre><code>curl  -i -v  -k -u guest:guest-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> <p>Following call to WebHDFS should report: {\"Path\":\"/user/sam\"} As sam is a member of group \"analyst\", authorization provider states user should be member of group \"analyst\"</p> <pre><code>curl  -i -v  -k -u sam:sam-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre>"},{"location":"config_ldap_group_lookup/#ldap-static-group-lookup-templates-authentication-and-group-lookup-from-different-directories","title":"LDAP Static Group Lookup Templates, authentication and group lookup from different  directories","text":"<ul> <li>topology file: sandbox.knoxrealm2.xml</li> <li>ldif file: users.ldapgroups.ldif</li> </ul> <p>To try this out</p> <pre><code>cd {GATEWAY_HOME}\ncp templates/sandbox.knoxrealm2.xml conf/topologies/sandbox.xml\ncp templates/users.ldapgroups.ldif conf/users.ldif\njava -jar bin/ldap.jar conf\njava -Dsandbox.ldcSystemPassword=guest-password -jar bin/gateway.jar -persist-master\n</code></pre> <p>Following call to WebHDFS should report HTTP/1.1 401 Unauthorized As guest is not a member of group \"analyst\", authorization provider states user should be member of group \"analyst\"</p> <pre><code>curl  -i -v  -k -u guest:guest-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> <p>Following call to WebHDFS should report: {\"Path\":\"/user/sam\"} As sam is a member of group \"analyst\", authorization provider states user should be member of group \"analyst\"</p> <pre><code>curl  -i -v  -k -u sam:sam-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre>"},{"location":"config_ldap_group_lookup/#ldap-dynamic-group-lookup-templates-authentication-and-dynamic-group-lookup-from-same-directory","title":"LDAP Dynamic Group Lookup Templates, authentication and dynamic group lookup from same  directory","text":"<ul> <li>topology file: sandbox.knoxrealmdg.xml</li> <li>ldif file: users.ldapdynamicgroups.ldif</li> </ul> <p>To try this out</p> <pre><code>cd {GATEWAY_HOME}\ncp templates/sandbox.knoxrealmdg.xml conf/topologies/sandbox.xml\ncp templates/users.ldapdynamicgroups.ldif conf/users.ldif\njava -jar bin/ldap.jar conf\njava -Dsandbox.ldcSystemPassword=guest-password -jar bin/gateway.jar -persist-master\n</code></pre> <p>Please note that user.ldapdynamicgroups.ldif also loads necessary schema to create dynamic groups in Apache DS.</p> <p>Following call to WebHDFS should report HTTP/1.1 401 Unauthorized As guest is not a member of dynamic group \"directors\", authorization provider states user should be member of group \"directors\"</p> <pre><code>curl  -i -v  -k -u guest:guest-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre> <p>Following call to WebHDFS should report: {\"Path\":\"/user/bob\"} As bob is a member of dynamic group \"directors\", authorization provider states user should be member of group \"directors\"</p> <pre><code>curl  -i -v  -k -u sam:sam-password  -X GET https://localhost:8443/gateway/sandbox/webhdfs/v1?op=GETHOMEDIRECTORY\n</code></pre>"},{"location":"config_metrics/","title":"Metrics","text":""},{"location":"config_metrics/#metrics","title":"Metrics","text":"<p>See the KIP for details on the implementation of metrics available in the gateway.</p> <p>Metrics KIP</p>"},{"location":"config_metrics/#metrics-configuration","title":"Metrics Configuration","text":"<p>Metrics configuration can be done in <code>gateway-site.xml</code>.</p> <p>The initial configuration is mainly for turning on or off the metrics collection and then enabling reporters with their required config.</p> <p>The two initial reporters implemented are JMX and Graphite.</p> <pre><code>gateway.metrics.enabled\n</code></pre> <p>Turns on or off the metrics, default is 'true'</p> <pre><code>gateway.jmx.metrics.reporting.enabled\n</code></pre> <p>Turns on or off the jmx reporter, default is 'true'</p> <pre><code>gateway.graphite.metrics.reporting.enabled\n</code></pre> <p>Turns on or off the graphite reporter, default is 'false'</p> <pre><code>gateway.graphite.metrics.reporting.host\ngateway.graphite.metrics.reporting.port\ngateway.graphite.metrics.reporting.frequency\n</code></pre> <p>The above are the host, port and frequency of reporting (in seconds) parameters for the graphite reporter.</p>"},{"location":"config_mutual_authentication_ssl/","title":"Mutual Auth SSL","text":""},{"location":"config_mutual_authentication_ssl/#mutual-authentication-with-ssl","title":"Mutual Authentication with SSL","text":"<p>To establish a stronger trust relationship between client and server, we provide mutual authentication with SSL via client certs. This is particularly useful in providing additional validation for Preauthenticated SSO with HTTP Headers. Rather than just IP address validation, connections will only be accepted by Knox from clients presenting trusted certificates.</p> <p>This behavior is configured for the entire gateway instance within the gateway-site.xml file. All topologies deployed within the configured gateway instance will require incoming connections to present trusted client certificates during the SSL handshake. Otherwise, connections will be refused.</p> <p>The following table describes the configuration elements related to mutual authentication and their defaults:</p> Configuration Element Description gateway.client.auth.needed True|False - indicating the need for client authentication. Default is False. gateway.truststore.path Fully qualified path to the trust store to use. Default is the keystore used to hold the Gateway's identity.  See <code>gateway.tls.keystore.path</code>. gateway.truststore.type Keystore type of the trust store. Default is JKS. gateway.truststore.password.alias Alias for the password to the trust store. gateway.trust.all.certs Allows for all certificates to be trusted. Default is false. <p>By only indicating that it is needed with <code>gateway.client.auth.needed</code>, the keystore identified by <code>gateway.tls.keystore.path</code> is used.  By default this is <code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code>.  This is the identity keystore for the server, which can also be used as the truststore. To use a dedicated truststore, <code>gateway.truststore.path</code> may be set to the absolute path of the truststore file. The type of truststore file should be set using <code>gateway.truststore.type</code>; else, JKS will be assumed. If the truststore password is different from the Gateway's master secret then it can be set using</p> <pre><code>knoxcli.sh create-alias {password-alias} --value {pwd}\n</code></pre> <p>The password alias name (<code>{password-alias}</code>) is set using <code>gateway.truststore.password.alias</code>; else, the alias name of \"gateway-truststore-password\" should be used. If a password is not found using the provided (or default) alias name, then the Gateway's master secret will be used.</p>"},{"location":"config_mutual_authentication_ssl/#exclude-a-topology-from-mtls","title":"Exclude a Topology from mTLS","text":"<p>There is a possibility to exclude specific topologies from mutual authentication.</p> Configuration Element Description <code>gateway.client.auth.needed</code> True - Indicating the need for client authentication. <code>gateway.port.mapping.enabled</code> True - Enabling the port mapping feature. It is turned on by default. <code>gateway.port.mapping.{topologyName}</code> The port number that this topology will listen on. <code>gateway.client.auth.exclude</code> The names of the topologies separated by comma. These topologies will be excluded from mTLS. <p>To exclude a topology from mTLS we use the port mapping feature. The <code>gateway.port.mapping.enabled</code> feature has to be enabled which is the default behaviour and a port number has to be provided for the topology with the <code>gateway.port.mapping.{topologyName}</code> property. The same topology needs to be added to the <code>gateway.client.auth.exclude</code> property.</p> <p>The below example excludes the <code>health</code> topology from mTLS on the 9443 port.</p> <pre><code>  &lt;property&gt;\n      &lt;name&gt;gateway.port.mapping.health&lt;/name&gt;\n      &lt;value&gt;9443&lt;/value&gt;\n      &lt;description&gt;Topology and Port mapping&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;gateway.client.auth.exclude&lt;/name&gt;\n      &lt;value&gt;health&lt;/value&gt;\n      &lt;description&gt;Topology excluded from mTLS&lt;/description&gt;\n  &lt;/property&gt;\n</code></pre> <p>An example how one can access the health topology on port 9443 without mTLS.</p> <pre><code> https://{gateway-host}:9443/{gateway-path}/health\n</code></pre>"},{"location":"config_pac4j_provider/","title":"PAC4J","text":""},{"location":"config_pac4j_provider/#pac4j-provider-cas-oauth-saml-openid-connect","title":"Pac4j Provider - CAS / OAuth / SAML / OpenID Connect","text":"<p>pac4j is a Java security engine to authenticate users, get their profiles and manage their authorizations in order to secure Java web applications.</p> <p>It supports many authentication mechanisms for UI and web services and is implemented by many frameworks and tools.</p> <p>For Knox, it is used as a federation provider to support the OAuth, CAS, SAML and OpenID Connect protocols. It must be used for SSO, in association with the KnoxSSO service and optionally with the SSOCookieProvider for access to REST APIs.</p>"},{"location":"config_pac4j_provider/#configuration","title":"Configuration","text":""},{"location":"config_pac4j_provider/#sso-topology","title":"SSO topology","text":"<p>To enable SSO for REST API access through the Knox gateway, you need to protect your Hadoop services with the SSOCookieProvider configured to use the KnoxSSO service (sandbox.xml topology):</p> <pre><code>&lt;gateway&gt;\n  &lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;SSOCookieProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n      &lt;name&gt;sso.authentication.provider.url&lt;/name&gt;\n      &lt;value&gt;https://127.0.0.1:8443/gateway/knoxsso/api/v1/websso&lt;/value&gt;\n    &lt;/param&gt;\n  &lt;/provider&gt;\n  &lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Default&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n  &lt;/provider&gt;\n&lt;/gateway&gt;\n\n&lt;service&gt;\n  &lt;role&gt;NAMENODE&lt;/role&gt;\n  &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n&lt;/service&gt;\n\n...\n</code></pre> <p>and protect the KnoxSSO service by the pac4j provider (knoxsso.xml topology):</p> <pre><code>&lt;gateway&gt;\n  &lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;pac4j&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n      &lt;name&gt;pac4j.callbackUrl&lt;/name&gt;\n      &lt;value&gt;https://127.0.0.1:8443/gateway/knoxsso/api/v1/websso&lt;/value&gt;\n    &lt;/param&gt;\n    &lt;param&gt;\n      &lt;name&gt;cas.loginUrl&lt;/name&gt;\n      &lt;value&gt;https://casserverpac4j.herokuapp.com/login&lt;/value&gt;\n    &lt;/param&gt;\n  &lt;/provider&gt;\n  &lt;provider&gt;\n    &lt;role&gt;identity-assertion&lt;/role&gt;\n    &lt;name&gt;Default&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n  &lt;/provider&gt;\n&lt;/gateway&gt;\n\n&lt;service&gt;\n  &lt;role&gt;KNOXSSO&lt;/role&gt;\n  &lt;param&gt;\n    &lt;name&gt;knoxsso.cookie.secure.only&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;knoxsso.token.ttl&lt;/name&gt;\n    &lt;value&gt;100000&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n     &lt;name&gt;knoxsso.redirect.whitelist.regex&lt;/name&gt;\n     &lt;value&gt;^https?:\\/\\/(localhost|127\\.0\\.0\\.1|0:0:0:0:0:0:0:1|::1):[0-9].*$&lt;/value&gt;\n  &lt;/param&gt;\n&lt;/service&gt;\n</code></pre> <p>Notice that the pac4j callback URL is the KnoxSSO URL (<code>pac4j.callbackUrl</code> parameter). An additional <code>pac4j.cookie.domain.suffix</code> parameter allows you to define the domain suffix for the pac4j cookies.</p> <p>In this example, the pac4j provider is configured to authenticate users via a CAS server hosted at: https://casserverpac4j.herokuapp.com/login.</p>"},{"location":"config_pac4j_provider/#parameters","title":"Parameters","text":"<p>You can define the identity provider client/s to be used for authentication with the appropriate parameters - as defined below. When configuring any pac4j identity provider client there is a mandatory parameter that must be defined to indicate the order in which the providers should be engaged with the first in the comma separated list being the default. Consuming applications may indicate their desire to use one of the configured clients with a query parameter called client_name. When there is no client_name specified, the default (first) provider is selected.</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;clientName&lt;/name&gt;\n  &lt;value&gt;CLIENTNAME[,CLIENTNAME]&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>Valid client names are: <code>FacebookClient</code>, <code>TwitterClient</code>, <code>CasClient</code>, <code>SAML2Client</code> or <code>OidcClient</code></p> <p>For tests only, you can use a basic authentication where login equals password by defining the following configuration:</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;clientName&lt;/name&gt;\n  &lt;value&gt;testBasicAuth&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>NOTE: This is NOT a secure mechanism and must NOT be used in production deployments.</p> <p>By default Knox will accept the subject of the returned UserProfile and pass it as the PrimaryPrincipal to the proxied service. If you want to use a different user attribute, you can set the UserProfile attribute name as configuration parameter called pac4j.id_attribute.</p> <pre><code>&lt;param&gt;\n  &lt;name&gt;pac4j.id_attribute&lt;/name&gt;\n  &lt;value&gt;nickname&lt;/value&gt;\n&lt;/param&gt;\n</code></pre> <p>Otherwise, you can use Facebook, Twitter, a CAS server, a SAML IdP or an OpenID Connect provider by using the following parameters:</p>"},{"location":"config_pac4j_provider/#for-oauth-support","title":"For OAuth support:","text":"Name Value facebook.id Identifier of the OAuth Facebook application facebook.secret Secret of the OAuth Facebook application facebook.scope Requested scope at Facebook login facebook.fields Fields returned by Facebook twitter.id Identifier of the OAuth Twitter application twitter.secret Secret of the OAuth Twitter application"},{"location":"config_pac4j_provider/#for-cas-support","title":"For CAS support:","text":"Name Value cas.loginUrl Login URL of the CAS server cas.protocol CAS protocol (<code>CAS10</code>, <code>CAS20</code>, <code>CAS20_PROXY</code>, <code>CAS30</code>, <code>CAS30_PROXY</code>, <code>SAML</code>)"},{"location":"config_pac4j_provider/#for-saml-support","title":"For SAML support:","text":"Name Value saml.keystorePassword Password of the keystore (storepass) saml.privateKeyPassword Password for the private key (keypass) saml.keystorePath Path of the keystore saml.identityProviderMetadataPath Path of the identity provider metadata saml.maximumAuthenticationLifetime Maximum lifetime for authentication saml.serviceProviderEntityId Identifier of the service provider saml.serviceProviderMetadataPath Path of the service provider metadata <p>Get more details on the pac4j wiki.</p> <p>The SSO URL in your SAML 2 provider config will need to include a special query parameter that lets the pac4j provider know that the request is coming back from the provider rather than from a redirect from a KnoxSSO participating application. This query parameter is \"pac4jCallback=true\".</p> <p>This results in a URL that looks something like:</p> <pre><code>https://hostname:8443/gateway/knoxsso/api/v1/websso?pac4jCallback=true&amp;client_name=SAML2Client\n</code></pre> <p>This also means that the SP Entity ID should also include this query parameter as appropriate for your provider. Often something like the above URL is used for both the SSO URL and SP Entity ID.</p>"},{"location":"config_pac4j_provider/#for-openid-connect-support","title":"For OpenID Connect support:","text":"Name Value oidc.id Identifier of the OpenID Connect provider oidc.secret Secret of the OpenID Connect provider oidc.discoveryUri Direcovery URI of the OpenID Connect provider oidc.useNonce Whether to use nonce during login process oidc.preferredJwsAlgorithm Preferred JWS algorithm oidc.maxClockSkew Max clock skew during login process oidc.customParamKey1 Key of the first custom parameter oidc.customParamValue1 Value of the first custom parameter oidc.customParamKey2 Key of the second custom parameter oidc.customParamValue2 Value of the second custom parameter <p>Get more details on the pac4j wiki.</p> <p>In fact, you can even define several identity providers at the same time, the first being chosen by default unless you define a <code>client_name</code> parameter to specify it (<code>FacebookClient</code>, <code>TwitterClient</code>, <code>CasClient</code>, <code>SAML2Client</code> or <code>OidcClient</code>).</p>"},{"location":"config_pac4j_provider/#ui-invocation","title":"UI invocation","text":"<p>In a browser, when calling your Hadoop service (for example: <code>https://127.0.0.1:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS</code>), you are redirected to the identity provider for login. Then, after a successful authentication, your are redirected back to your originally requested URL and your KnoxSSO session is initialized.</p>"},{"location":"config_pam_authn/","title":"PAM Authentication","text":""},{"location":"config_pam_authn/#pam-based-authentication","title":"PAM based Authentication","text":"<p>There is a large number of pluggable authentication modules available on many Linux installations and from vendors of authentication solutions that are great to leverage for authenticating access to Hadoop through the Knox Gateway. In addition to LDAP support described in this guide, the ShiroProvider also includes support for PAM based authentication for unix based systems.</p> <p>This opens up the integration possibilities to many other readily available authentication mechanisms as well as other implementations for LDAP based authentication. More flexibility may be available through various PAM modules for group lookup, more complicated LDAP schemas or other areas where the KnoxLdapRealm is not sufficient.</p>"},{"location":"config_pam_authn/#configuration","title":"Configuration","text":""},{"location":"config_pam_authn/#overview","title":"Overview","text":"<p>The primary motivation for leveraging PAM based authentication is to provide the ability to use the configuration provided by existing PAM modules that are available in a system's <code>/etc/pam.d/</code> directory. Therefore, the solution provided here is as simple as possible in order to allow the PAM module config itself to be the source of truth. What we do need to configure is the fact that we are using PAM through the <code>main.pamRealm</code> parameter and the KnoxPamRealm classname and the particular PAM module to use with the <code>main.pamRealm.service</code> parameter in the below example we have 'login'.</p> <pre><code>&lt;provider&gt; \n   &lt;role&gt;authentication&lt;/role&gt; \n   &lt;name&gt;ShiroProvider&lt;/name&gt; \n   &lt;enabled&gt;true&lt;/enabled&gt; \n   &lt;param&gt; \n        &lt;name&gt;sessionTimeout&lt;/name&gt; \n        &lt;value&gt;30&lt;/value&gt;\n    &lt;/param&gt;                                              \n    &lt;param&gt;\n        &lt;name&gt;main.pamRealm&lt;/name&gt; \n        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxPamRealm&lt;/value&gt;\n    &lt;/param&gt; \n    &lt;param&gt;                                                    \n       &lt;name&gt;main.pamRealm.service&lt;/name&gt; \n       &lt;value&gt;login&lt;/value&gt; \n    &lt;/param&gt;\n    &lt;param&gt;                                                    \n       &lt;name&gt;urls./**&lt;/name&gt; \n       &lt;value&gt;authcBasic&lt;/value&gt; \n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>As a non-normative example of a PAM config file see the below from my MacBook <code>/etc/pam.d/login</code>:</p> <pre><code># login: auth account password session\nauth       optional       pam_krb5.so use_kcminit\nauth       optional       pam_ntlm.so try_first_pass\nauth       optional       pam_mount.so try_first_pass\nauth       required       pam_opendirectory.so try_first_pass\naccount    required       pam_nologin.so\naccount    required       pam_opendirectory.so\npassword   required       pam_opendirectory.so\nsession    required       pam_launchd.so\nsession    required       pam_uwtmp.so\nsession    optional       pam_mount.so\n</code></pre> <p>The first four fields are: service-name, module-type, control-flag and module-filename. The fifth and greater fields are for optional arguments that are specific to the individual authentication modules.</p> <p>The second field in the configuration file is the module-type, it indicates which of the four PAM management services the corresponding module will provide to the application. Our sample configuration file refers to all four groups:</p> <ul> <li>auth: identifies the PAMs that are invoked when the application calls pam_authenticate() and pam_setcred().</li> <li>account: maps to the pam_acct_mgmt() function.</li> <li>session: indicates the mapping for the pam_open_session() and pam_close_session() calls.</li> <li>password: group refers to the pam_chauthtok() function.</li> </ul> <p>Generally, you only need to supply mappings for the functions that are needed by a specific application. For example, the standard password changing application, passwd, only requires a password group entry; any other entries are ignored.</p> <p>The third field indicates what action is to be taken based on the success or failure of the corresponding module. Choices for tokens to fill this field are:</p> <ul> <li>requisite: Failure instantly returns control to the application indicating the nature of the first module failure.</li> <li>required: All these modules are required to succeed for libpam to return success to the application.</li> <li>sufficient: Given that all preceding modules have succeeded, the success of this module leads to an immediate and successful return to the application (failure of this module is ignored).</li> <li>optional: The success or failure of this module is generally not recorded.</li> </ul> <p>The fourth field contains the name of the loadable module, pam_*.so. For the sake of readability, the full pathname of each module is not given. Before Linux-PAM-0.56 was released, there was no support for a default authentication-module directory. If you have an earlier version of Linux-PAM installed, you will have to specify the full path for each of the modules. Your distribution most likely placed these modules exclusively in one of the following directories: /lib/security/ or /usr/lib/security/.</p> <p>Also, find below a non-normative example of a PAM config file(/etc/pam.d/login) for Ubuntu:</p> <pre><code>#%PAM-1.0\n\nauth       required     pam_sepermit.so\n# pam_selinux.so close should be the first session rule\nsession    required     pam_selinux.so close\nsession    required     pam_loginuid.so\n# pam_selinux.so open should only be followed by sessions to be executed in the user context\nsession    required     pam_selinux.so open env_params\nsession    optional     pam_keyinit.so force revoke\n\nsession    required     pam_env.so user_readenv=1 envfile=/etc/default/locale\n@include password-auth\n</code></pre>"},{"location":"config_preauth_sso_provider/","title":"Pre-auth SSO","text":""},{"location":"config_preauth_sso_provider/#preauthenticated-sso-provider","title":"Preauthenticated SSO Provider","text":"<p>A number of SSO solutions provide mechanisms for federating an authenticated identity across applications. These mechanisms are at times simple HTTP Header type tokens that can be used to propagate the identity across process boundaries.</p> <p>Knox Gateway needs a pluggable mechanism for consuming these tokens and federating the asserted identity through an interaction with the Hadoop cluster. </p> <p>CAUTION: The use of this provider requires that proper network security and identity provider configuration and deployment does not allow requests directly to the Knox gateway. Otherwise, this provider will leave the gateway exposed to identity spoofing.</p>"},{"location":"config_preauth_sso_provider/#configuration","title":"Configuration","text":""},{"location":"config_preauth_sso_provider/#overview","title":"Overview","text":"<p>This provider was designed for use with identity solutions such as those provided by CA's SiteMinder and IBM's Tivoli Access Manager. While direct testing with these products has not been done, there has been extensive unit and functional testing that ensure that it should work with such providers.</p> <p>The HeaderPreAuth provider is configured within the topology file and has a minimal configuration that assumes SM_USER for CA SiteMinder. The following example is the bare minimum configuration for SiteMinder (with no IP address validation).</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;HeaderPreAuth&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/provider&gt;\n</code></pre> <p>The following table describes the configuration options for the web app security provider:</p>"},{"location":"config_preauth_sso_provider/#descriptions","title":"Descriptions","text":"Name Description Default preauth.validation.method Optional parameter that indicates the types of trust validation to perform on incoming requests. There could be one or more comma-separated validators defined in this property. If there are multiple validators, Apache Knox validates each validator in the same sequence as it is configured. This works similar to short-circuit AND operation i.e. if any validator fails, Knox does not perform further validation and returns overall failure immediately. Possible values are: null, preauth.default.validation, preauth.ip.validation, custom validator (details described in Custom Validator). Failure results in a 403 forbidden HTTP status response. null - which means 'preauth.default.validation' that is  no validation will be performed and that we are assuming that the network security and external authentication system is sufficient. preauth.ip.addresses Optional parameter that indicates the list of trusted ip addresses. When preauth.ip.validation is indicated as the validation method this parameter must be provided to indicate the trusted ip address set. Wildcarded IPs may be used to indicate subnet level trust. ie. 127.0.* null - which means that no validation will be performed. preauth.custom.header Required parameter for indicating a custom header to use for extracting the preauthenticated principal. The value extracted from this header is utilized as the PrimaryPrincipal within the established Subject. An incoming request that is missing the configured header will be refused with a 401 unauthorized HTTP status. SM_USER for SiteMinder usecase preauth.custom.group.header Optional parameter for indicating a HTTP header name that contains a comma separated list of groups. These are added to the authenticated Subject as group principals. A missing group header will result in no groups being extracted from the incoming request and a log entry but processing will continue. null - which means that there will be no group principals extracted from the request and added to the established Subject. <p>NOTE: Mutual authentication can be used to establish a strong trust relationship between clients and servers while using the Preauthenticated SSO provider. See the configuration for Mutual Authentication with SSL in this document.</p>"},{"location":"config_preauth_sso_provider/#configuration-for-siteminder","title":"Configuration for SiteMinder","text":"<p>The following is an example of a configuration of the preauthenticated SSO provider that leverages the default SM_USER header name - assuming use with CA SiteMinder. It further configures the validation based on the IP address from the incoming request.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;HeaderPreAuth&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;preauth.validation.method&lt;/name&gt;&lt;value&gt;preauth.ip.validation&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;preauth.ip.addresses&lt;/name&gt;&lt;value&gt;127.0.0.2,127.0.0.1&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_preauth_sso_provider/#rest-invocation-for-siteminder","title":"REST Invocation for SiteMinder","text":"<p>The following curl command can be used to request a directory listing from HDFS while passing in the expected header SM_USER.</p> <pre><code>curl -k -i --header \"SM_USER: guest\" -v https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre> <p>Omitting the <code>--header \"SM_USER: guest\"</code> above will result in a rejected request.</p>"},{"location":"config_preauth_sso_provider/#configuration-for-ibm-tivoli-am","title":"Configuration for IBM Tivoli AM","text":"<p>As an example for configuring the preauthenticated SSO provider for another SSO provider, the following illustrates the values used for IBM's Tivoli Access Manager:</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;HeaderPreAuth&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;preauth.custom.header&lt;/name&gt;&lt;value&gt;iv_user&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;preauth.custom.group.header&lt;/name&gt;&lt;value&gt;iv_group&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;preauth.validation.method&lt;/name&gt;&lt;value&gt;preauth.ip.validation&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;preauth.ip.addresses&lt;/name&gt;&lt;value&gt;127.0.0.2,127.0.0.1&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_preauth_sso_provider/#rest-invocation-for-tivoli-am","title":"REST Invocation for Tivoli AM","text":"<p>The following curl command can be used to request a directory listing from HDFS while passing in the expected headers of iv_user and iv_group. Note that the iv_group value in this command matches the expected ACL for webhdfs in the above topology file. Changing this from \"admin\" to \"admin2\" should result in a 401 unauthorized response.</p> <pre><code>curl -k -i --header \"iv_user: guest\" --header \"iv_group: admin\" -v https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre> <p>Omitting the <code>--header \"iv_user: guest\"</code> above will result in a rejected request.</p>"},{"location":"config_sandbox/","title":"Config sandbox","text":""},{"location":"config_sandbox/#sandbox-configuration","title":"Sandbox Configuration","text":""},{"location":"config_sandbox/#sandbox-2x-configuration","title":"Sandbox 2.x Configuration","text":"<p>TODO</p>"},{"location":"config_sandbox/#sandbox-1x-configuration","title":"Sandbox 1.x Configuration","text":"<p>TODO - Update this section to use hostmap if that simplifies things.</p> <p>This version of the Apache Knox Gateway is tested against [Hortonworks Sandbox 1.x][sandbox]</p> <p>Currently there is an issue with Sandbox that prevents it from being easily used with the gateway. In order to correct the issue, you can use the commands below to login to the Sandbox VM and modify the configuration. This assumes that the name sandbox is setup to resolve to the Sandbox VM. It may be necessary to use the IP address of the Sandbox VM instead. This is frequently but not always <code>192.168.56.101</code>.</p> <pre><code>ssh root@sandbox\ncp /usr/lib/hadoop/conf/hdfs-site.xml /usr/lib/hadoop/conf/hdfs-site.xml.orig\nsed -e s/localhost/sandbox/ /usr/lib/hadoop/conf/hdfs-site.xml.orig &gt; /usr/lib/hadoop/conf/hdfs-site.xml\nshutdown -r now\n</code></pre> <p>In addition to make it very easy to follow along with the samples for the gateway you can configure your local system to resolve the address of the Sandbox by the names <code>vm</code> and <code>sandbox</code>. The IP address that is shown below should be that of the Sandbox VM as it is known on your system. This will likely, but not always, be <code>192.168.56.101</code>.</p> <p>On Linux or Macintosh systems add a line like this to the end of the file <code>/etc/hosts</code> on your local machine, not the Sandbox VM. Note: The character between the 192.168.56.101 and vm below is a tab character.</p> <pre><code>192.168.56.101  vm sandbox\n</code></pre> <p>On Windows systems a similar but different mechanism can be used.  On recent versions of windows the file that should be modified is <code>%systemroot%\\system32\\drivers\\etc\\hosts</code></p>"},{"location":"config_sso_cookie_provider/","title":"SSO Cookie","text":""},{"location":"config_sso_cookie_provider/#sso-cookie-provider","title":"SSO Cookie Provider","text":""},{"location":"config_sso_cookie_provider/#overview","title":"Overview","text":"<p>The SSOCookieProvider enables the federation of the authentication event that occurred through KnoxSSO. KnoxSSO is a typical SP initiated websso mechanism that sets a cookie to be presented by browsers to participating applications and cryptographically verified.</p> <p>Knox Gateway needs a pluggable mechanism for consuming these cookies and federating the KnoxSSO authentication event as an asserted identity in its interaction with the Hadoop cluster for REST API invocations. This provider is useful when an application that is integrated with KnoxSSO for authentication also consumes REST APIs through the Knox Gateway.</p> <p>Based on our understanding of the WebSSO flow it should behave like:</p> <ul> <li>SSOCookieProvider checks for hadoop-jwt cookie and in its absence redirects to the configured SSO provider URL (knoxsso endpoint)</li> <li>The configured Provider on the KnoxSSO endpoint challenges the user in a provider specific way (presents form, redirects to SAML IdP, etc.)</li> <li>The authentication provider on KnoxSSO validates the identity of the user through credentials/tokens</li> <li>The WebSSO service exchanges the normalized Java Subject into a JWT token and sets it on the response as a cookie named <code>hadoop-jwt</code></li> <li>The WebSSO service then redirects the user agent back to the originally requested URL - the requested Knox service subsequent invocations will find the cookie in the incoming request and not need to engage the WebSSO service again until it expires.</li> </ul>"},{"location":"config_sso_cookie_provider/#configuration","title":"Configuration","text":""},{"location":"config_sso_cookie_provider/#sandboxjson-topology-example","title":"sandbox.json Topology Example","text":"<p>Configuring one of the cluster topologies to use the SSOCookieProvider instead of the out of the box ShiroProvider would look something like the following:</p> <p>sso-provider.json</p> <pre><code>{\n  \"providers\": [\n    {\n      \"role\": \"federation\",\n      \"name\": \"SSOCookieProvider\",\n      \"enabled\": \"true\",\n      \"params\": {\n        \"sso.authentication.provider.url\": \"https://localhost:9443/gateway/idp/api/v1/websso\"\n      }\n    }\n  ]\n}\n</code></pre> <p>sandbox.json</p> <pre><code>{\n  \"provider-config-ref\": \"sso-provider\",\n  \"services\": [\n    {\n      \"name\": \"WEBHDFS\",\n      \"urls\": [\n        \"http://localhost:50070/webhdfs\"\n      ]\n    },\n    {\n      \"name\": \"WEBHCAT\",\n      \"urls\": [\n        \"http://localhost:50111/templeton\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>The following table describes the configuration options for the sso cookie provider:</p>"},{"location":"config_sso_cookie_provider/#descriptions","title":"Descriptions","text":"Name Description Default sso.authentication.provider.url Required parameter that indicates the location of the KnoxSSO endpoint and where to redirect the useragent when no SSO cookie is found in the incoming request. N/A sso.token.verification.pem Optional parameter that specifies public key used to validate hadoop-jwt token. The key must be in PEM encoded format excluding the header and footer lines. N/A sso.expected.audiences Optional parameter used to constrain the use of tokens on this endpoint to those that have tokens with at least one of the configured audience claims. N/A sso.unauthenticated.path.list Optional - List of paths that should bypass the SSO flow. favicon.ico"},{"location":"config_sso_cookie_provider/#jwt-provider","title":"JWT Provider","text":""},{"location":"config_sso_cookie_provider/#overview_1","title":"Overview","text":"<p>The JWT federation provider accepts JWT tokens as Bearer tokens within the Authorization header of the incoming request. Upon successfully extracting and verifying the token, the request is then processed on behalf of the user represented by the JWT token.</p> <p>This provider is closely related to the Knox Token Service and is essentially the provider that is used to consume the tokens issued by the Knox Token Service.</p> <p>Typical deployments have the KnoxToken service defined in a topology that authenticates users based on username and password with the ShiroProvider. They also have another topology dedicated to clients that wish to use KnoxTokens to access Hadoop resources through Knox.  The following provider configuration can be used with such a topology.</p> <pre><code>\"providers\": [\n  {\n    \"role\": \"federation\",\n    \"name\": \"JWTProvider\",\n    \"enabled\": \"true\",\n    \"params\": {\n      \"knox.token.audiences\": \"tokenbased\"\n    }\n  }\n]\n</code></pre> <p>The <code>knox.token.audiences</code> parameter above indicates that any token in an incoming request must contain an audience claim called \"tokenbased\". In this case, the idea is that the issuing KnoxToken service will be configured to include such an audience claim and that the resulting token is valid to use in the topology that contains configuration like above. This would generally be the name of the topology but you can standardize on anything.</p> <p>The following table describes the configuration options for the JWT federation provider:</p>"},{"location":"config_sso_cookie_provider/#descriptions_1","title":"Descriptions","text":"Name Description Default knox.token.audiences Optional parameter. This parameter allows the administrator to constrain the use of tokens on this endpoint to those that have tokens with at least one of the configured audience claims. These claims have associated configuration within the KnoxToken service as well. This provides an interesting way to make sure that the token issued based on authentication to a particular LDAP server or other IdP is accepted but not others. N/A knox.token.exp.server-managed Optional parameter for specifying that server-managed token state should be referenced for evaluating token validity. false knox.token.verification.pem Optional parameter that specifies public key used to validate the token. The key must be in PEM encoded format excluding the header and footer lines. N/A knox.token.use.cookie Optional parameter that indicates if the JWT token can be retrieved from an HTTP cookie instead of the Authorization header. If this is set to <code>true</code>, then Knox will first check if the <code>hadoop-jwt</code> cookie (the cookie name is configurable) is available in the request and, if that's the case, Knox will try to fetch a JWT from that cookie. If the cookie is not present in the request, Knox will continue its authentication flow using the Authorization header. If the cookie is there, but it holds an invalid JWT, then authentication will fail. Sample use cases and <code>curl</code> commands are available in this GitHub Pull Request. false knox.token.cookie.name Optional parameter to use a custom cookie name in the request if <code>knox.token.use.cookie = true</code>. hadoop-jwt knox.token.allowed.jws.types With KNOX-2149, one can define their own JWKS URL which Knox can use for verification. Previous Knox implementations only supported JWTs with <code>\"typ: JWT\"</code> in their headers (or not type definition at all). In previous JOSE versions, there were other supported types such as <code>at+jwt</code> which Knox can support from now on. Please note, this configuration is only applied if token verification goes through the JWKS verification path. <code>JWT</code> <p>The optional <code>knox.token.exp.server-managed</code> parameter indicates that Knox is managing the state of tokens it issues (e.g., expiration) external from the token, and this external state should be referenced when validating tokens. This parameter can be ommitted if the global default is configured in gateway-site (see gateway.knox.token.exp.server-managed), and matches the requirements of this provider. Otherwise, this provider parameter overrides the gateway configuration for the provider's deployment.</p> <p>See the documentation for the Knox Token service for related details.</p>"},{"location":"config_tls_client_certificate_authentication_provider/","title":"TLS Client Certificate","text":""},{"location":"config_tls_client_certificate_authentication_provider/#tls-client-certificate-provider","title":"TLS Client Certificate Provider","text":"<p>The TLS client certificate authentication provider enables establishing the user based on the client provided TLS certificate. The user will be the DN from the certificate. This provider requires that the gateway is configured to require client authentication with either <code>gateway.client.auth.wanted</code> or <code>gateway.client.auth.needed</code> ( #[Mutual Authentication with SSL] ).</p>"},{"location":"config_tls_client_certificate_authentication_provider/#configuration","title":"Configuration","text":"<pre><code>&lt;provider&gt;\n    &lt;role&gt;authentication&lt;/role&gt;\n    &lt;name&gt;ClientCert&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_webappsec_provider/","title":"Web App Security","text":""},{"location":"config_webappsec_provider/#web-app-security-provider","title":"Web App Security Provider","text":"<p>Knox is a Web API (REST) Gateway for Hadoop. The fact that REST interactions are HTTP based means that they are vulnerable to a number of web application security vulnerabilities. This project introduces a web application security provider for plugging in various protection filters.</p>"},{"location":"config_webappsec_provider/#configuration","title":"Configuration","text":""},{"location":"config_webappsec_provider/#overview","title":"Overview","text":"<p>As with all providers in the Knox gateway, the web app security provider is configured through provider parameters. Unlike many other providers, the web app security provider may actually host multiple vulnerability/security filters. Currently, we only have implementations for CSRF, CORS and HTTP STS but others might follow, and you may be interested in creating your own.</p> <p>Because of this one-to-many provider/filter relationship, there is an extra configuration element for this provider per filter. As you can see in the sample below, the actual filter configuration is defined entirely within the parameters of the WebAppSec provider.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;webappsec&lt;/role&gt;\n    &lt;name&gt;WebAppSec&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;csrf.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;csrf.customHeader&lt;/name&gt;&lt;value&gt;X-XSRF-Header&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;csrf.methodsToIgnore&lt;/name&gt;&lt;value&gt;GET,OPTIONS,HEAD&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;cors.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;xframe.options.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;xss.protection.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;strict.transport.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"config_webappsec_provider/#descriptions","title":"Descriptions","text":"<p>The following tables describes the configuration options for the web app security provider:</p>"},{"location":"config_webappsec_provider/#csrf","title":"CSRF","text":"<p>Cross site request forgery (CSRF) attacks attempt to force an authenticated user to  execute functionality without their knowledge. By presenting them with a link or image that when clicked invokes a request to another site with which the user may have already established an active session.</p> <p>CSRF is entirely a browser-based attack. Some background knowledge of how browsers work enables us to provide a filter that will prevent CSRF attacks. HTTP requests from a web browser performed via form, image, iframe, etc. are unable to set custom HTTP headers. The only way to create a HTTP request from a browser with a custom HTTP header is to use a technology such as JavaScript XMLHttpRequest or Flash. These technologies can set custom HTTP headers but have security policies built in to prevent web sites from sending requests to each other  unless specifically allowed by policy. </p> <p>This means that a website www.bad.com cannot send a request to  http://bank.example.com with the custom header X-XSRF-Header unless they use a technology such as a XMLHttpRequest. That technology would prevent such a request from being made unless the bank.example.com domain specifically allowed it. This then results in a REST endpoint that can only be called via XMLHttpRequest (or similar technology).</p> <p>NOTE: by enabling this protection within the topology, this custom header will be required for all clients that interact with it - not just browsers.</p>"},{"location":"config_webappsec_provider/#config","title":"Config","text":"Name Description Default csrf.enabled This parameter enables the CSRF protection capabilities false csrf.customHeader This is an optional parameter that indicates the name of the header to be used in order to determine that the request is from a trusted source. It defaults to the header name described by the NSA in its guidelines for dealing with CSRF in REST. X-XSRF-Header csrf.methodsToIgnore This is also an optional parameter that enumerates the HTTP methods to allow through without the custom HTTP header. This is useful for allowing things like GET requests from the URL bar of a browser, but it assumes that the GET request adheres to REST principals in terms of being idempotent. If this cannot be assumed then it would be wise to not include GET in the list of methods to ignore. GET,OPTIONS,HEAD"},{"location":"config_webappsec_provider/#rest-invocation","title":"REST Invocation","text":"<p>The following curl command can be used to request a directory listing from HDFS while passing in the expected header X-XSRF-Header.</p> <pre><code>curl -k -i --header \"X-XSRF-Header: valid\" -v -u guest:guest-password https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre> <p>Omitting the <code>--header \"X-XSRF-Header: valid\"</code> above should result in an HTTP 400 bad_request.</p> <p>Disabling the provider will then allow a request that is missing the header through. </p>"},{"location":"config_webappsec_provider/#cors","title":"CORS","text":"<p>For security reasons, browsers restrict cross-origin HTTP requests initiated from within scripts. For example, XMLHttpRequest follows the same-origin policy. So, a web application using XMLHttpRequest could only make HTTP requests to its own domain. To improve web applications, developers asked browser vendors to allow XMLHttpRequest to make cross-domain requests.</p> <p>Cross Origin Resource Sharing is a way to explicitly alter the same-origin policy for a given application or API. In order to allow for applications to make cross domain requests through Apache Knox, we need to configure the CORS filter of the WebAppSec provider.</p>"},{"location":"config_webappsec_provider/#config_1","title":"Config","text":"Name Description Default cors.enabled Setting this parameter to true allows cross origin requests. The default of false prevents cross origin requests. false cors.allowGenericHttpRequests {true|false} defaults to true. If true, generic HTTP requests will be allowed to pass through the filter, else only valid and accepted CORS requests will be allowed (strict CORS filtering). true cors.allowOrigin {\"*\"|origin-list} defaults to \"*\". Whitespace-separated list of origins that the CORS filter must allow. Requests from origins not included here will be refused with an HTTP 403 \"Forbidden\" response. If set to * (asterisk) any origin will be allowed. \"*\" cors.allowSubdomains {true|false} defaults to false. If true, the CORS filter will allow requests from any origin which is a subdomain origin of the allowed origins. A subdomain is matched by comparing its scheme and suffix (host name / IP address and optional port number). false cors.supportedMethods {method-list} defaults to GET, POST, HEAD, OPTIONS. List of the supported HTTP methods. These are advertised through the Access-Control-Allow-Methods header and must also be implemented by the actual CORS web service. Requests for methods not included here will be refused by the CORS filter with an HTTP 405 \"Method not allowed\" response. GET, POST, HEAD, OPTIONS cors.supportedHeaders {\"*\"|header-list} defaults to *. The names of the supported author request headers. These are advertised through the Access-Control-Allow-Headers header. If the configuration property value is set to * (asterisk) any author request header will be allowed. The CORS Filter implements this by simply echoing the requested value back to the browser. * cors.exposedHeaders {header-list} defaults to empty list. List of the response headers other than simple response headers that the browser should expose to the author of the cross-domain request through the XMLHttpRequest.getResponseHeader() method. The CORS filter supplies this information through the Access-Control-Expose-Headers header. empty cors.supportsCredentials {true|false} defaults to true. Indicates whether user credentials, such as cookies, HTTP authentication or client-side certificates, are supported. The CORS filter uses this value in constructing the Access-Control-Allow-Credentials header. true cors.maxAge {int} defaults to -1 (unspecified). Indicates how long the results of a preflight request can be cached by the web browser, in seconds. If -1 unspecified. This information is passed to the browser via the Access-Control-Max-Age header. -1 cors.tagRequests {true|false} defaults to false (no tagging). Enables HTTP servlet request tagging to provide CORS information to downstream handlers (filters and/or servlets). false"},{"location":"config_webappsec_provider/#x-frame-options","title":"X-Frame-Options","text":"<p>Cross Frame Scripting and Clickjacking are attacks that can be prevented by controlling the ability for a third-party to embed an application or resource within a Frame, IFrame or Object html element. This can be done adding the X-Frame-Options HTTP header to responses.</p>"},{"location":"config_webappsec_provider/#config_2","title":"Config","text":"Name Description Default xframe.options.enabled This parameter enables the X-Frame-Options capabilities false xframe.options.value This parameter specifies a particular value for the X-Frame-Options header. Most often the default value of DENY will be most appropriate. You can also use SAMEORIGIN or ALLOW-FROM uri DENY"},{"location":"config_webappsec_provider/#x-xss-protection","title":"X-XSS-Protection","text":"<p>Cross-site Scripting (XSS) type attacks can be prevented by adding the X-XSS-Protection header to HTTP response. The <code>1; mode=block</code> value will force browser to stop rendering the page if XSS attack is detected. </p>"},{"location":"config_webappsec_provider/#config_3","title":"Config","text":"Name Description Default xss.protection.enabled This parameter specifies a particular value for the X-XSS-Protection header. When it is set to true, it will add <code>X-Xss-Protection: '1; mode=block'</code> header to HTTP response false"},{"location":"config_webappsec_provider/#x-content-type-options","title":"X-Content-Type-Options","text":"<p>Browser MIME content type sniffing can be exploited for malicious purposes. Adding the X-Content-Type-Options HTTP header to responses directs the browser to honor the type specified in the Content-Type header, rather than trying to determine the type from the content itself. Most modern browsers support this.</p>"},{"location":"config_webappsec_provider/#config_4","title":"Config","text":"Name Description Default xcontent-type.options.enabled This param enables the X-Content-Type-Options header inclusion false xcontent-type.options This param specifies a particular value for the X-Content-Type-Options header. The default value is really the only meaningful value nosniff"},{"location":"config_webappsec_provider/#http-strict-transport-security","title":"HTTP Strict Transport Security","text":"<p>HTTP Strict Transport Security (HSTS) is a web security policy mechanism which helps to protect websites against protocol downgrade attacks and cookie hijacking. It allows web servers to declare that web browsers (or other complying user agents) should only interact with it using secure HTTPS connections and never via the insecure HTTP protocol.</p>"},{"location":"config_webappsec_provider/#config_5","title":"Config","text":"Name Description Default strict.transport.enabled This parameter enables the HTTP Strict-Transport-Security response header false strict.transport This parameter specifies a particular value for the HTTP Strict-Transport-Security header. Default value is max-age=31536000. You can also use <code>max-age=&lt;expire-time&gt;</code> or <code>max-age=&lt;expire-time&gt;; includeSubDomains</code> or <code>max-age=&lt;expire-time&gt;;preload</code> max-age=31536000"},{"location":"config_webappsec_provider/#rate-limiting","title":"Rate limiting","text":"<p>Rate limiting is very useful for limiting exposure to abuse from request flooding, whether malicious, or as a result of a misconfigured client. Following are the configurable options:</p> Config Name Description Default rate.limiting.maxRequestsPerSec Maximum number of requests from a connection per second. Requests in excess of this are first delayed, then throttled. 25 rate.limiting.delayMs Delay imposed on all requests over the rate limit, before they are considered at all, in ms. <code>-1 = Reject request</code>, <code>0 = No delay</code>, <code>any other value = Delay in ms</code>. NOTE: with a non-negative value (including <code>0</code>) the <code>gateway.servlet.async.supported</code> property in the <code>gateway-site.xml</code> has to be set to <code>true</code> (it is false by default). 100 rate.limiting.maxWaitMs Length of time to blocking wait for the throttle semaphore in ms. 50 rate.limiting.throttledRequests Number of requests over the rate limit able to be considered at once. 5 rate.limiting.throttleMs Length of time, in ms, to async wait for semaphore. 30000L rate.limiting.maxRequestMs Length of time, in ms, to allow the request to run. 30000L rate.limiting.maxIdleTrackerMs Length of time, in ms, to keep track of request rates for a connection, before deciding that the user has gone away, and discarding it. 30000L rate.limiting.insertHeaders If true, insert the DoSFilter headers into the response. true rate.limiting.trackSessions If true, usage rate is tracked by session if a session exists. true rate.limiting.remotePort If true and session tracking is not used, then rate is tracked by IP and port (effectively connection). false rate.limiting.ipWhitelist A comma-separated list of IP addresses that will not be rate limited. empty <p>When using the gateway with long running requests the <code>rate.limiting.maxRequestMs</code> parameter should be configured accordingly, otherwise the requests running longer then the default <code>30000ms</code> value, will be unsuccessful.</p>"},{"location":"config_webappsec_provider/#config_6","title":"Config","text":"Name Description Default rate.limiting.enabled This parameter enables the rate limiting feature false"},{"location":"knox_cli/","title":"Knox CLI","text":""},{"location":"knox_cli/#knox-cli","title":"Knox CLI","text":"<p>The Knox CLI is a command line utility for the management of various aspects of the Knox deployment. It is primarily concerned with the management of the security artifacts for the gateway instance and each of the deployed topologies or Hadoop clusters that are gated by the Knox Gateway instance.</p> <p>The various security artifacts are also generated and populated automatically by the Knox Gateway runtime when they are not found at startup. The assumptions made in those cases are appropriate for a test or development gateway instance and assume 'localhost' for hostname specific activities. For production deployments the use of the CLI may aid in managing some production deployments.</p> <p>The <code>knoxcli.sh</code> script is located in the <code>{GATEWAY_HOME}/bin</code> directory.</p>"},{"location":"knox_cli/#help","title":"Help","text":""},{"location":"knox_cli/#binknoxclish-help","title":"<code>bin/knoxcli.sh [--help]</code>","text":"<p>prints help for all commands</p>"},{"location":"knox_cli/#knox-version-info","title":"Knox Version Info","text":""},{"location":"knox_cli/#binknoxclish-version-help","title":"<code>bin/knoxcli.sh version [--help]</code>","text":"<p>Displays Knox version information.</p>"},{"location":"knox_cli/#master-secret-persistence","title":"Master secret persistence","text":""},{"location":"knox_cli/#binknoxclish-create-master-force-master-mastersecret-generate","title":"<code>bin/knoxcli.sh create-master [--force] [--master mastersecret] [--generate]</code>","text":"<p>The create-master command persists the master secret in a file located at: <code>{GATEWAY_HOME}/data/security/master</code>.</p> <p>It will prompt the user for the secret to persist.</p> <p>Use <code>--force</code> to overwrite the master secret.</p> <p>Use <code>--master</code> to pass in a master secret to persist. This can be used to persist the secret without any user interaction. Be careful as the secret might appear in shell histories or process listings. Instead of <code>--master</code> it is usually a better idea to use <code>--generate</code> instead!</p> <p>Use <code>--generate</code> to have Knox automatically generate a random secret. The generated secret will not be printed or otherwise exposed.</p> <p>Do not specify both <code>--master</code> and <code>--generate</code> at the same time.</p> <p>NOTE: This command fails when there is an existing master file in the expected location. You may force it to overwrite the master file with the --force switch. NOTE: this will require you to change passwords protecting the keystores for the gateway identity keystores and all credential stores.</p>"},{"location":"knox_cli/#alias-creation","title":"Alias creation","text":""},{"location":"knox_cli/#binknoxclish-create-alias-name-cluster-c-value-v-generate-help","title":"<code>bin/knoxcli.sh create-alias name [--cluster c] [--value v] [--generate] [--help]</code>","text":"<p>Creates a password alias and stores it in a credential store within the <code>{GATEWAY_HOME}/data/security/keystores</code> dir.</p> Argument Description name Name of the alias to create --cluster Name of Hadoop cluster for the cluster specific credential store otherwise assumes that it is for the gateway itself --value Parameter for specifying the actual password otherwise prompted. Escape complex passwords or surround with single quotes --generate Boolean flag to indicate whether the tool should just generate the value. This assumes that --value is not set - will result in error otherwise. User will not be prompted for the value when --generate is set."},{"location":"knox_cli/#batch-alias-creation","title":"Batch alias creation","text":""},{"location":"knox_cli/#binknoxclish-create-aliases-alias-alias1-value-value1-alias-alias2-value-value2-alias-aliasn-value-valuen-cluster-clustername-generate","title":"<code>bin/knoxcli.sh create-aliases --alias alias1 [--value value1] --alias alias2 [--value value2] --alias aliasN [--value valueN] ... [--cluster clustername] [--generate]</code>","text":"<p>Creates multiple password aliases and stores them in a credential store within the <code>{GATEWAY_HOME}/data/security/keystores</code> dir.</p> Argument Description --alias Name of an alias to create. --value Parameter for specifying the actual password otherwise prompted. Escape complex passwords or surround with single quotes. --generate Boolean flag to indicate whether the tool should just generate the value. This assumes that --value is not set - will result in error otherwise. User will not be prompted for the value when --generate is set. --cluster Name of Hadoop cluster for the cluster specific credential store otherwise assumes that it is for the gateway itself"},{"location":"knox_cli/#alias-deletion","title":"Alias deletion","text":""},{"location":"knox_cli/#binknoxclish-delete-alias-name-cluster-c-help","title":"<code>bin/knoxcli.sh delete-alias name [--cluster c] [--help]</code>","text":"<p>Deletes a password and alias mapping from a credential store within <code>{GATEWAY_HOME}/data/security/keystores</code>.</p> Argument Description name Name of the alias to delete --cluster Name of Hadoop cluster for the cluster specific credential store otherwise assumes '__gateway'"},{"location":"knox_cli/#alias-listing","title":"Alias listing","text":""},{"location":"knox_cli/#binknoxclish-list-alias-cluster-c-help","title":"<code>bin/knoxcli.sh list-alias [--cluster c] [--help]</code>","text":"<p>Lists the alias names for the credential store within <code>{GATEWAY_HOME}/data/security/keystores</code>.</p> <p>NOTE: This command will list the aliases in lowercase which is a result of the underlying credential store implementation. Lookup of credentials is a case insensitive operation - so this is not an issue.</p> Argument Description --cluster Name of Hadoop cluster for the cluster specific credential store otherwise assumes '__gateway'"},{"location":"knox_cli/#self-signed-cert-creation","title":"Self-signed cert creation","text":""},{"location":"knox_cli/#binknoxclish-create-cert-hostname-n-help","title":"<code>bin/knoxcli.sh create-cert [--hostname n] [--help]</code>","text":"<p>Creates and stores a self-signed certificate to represent the identity of the gateway instance. This is stored within the <code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code> keystore.</p> Argument Description --hostname Name of the host to be used in the self-signed certificate. This allows multi-host deployments to specify the proper hostnames for hostname verification to succeed on the client side of the SSL connection. The default is 'localhost'."},{"location":"knox_cli/#certificate-export","title":"Certificate Export","text":""},{"location":"knox_cli/#binknoxclish-export-cert-type-jkspemjcekspkcs12-help","title":"<code>bin/knoxcli.sh export-cert [--type JKS|PEM|JCEKS|PKCS12] [--help]</code>","text":"<p>The export-cert command exports the public certificate from the a gateway.jks keystore with the alias of gateway-identity. It will be exported to <code>{GATEWAY_HOME}/data/security/keystores/</code> with a name of <code>gateway-client-trust.&lt;type&gt;</code>. Using the <code>--type</code> option you can specify which keystore type you need (default: PEM)</p> <p>NOTE: The password for the JKS, JCEKS and PKCS12 types is <code>changeit</code>. It can be changed using: <code>keytool -storepasswd -storetype &lt;type&gt; -keystore gateway-client-trust.&lt;type&gt;</code></p>"},{"location":"knox_cli/#topology-redeploy","title":"Topology Redeploy","text":""},{"location":"knox_cli/#binknoxclish-redeploy-cluster-c","title":"<code>bin/knoxcli.sh redeploy [--cluster c]</code>","text":"<p>Redeploys one or all of the gateway's clusters (a.k.a topologies).</p>"},{"location":"knox_cli/#topology-listing","title":"Topology Listing","text":""},{"location":"knox_cli/#binknoxclish-list-topologies-help","title":"<code>bin/knoxcli.sh list-topologies [--help]</code>","text":"<p>Lists all of the topologies found in Knox's topologies directory. Useful for specifying a valid --cluster argument.</p>"},{"location":"knox_cli/#topology-validation","title":"Topology Validation","text":""},{"location":"knox_cli/#binknoxclish-validate-topology-cluster-c-path-path-help","title":"<code>bin/knoxcli.sh validate-topology [--cluster c] [--path path] [--help]</code>","text":"<p>This ensures that a cluster's description (a.k.a. topology) follows the correct formatting rules. It is possible to specify a name of a cluster already in the topology directory, or a path to any file.</p> Argument Description --cluster Name of Hadoop cluster for which you want to validate --path Path to topology file that you wish to validate."},{"location":"knox_cli/#ldap-authentication-and-authorization","title":"LDAP Authentication and Authorization","text":""},{"location":"knox_cli/#binknoxclish-user-auth-test-cluster-c-u-username-p-password-g-d-help","title":"<code>bin/knoxcli.sh user-auth-test [--cluster c] [--u username] [--p password] [--g] [--d] [--help]</code>","text":"<p>This command will test a topology's ability to connect, authenticate, and authorize a user with an LDAP server. The only required argument is the --cluster argument to specify the name of the topology you wish to use. The topology must be valid (passes validate-topology command). If a <code>--u</code> and <code>--p</code> argument are not specified, the command line will prompt for a username and password. If authentication is successful then the command will attempt to use the topology to do an LDAP group lookup. The topology must be configured correctly to do this. If it is not, groups will not return and no errors will be printed unless the <code>--g</code> command is specified. Currently this command only works if a topology supports the use of ShiroProvider for authentication.</p> Argument Description --cluster Required; Name of cluster for which you want to test authentication --u Optional; Username you wish you authenticate with --p Optional; Password you wish to authenticate with --g Optional; Specify that you are looking to return a user's groups. If not specified, group lookup errors won't return --d Optional; Print extra debug info on failed authentication"},{"location":"knox_cli/#topology-ldap-bind","title":"Topology LDAP Bind","text":""},{"location":"knox_cli/#binknoxclish-system-user-auth-test-cluster-c-d-help","title":"<code>bin/knoxcli.sh system-user-auth-test [--cluster c] [--d] [--help]</code>","text":"<p>This command will test a given topology's ability to connect, bind, and authenticate with the LDAP server from the settings specified in the topology file. The bind currently only will with Shiro as the authentication provider. There are also two parameters required inside of the topology for these</p> Argument Description --cluster Required; Name of cluster for which you want to test authentication --d Optional; Print extra debug info on failed authentication"},{"location":"knox_cli/#gateway-service-test","title":"Gateway Service Test","text":""},{"location":"knox_cli/#binknoxclish-service-test-cluster-c-hostname-hostname-port-port-u-username-p-password-d-help","title":"<code>bin/knoxcli.sh service-test [--cluster c] [--hostname hostname] [--port port] [--u username] [--p password] [--d] [--help]</code>","text":"<p>This will test a topology configuration's ability to connect to multiple Hadoop services. Each service found in a topology will be tested with multiple URLs. Results are printed to the console in JSON format.</p> Argument Description --cluster Required; Name of cluster for which you want to test authentication --hostname Required; Hostname of the cluster currently running on the machine --port Optional; Port that the cluster is running on. If not supplied CLI will try to read config files to find the port. --u Required; Username to authorize against Hadoop services --p Required; Password to match username --d Optional; Print extra debug info on failed authentication"},{"location":"knox_cli/#remote-configuration-registry-client-listing","title":"Remote Configuration Registry Client Listing","text":""},{"location":"knox_cli/#binknoxclish-list-registry-clients","title":"<code>bin/knoxcli.sh list-registry-clients</code>","text":"<p>Lists the remote configuration registry clients defined in '{GATEWAY_HOME}/conf/gateway-site.xml'.</p>"},{"location":"knox_cli/#list-provider-configurations-in-a-remote-configuration-registry","title":"List Provider Configurations in a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-list-provider-configs-registry-client-name","title":"<code>bin/knoxcli.sh list-provider-configs --registry-client name</code>","text":"<p>List the provider configurations in the remote configuration registry for which the referenced client provides access.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml"},{"location":"knox_cli/#list-descriptors-in-a-remote-configuration-registry","title":"List Descriptors in a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-list-descriptors-registry-client-name","title":"<code>bin/knoxcli.sh list-descriptors --registry-client name</code>","text":"<p>List the descriptors in the remote configuration registry for which the referenced client provides access.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml"},{"location":"knox_cli/#upload-provider-configuration-to-a-remote-configuration-registry","title":"Upload Provider Configuration to a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-upload-provider-config-providerconfigfile-registry-client-name-entry-name-entryname","title":"<code>bin/knoxcli.sh upload-provider-config providerConfigFile --registry-client name [--entry-name entryName]</code>","text":"<p>Upload a provider configuration file to the remote configuration registry for which the referenced client provides access. By default, the entry name will be the same as the uploaded file's name.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml --entry-name Optional; The name of the entry for the uploaded content in the registry."},{"location":"knox_cli/#upload-descriptor-to-a-remote-configuration-registry","title":"Upload Descriptor to a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-upload-descriptor-descriptorfile-registry-client-name-entry-name-entryname","title":"<code>bin/knoxcli.sh upload-descriptor descriptorFile --registry-client name [--entry-name entryName]</code>","text":"<p>Upload a descriptor file to the remote configuration registry for which the referenced client provides access. By default, the entry name will be the same as the uploaded file's name.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml --entry-name Optional; The name of the entry for the uploaded content in the registry."},{"location":"knox_cli/#delete-a-provider-configuration-from-a-remote-configuration-registry","title":"Delete a Provider Configuration From a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-delete-provider-config-providerconfig-registry-client-name","title":"<code>bin/knoxcli.sh delete-provider-config providerConfig --registry-client name</code>","text":"<p>Delete a provider configuration from the remote configuration registry for which the referenced client provides access.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml"},{"location":"knox_cli/#delete-a-descriptor-from-a-remote-configuration-registry","title":"Delete a Descriptor From a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-delete-descriptor-descriptor-registry-client-name","title":"<code>bin/knoxcli.sh delete-descriptor descriptor --registry-client name</code>","text":"<p>Delete a descriptor from the remote configuration registry for which the referenced client provides access.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml"},{"location":"knox_cli/#get-the-acl-for-an-entry-in-a-remote-configuration-registry","title":"Get the ACL For an Entry in a Remote Configuration Registry","text":""},{"location":"knox_cli/#binknoxclish-get-registry-acl-entry-registry-client-name","title":"<code>bin/knoxcli.sh get-registry-acl entry --registry-client name</code>","text":"<p>List the ACL set for the specified entry in the remote configuration registry for which the referenced client provides access.</p> Argument Description --registry-client Required; The name of a remote configuration registry client, as defined in gateway-site.xml"},{"location":"knox_cli/#convert-topology-file-to-provider-and-descriptor-config-files","title":"Convert topology file to provider and descriptor config files","text":""},{"location":"knox_cli/#binknoxclish-convert-topology-path-pathtotopologyxml-provider-name-my-provjson-descriptor-name-my-descjson","title":"<code>bin/knoxcli.sh convert-topology --path path/to/topology.xml --provider-name my-prov.json [--descriptor-name my-desc.json]</code>","text":"<p>Convert topology xml files to provider and descriptor config files.</p> Argument Description --path Required; Path to topology xml file. --provider-name Required; Name of the provider json config file (including .json extension). --descriptor-name Optional; Name of descriptor json config file (including .json extension). --topology-name Optional; topology-name can be use instead of --path option, if used, KnoxCLI will attempt to find topology from deployed topologies directory. If not provided, topology name will be used as descriptor name --output-path Optional; Output directory to save provider and descriptor config files if not provided, config files will be saved in appropriate Knox config directory. --force Optional; Force rewriting of existing files, if not used, command will fail when the config files with same name already exist. --cluster Optional; Cluster name, required for service discovery. --discovery-url Optional; Service discovery URL, required for service discovery. --discovery-user Optional; Service discovery user, required for service discovery. --discovery-pwd-alias Optional; Password alias for service discovery user, required for service discovery. --discovery-type Optional; Service discovery type, required for service discovery."},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>Here are the steps to have Apache Knox up and running against a Hadoop Cluster:</p> <ol> <li>Verify system requirements</li> <li>Download a virtual machine (VM) with Hadoop </li> <li>Download Apache Knox Gateway</li> <li>Start the virtual machine with Hadoop</li> <li>Install Knox</li> <li>Start the LDAP embedded within Knox</li> <li>Start the Knox Gateway</li> <li>Do Hadoop with Knox</li> </ol>"},{"location":"quick_start/#1-requirements","title":"1 - Requirements","text":""},{"location":"quick_start/#java","title":"Java","text":"<p>Java 1.8 is required for the Knox Gateway runtime. Use the command below to check the version of Java installed on the system where Knox will be running.</p> <pre><code>java -version\n</code></pre>"},{"location":"quick_start/#hadoop","title":"Hadoop","text":"<p>Knox 2.0.0 supports Hadoop 2.x and 3.x, the quick start instructions assume a Hadoop 2.x virtual machine based environment.</p>"},{"location":"quick_start/#2-download-hadoop-2x-vm","title":"2 - Download Hadoop 2.x VM","text":"<p>The quick start provides a link to download Hadoop 2.0 based Hortonworks virtual machine Sandbox. Please note Knox supports other Hadoop distributions and is configurable against a full-blown Hadoop cluster. Configuring Knox for Hadoop 2.x version, or Hadoop deployed in EC2 or a custom Hadoop cluster is documented in advance deployment guide.</p>"},{"location":"quick_start/#3-download-apache-knox-gateway","title":"3 - Download Apache Knox Gateway","text":"<p>Download one of the distributions below from the [Apache mirrors][mirror].</p> <ul> <li>Source archive: knox-2.0.0-src.zip (PGP signature, SHA1 digest, MD5 digest)</li> <li>Binary archive: knox-2.0.0.zip (PGP signature, SHA1 digest, MD5 digest)</li> </ul> <p>Apache Knox Gateway releases are available under the [Apache License, Version 2.0][asl]. See the NOTICE file contained in each release artifact for applicable copyright attribution notices.</p>"},{"location":"quick_start/#verify","title":"Verify","text":"<p>While recommended, verification of signatures is an optional step. You can verify the integrity of any downloaded files using the PGP signatures. Please read Verifying Apache HTTP Server Releases for more information on why you should verify our releases.</p> <p>The PGP signatures can be verified using PGP or GPG. First download the KEYS file as well as the <code>.asc</code> signature files for the relevant release packages. Make sure you get these files from the main distribution directory linked above, rather than from a mirror. Then verify the signatures using one of the methods below.</p> <pre><code>% pgpk -a KEYS\n% pgpv knox-2.0.0.zip.asc\n</code></pre> <p>or</p> <pre><code>% pgp -ka KEYS\n% pgp knox-2.0.0.zip.asc\n</code></pre> <p>or</p> <pre><code>% gpg --import KEYS\n% gpg --verify knox-2.0.0.zip.asc\n</code></pre>"},{"location":"quick_start/#4-start-hadoop-virtual-machine","title":"4 - Start Hadoop virtual machine","text":"<p>Start the Hadoop virtual machine.</p>"},{"location":"quick_start/#5-install-knox","title":"5 - Install Knox","text":"<p>The steps required to install the gateway will vary depending upon which distribution format (zip | rpm) was downloaded. In either case you will end up with a directory where the gateway is installed. This directory will be referred to as your <code>{GATEWAY_HOME}</code> throughout this document.</p>"},{"location":"quick_start/#zip","title":"ZIP","text":"<p>If you downloaded the Zip distribution you can simply extract the contents into a directory. The example below provides a command that can be executed to do this. Note the <code>{VERSION}</code> portion of the command must be replaced with an actual Apache Knox Gateway version number. This might be 2.0.0 for example.</p> <pre><code>unzip knox-{VERSION}.zip\n</code></pre> <p>This will create a directory <code>knox-{VERSION}</code> in your current directory. The directory <code>knox-{VERSION}</code> will considered your <code>{GATEWAY_HOME}</code></p>"},{"location":"quick_start/#6-start-ldap-embedded-in-knox","title":"6 - Start LDAP embedded in Knox","text":"<p>Knox comes with an LDAP server for demonstration purposes. Note: If the tool used to extract the contents of the Tar or tar.gz file was not capable of making the files in the bin directory executable</p> <pre><code>cd {GATEWAY_HOME}\nbin/ldap.sh start\n</code></pre>"},{"location":"quick_start/#7-create-the-master-secret","title":"7 - Create the Master Secret","text":"<p>Run the <code>knoxcli.sh create-master</code> command in order to persist the master secret that is used to protect the key and credential stores for the gateway instance.</p> <pre><code>cd {GATEWAY_HOME}\nbin/knoxcli.sh create-master\n</code></pre> <p>The CLI will prompt you for the master secret (i.e. password).</p>"},{"location":"quick_start/#7-start-knox","title":"7 - Start Knox","text":"<p>The gateway can be started using the provided shell script.</p> <p>The server will discover the persisted master secret during start up and complete the setup process for demo installs. A demo install will consist of a Knox gateway instance with an identity certificate for localhost. This will require clients to be on the same machine or to turn off hostname verification. For more involved deployments, See the Knox CLI section of this document for additional configuration options, including the ability to create a self-signed certificate for a specific hostname.</p> <pre><code>cd {GATEWAY_HOME}\nbin/gateway.sh start\n</code></pre> <p>When starting the gateway this way the process will be run in the background. The log files will be written to <code>{GATEWAY_HOME}/logs</code> and the process ID files (PIDs) will be written to <code>{GATEWAY_HOME}/pids</code>.</p> <p>In order to stop a gateway that was started with the script use this command:</p> <pre><code>cd {GATEWAY_HOME}\nbin/gateway.sh stop\n</code></pre> <p>If for some reason the gateway is stopped other than by using the command above you may need to clear the tracking PID:</p> <pre><code>cd {GATEWAY_HOME}\nbin/gateway.sh clean\n</code></pre> <p>NOTE: This command will also clear any <code>.out</code> and <code>.err</code> file from the <code>{GATEWAY_HOME}/logs</code> directory so use this with caution.</p>"},{"location":"quick_start/#8-access-hadoop-with-knox","title":"8 - Access Hadoop with Knox","text":""},{"location":"quick_start/#invoke-the-liststatus-operation-on-webhdfs-via-the-gateway","title":"Invoke the LISTSTATUS operation on WebHDFS via the gateway.","text":"<p>This will return a directory listing of the root (i.e. <code>/</code>) directory of HDFS.</p> <pre><code>curl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS'\n</code></pre> <p>The results of the above command should result in something to along the lines of the output below. The exact information returned is subject to the content within HDFS in your Hadoop cluster. Successfully executing this command at a minimum proves that the gateway is properly configured to provide access to WebHDFS. It does not necessarily mean that any of the other services are correctly configured to be accessible. To validate that see the sections for the individual services in #[Service Details].</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\nContent-Length: 760\nServer: Jetty(6.1.26)\n\n{\"FileStatuses\":{\"FileStatus\":[\n{\"accessTime\":0,\"blockSize\":0,\"group\":\"hdfs\",\"length\":0,\"modificationTime\":1350595859762,\"owner\":\"hdfs\",\"pathSuffix\":\"apps\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"},\n{\"accessTime\":0,\"blockSize\":0,\"group\":\"mapred\",\"length\":0,\"modificationTime\":1350595874024,\"owner\":\"mapred\",\"pathSuffix\":\"mapred\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"},\n{\"accessTime\":0,\"blockSize\":0,\"group\":\"hdfs\",\"length\":0,\"modificationTime\":1350596040075,\"owner\":\"hdfs\",\"pathSuffix\":\"tmp\",\"permission\":\"777\",\"replication\":0,\"type\":\"DIRECTORY\"},\n{\"accessTime\":0,\"blockSize\":0,\"group\":\"hdfs\",\"length\":0,\"modificationTime\":1350595857178,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"}\n]}}\n</code></pre>"},{"location":"quick_start/#put-a-file-in-hdfs-via-knox","title":"Put a file in HDFS via Knox.","text":"<pre><code>curl -i -k -u guest:guest-password -X PUT \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp/LICENSE?op=CREATE'\n\ncurl -i -k -u guest:guest-password -T LICENSE -X PUT \\\n    '{Value of Location header from response above}'\n</code></pre>"},{"location":"quick_start/#get-a-file-in-hdfs-via-knox","title":"Get a file in HDFS via Knox.","text":"<pre><code>curl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp/LICENSE?op=OPEN'\n\ncurl -i -k -u guest:guest-password -X GET \\\n    '{Value of Location header from command response above}'\n</code></pre>"},{"location":"service_avatica/","title":"Avatica","text":""},{"location":"service_avatica/#avatica","title":"Avatica","text":"<p>Knox provides gateway functionality for access to all Apache Avatica-based servers. The gateway can be used to provide authentication and encryption for clients to servers like the Apache Phoenix Query Server.</p>"},{"location":"service_avatica/#gateway-configuration","title":"Gateway configuration","text":"<p>The Gateway can be configured for Avatica by modifying the topology XML file and providing a new service XML file.</p> <p>In the topology XML file, add the following with the correct hostname:</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;AVATICA&lt;/role&gt;\n  &lt;url&gt;http://avatica:8765&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>Your installation likely already contains the following service files. Ensure that they are present in your installation. In <code>services/avatica/1.9.0/rewrite.xml</code>:</p> <pre><code>&lt;rules&gt;\n    &lt;rule dir=\"IN\" name=\"AVATICA/avatica/inbound/root\" pattern=\"*://*:*/**/avatica/\"&gt;\n        &lt;rewrite template=\"{$serviceUrl[AVATICA]}/\"/&gt;\n    &lt;/rule&gt;\n    &lt;rule dir=\"IN\" name=\"AVATICA/avatica/inbound/path\" pattern=\"*://*:*/**/avatica/{**}\"&gt;\n        &lt;rewrite template=\"{$serviceUrl[AVATICA]}/{**}\"/&gt;\n    &lt;/rule&gt;\n&lt;/rules&gt;\n</code></pre> <p>And in <code>services/avatica/1.9.0/service.xml</code>:</p> <pre><code>&lt;service role=\"AVATICA\" name=\"avatica\" version=\"1.9.0\"&gt;\n    &lt;policies&gt;\n        &lt;policy role=\"webappsec\"/&gt;\n        &lt;policy role=\"authentication\"/&gt;\n        &lt;policy role=\"rewrite\"/&gt;\n        &lt;policy role=\"authorization\"/&gt;\n    &lt;/policies&gt;\n    &lt;routes&gt;\n        &lt;route path=\"/avatica\"&gt;\n            &lt;rewrite apply=\"AVATICA/avatica/inbound/root\" to=\"request.url\"/&gt;\n        &lt;/route&gt;\n        &lt;route path=\"/avatica/**\"&gt;\n            &lt;rewrite apply=\"AVATICA/avatica/inbound/path\" to=\"request.url\"/&gt;\n        &lt;/route&gt;\n    &lt;/routes&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_avatica/#jdbc-drivers","title":"JDBC Drivers","text":"<p>In most cases, users only need to modify the hostname of the Avatica server to instead be the Knox Gateway. To enable authentication, some of the Avatica property need to be added to the Properties object used when constructing the <code>Connection</code> or to the JDBC URL directly.</p> <p>The JDBC URL can be modified like:</p> <pre><code>jdbc:avatica:remote:url=https://knox_gateway.domain:8443/gateway/sandbox/avatica;avatica_user=username;avatica_password=password;authentication=BASIC\n</code></pre> <p>Or, using the <code>Properties</code> class:</p> <pre><code>Properties props = new Properties();\nprops.setProperty(\"avatica_user\", \"username\");\nprops.setProperty(\"avatica_password\", \"password\");\nprops.setProperty(\"authentication\", \"BASIC\");\nDriverManager.getConnection(url, props);\n</code></pre> <p>Additionally, when the TLS certificate of the Knox Gateway is not trusted by your JVM installation, it will be necessary for you to pass in a custom truststore and truststore password to perform the necessary TLS handshake. This can be realized with the <code>truststore</code> and <code>truststore_password</code> properties using the same approaches as above.</p> <p>Via the JDBC URL:</p> <pre><code>jdbc:avatica:remote:url=https://...;authentication=BASIC;truststore=/tmp/knox_truststore.jks;truststore_password=very_secret\n</code></pre> <p>Using Java code:</p> <pre><code>...\nprops.setProperty(\"truststore\", \"/tmp/knox_truststore.jks\");\nprops.setProperty(\"truststore_password\", \"very_secret\");\nDriverManager.getConnection(url, props);\n</code></pre>"},{"location":"service_cloudera_manager/","title":"Cloudera Manager","text":""},{"location":"service_cloudera_manager/#cloudera-manager","title":"Cloudera Manager","text":"<p>Knox provides proxied access to Cloudera Manager API.</p>"},{"location":"service_cloudera_manager/#gateway-configuration","title":"Gateway configuration","text":"<p>The Gateway can be configured for Cloudera Manager API by modifying the topology XML file and providing a new service XML file.</p> <p>In the topology XML file, add the following with the correct hostname:</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;CM-API&lt;/role&gt;\n  &lt;url&gt;http://&lt;cloudera-manager&gt;:7180/api&lt;/url&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_config/","title":"Overview","text":""},{"location":"service_config/#common-service-config","title":"Common Service Config","text":"<p>It is possible to override a few of the global configuration settings provided in gateway-site.xml at the service level. These overrides are specified as name/value pairs within the \\ elements of a particular service. The overridden settings apply only to that service. <p>The following table shows the common configuration settings available at the service level via service level parameters. Individual services may support additional service level parameters.</p> Property Description Default httpclient.maxConnections The maximum number of connections that a single httpclient will maintain to a single host:port. 32 httpclient.connectionTimeout The amount of time to wait when attempting a connection. The natural unit is milliseconds, but a 's' or 'm' suffix may be used for seconds or minutes respectively. The default timeout is system dependent. 20s httpclient.socketTimeout The amount of time to wait for data on a socket before aborting the connection. The natural unit is milliseconds, but a 's' or 'm' suffix may be used for seconds or minutes respectively. The default timeout is system dependent but is likely to be indefinite. 20s <p>The example below demonstrates how these service level parameters are used.</p> <pre><code>&lt;service&gt;\n     &lt;role&gt;HIVE&lt;/role&gt;\n     &lt;param&gt;\n         &lt;name&gt;httpclient.socketTimeout&lt;/name&gt;\n         &lt;value&gt;180s&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_default_ha/","title":"Default HA","text":""},{"location":"service_default_ha/#default-service-ha-support","title":"Default Service HA support","text":"<p>Knox provides connectivity based failover functionality for service calls that can be made to more than one server instance in a cluster. To enable this functionality HaProvider configuration needs to be enabled for the service and the service itself needs to be configured with more than one URL in the topology file.</p> <p>The default HA functionality works on a simple round robin algorithm which can be configured to run in the following modes</p> <ul> <li>The top of the list of URLs is used to route all of a service's REST calls until a connection error occurs (Default).</li> <li>Round robin all the requests, distributing the load evenly across all the HA url's (<code>enableLoadBalancing</code>)</li> <li>Round robin with sticky session, requires cookies. Here, only new sessions will round robin ensuring sticky sessions. In case of failure next configured HA url is picked up to dispatch the request which might cause loss of session depending on the backend implementation (<code>enableStickySession</code>).</li> <li>Round robin with sticky session and no fallback, depends on <code>enableStickySession</code> to be true. Here, new sessions will round robin ensuring sticky sessions. In case of failure, Knox returns http status code 502. By default noFallback is turned off (<code>noFallback</code>).</li> <li>Turn off HA round robin feature for a request based on user-agent header property (<code>disableLoadBalancingForUserAgents</code>).</li> </ul> <p>This goes on until the setting of 'maxFailoverAttempts' is reached.</p> <p>At present the following services can use this default High Availability functionality and have been tested for the same:</p> <ul> <li>WEBHCAT</li> <li>HBASE</li> <li>OOZIE</li> <li>HIVE</li> </ul> <p>To enable HA functionality for a service in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n     &lt;role&gt;ha&lt;/role&gt;\n     &lt;name&gt;HaProvider&lt;/name&gt;\n     &lt;enabled&gt;true&lt;/enabled&gt;\n     &lt;param&gt;\n         &lt;name&gt;{SERVICE}&lt;/name&gt;\n         &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true;enableStickySession=true;&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section i.e. <code>{SERVICE}</code> must match that of the service role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. For example, the value of <code>{SERVICE}</code> can be 'WEBHCAT', 'HBASE' or 'OOZIE'.</p> <p>To configure multiple services in HA mode, additional 'param' sections can be added.</p> <p>For example,</p> <pre><code>&lt;provider&gt;\n     &lt;role&gt;ha&lt;/role&gt;\n     &lt;name&gt;HaProvider&lt;/name&gt;\n     &lt;enabled&gt;true&lt;/enabled&gt;\n     &lt;param&gt;\n         &lt;name&gt;OOZIE&lt;/name&gt;\n         &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n         &lt;name&gt;HBASE&lt;/name&gt;\n         &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true&lt;/value&gt;\n     &lt;/param&gt;\n     &lt;param&gt;\n         &lt;name&gt;WEBHCAT&lt;/name&gt;\n         &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts - This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL will be tried again.</p> </li> <li> <p>failoverSleep - The amount of time in millis that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled - Flag to turn the particular service on or off for HA.</p> </li> <li> <p>enableLoadBalancing - Round robin all the requests, distributing the load evenly across all the HA url's (no sticky sessions)</p> </li> <li> <p>enableStickySession - Round robin with sticky session. </p> </li> <li> <p>noFallback - Round robin with sticky session and no fallback, requires <code>enableStickySession</code> to be true.</p> </li> <li> <p>stickySessionCookieName - Customize sticky session cookie name, default is 'KNOX_BACKEND-{serviceName}'.</p> </li> </ul> <p>And for the service configuration itself the additional URLs should be added to the list.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;{SERVICE}&lt;/role&gt;\n    &lt;url&gt;http://host1:port1&lt;/url&gt;\n    &lt;url&gt;http://host2:port2&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>For example,</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;OOZIE&lt;/role&gt;\n    &lt;url&gt;http://sandbox1:11000/oozie&lt;/url&gt;\n    &lt;url&gt;http://sandbox2:11000/oozie&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <ul> <li>disableLoadBalancingForUserAgents - HA round robin feature can be turned off based on client user-agent header property. To turn it off for a specific user-agent add the user-agent value to parameter <code>disableLoadBalancingForUserAgents</code> which takes a list of user-agents (NOTE: user-agent value does not have to be an exact match, partial match will work). Default value is <code>ClouderaODBCDriverforApacheHive</code></li> </ul> <p>example:</p> <pre><code>&lt;provider&gt;\n     &lt;role&gt;ha&lt;/role&gt;\n     &lt;name&gt;HaProvider&lt;/name&gt;\n     &lt;enabled&gt;true&lt;/enabled&gt;\n     &lt;param&gt;\n         &lt;name&gt;HIVE&lt;/name&gt;\n         &lt;value&gt;enableStickySession=true;enableLoadBalancing=true;enabled=true;disableLoadBalancingForUserAgents=Test User Agent, Test User Agent2,Test User Agent3 ,Test User Agent4 ;retrySleep=1000&lt;/value&gt;\n     &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"service_elasticsearch/","title":"Elasticsearch","text":""},{"location":"service_elasticsearch/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch provides a REST API for communicating with Elasticsearch via JSON over HTTP. Elasticsearch uses X-Pack to do its own security (authentication and authorization). Therefore, the Knox Gateway is to forward the user credentials to Elasticsearch, and treats the Elasticsearch-authenticated user as \"anonymous\" to the backend service via a doas query param while Knox will authenticate to backend services as itself.</p>"},{"location":"service_elasticsearch/#gateway-configuration","title":"Gateway configuration","text":"<p>The Gateway can be configured for Elasticsearch by modifying the topology XML file and providing a new service XML file.</p> <p>In the topology XML file, add the following new service named \"ELASTICSEARCH\" with the correct elasticsearch-rest-server hostname and port number (e.g., 9200):</p> <pre><code> &lt;service&gt;\n   &lt;role&gt;ELASTICSEARCH&lt;/role&gt;\n   &lt;url&gt;http://&lt;elasticsearch-rest-server&gt;:9200/&lt;/url&gt;\n   &lt;name&gt;elasticsearch&lt;/name&gt;\n &lt;/service&gt;\n</code></pre>"},{"location":"service_elasticsearch/#elasticsearch-via-knox-gateway","title":"Elasticsearch via Knox Gateway","text":"<p>After adding the above to a topology, you can make a cURL request similar to the following structures:</p>"},{"location":"service_elasticsearch/#1-elasticsearch-node-root-query","title":"1.  Elasticsearch Node Root Query","text":"<pre><code>curl -i -k -u username:password -H \"Accept: application/json\"  -X GET  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch\"\n\nor\n\ncurl -i -k -u username:password -H \"Accept: application/json\"  -X GET  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/\"\n</code></pre> <p>The quotation marks around the URL, can be single quotes or double quotes on both sides, and can also be omitted (Note: This is true for all other Elasticsearch queries via Knox). Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 16:36:34 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 356\n Server: Jetty(9.2.15.v20160210)\n\n {\"name\":\"w0A80p0\",\"cluster_name\":\"elasticsearch\",\"cluster_uuid\":\"poU7j48pSpu5qQONr64HLQ\",\"version\":{\"number\":\"6.2.4\",\"build_hash\":\"ccec39f\",\"build_date\":\"2018-04-12T20:37:28.497551Z\",\"build_snapshot\":false,\"lucene_version\":\"7.2.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n</code></pre>"},{"location":"service_elasticsearch/#2-elasticsearch-index-creation-deletion-refreshing-and-data-operations-writing-updating-and-retrieval","title":"2.  Elasticsearch Index - Creation, Deletion, Refreshing and Data Operations - Writing, Updating and Retrieval","text":""},{"location":"service_elasticsearch/#1-index-creation","title":"(1) Index Creation","text":"<pre><code>curl -i -k -u username:password -H \"Content-Type: application/json\"  -X PUT  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}\"  -d '{\n\"settings\" : {\n    \"index\" : {\n        \"number_of_shards\" : {index-shards-number},\n        \"number_of_replicas\" : {index-replicas-number}\n    }\n  }\n}'\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 16:51:31 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 65\n Server: Jetty(9.2.15.v20160210)\n\n {\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"estest\"}\n</code></pre>"},{"location":"service_elasticsearch/#2-index-data-writing","title":"(2) Index Data Writing","text":"<p>For adding a \"Hello Joe Smith\" document:</p> <pre><code>curl -i -k -u username:password -H \"Content-Type: application/json\"  -X PUT  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}/{document-type-name}/{document-id}\"  -d '{\n    \"title\":\"Hello Joe Smith\" \n}'\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 201 Created\n Date: Wed, 23 May 2018 17:00:17 GMT\n Location: /estest/greeting/1\n Content-Type: application/json; charset=UTF-8\n Content-Length: 158\n Server: Jetty(9.2.15.v20160210)\n\n {\"_index\":\"estest\",\"_type\":\"greeting\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n</code></pre>"},{"location":"service_elasticsearch/#3-index-refreshing","title":"(3) Index Refreshing","text":"<pre><code>curl -i -k -u username:password  -X POST  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}/_refresh\"\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 17:02:32 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 49\n Server: Jetty(9.2.15.v20160210)\n\n {\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0}}\n</code></pre>"},{"location":"service_elasticsearch/#4-index-data-upgrading","title":"(4) Index Data Upgrading","text":"<p>For changing the Person Joe Smith to Tom Smith:</p> <pre><code>curl -i -k -u username:password -H \"Content-Type: application/json\"  -X PUT  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}/{document-type-name}/{document-id}\"  -d '{ \n\"title\":\"Hello Tom Smith\" \n}'\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 17:09:59 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 158\n Server: Jetty(9.2.15.v20160210)\n\n {\"_index\":\"estest\",\"_type\":\"greeting\",\"_id\":\"1\",\"_version\":2,\"result\":\"updated\",\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":1,\"_primary_term\":1}\n</code></pre>"},{"location":"service_elasticsearch/#5-index-data-retrieval-or-search","title":"(5) Index Data Retrieval or Search","text":"<p>For finding documents with \"title\":\"Hello\" in a specified document-type:</p> <pre><code>curl -i -k -u username:password -H \"Accept: application/json\" -X GET  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}/{document-type-name}/ _search?pretty=true;q=title:Hello\"\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 17:13:08 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 244\n Server: Jetty(9.2.15.v20160210)\n\n {\"took\":0,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":1,\"max_score\":0.2876821,\"hits\":[{\"_index\":\"estest\",\"_type\":\"greeting\",\"_id\":\"1\",\"_score\":0.2876821,\"_source\":{\"title\":\"Hello Tom Smith\"}}]}}\n</code></pre>"},{"location":"service_elasticsearch/#6-index-deleting","title":"(6) Index Deleting","text":"<pre><code>curl -i -k -u username:password  -X DELETE  \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/elasticsearch/{index-name}\"\n</code></pre> <p>Below is an example response:</p> <pre><code> HTTP/1.1 200 OK\n Date: Wed, 23 May 2018 17:20:19 GMT\n Content-Type: application/json; charset=UTF-8\n Content-Length: 21\n Server: Jetty(9.2.15.v20160210)\n\n {\"acknowledged\":true}\n</code></pre>"},{"location":"service_hbase/","title":"HBase","text":""},{"location":"service_hbase/#hbase","title":"HBase","text":"<p>HBase provides an optional REST API (previously called Stargate). See the HBase REST Setup section below for getting started with the HBase REST API and Knox with the Hortonworks Sandbox environment.</p> <p>The gateway by default includes a sample topology descriptor file <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>.  The value in this sample is configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;WEBHBASE&lt;/role&gt;\n    &lt;url&gt;http://localhost:60080&lt;/url&gt;\n    &lt;param&gt;\n        &lt;name&gt;replayBufferSize&lt;/name&gt;\n        &lt;value&gt;8&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/service&gt;\n</code></pre> <p>By default the gateway is configured to use port 60080 for Hbase in the Sandbox.  Please see the steps to configure the port mapping below.</p> <p>A default replayBufferSize of 8KB is shown in the sample topology file above.  This may need to be increased if your query size is larger.</p>"},{"location":"service_hbase/#hbase-url-mapping","title":"HBase URL Mapping","text":"<p>| ------- | ----------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/hbase</code> | | Cluster | <code>http://{hbase-rest-host}:8080/</code>                                         |</p>"},{"location":"service_hbase/#hbase-examples","title":"HBase Examples","text":"<p>The examples below illustrate the set of basic operations with HBase instance using the REST API. Use following link to get more details about HBase REST API: http://hbase.apache.org/book.html#_rest.</p> <p>Note: Some HBase examples may not work due to enabled Access Control. User may not be granted access for performing operations in the samples. In order to check if Access Control is configured in the HBase instance verify <code>hbase-site.xml</code> for a presence of <code>org.apache.hadoop.hbase.security.access.AccessController</code> in <code>hbase.coprocessor.master.classes</code> and <code>hbase.coprocessor.region.classes</code> properties. To grant the Read, Write, Create permissions to <code>guest</code> user execute the following command:</p> <pre><code>echo grant 'guest', 'RWC' | hbase shell\n</code></pre> <p>If you are using a cluster secured with Kerberos you will need to have used <code>kinit</code> to authenticate to the KDC.</p>"},{"location":"service_hbase/#hbase-rest-api-setup","title":"HBase REST API Setup","text":""},{"location":"service_hbase/#launch-rest-api","title":"Launch REST API","text":"<p>The command below launches the REST daemon on port 8080 (the default)</p> <pre><code>sudo {HBASE_BIN}/hbase-daemon.sh start rest\n</code></pre> <p>Where <code>{HBASE_BIN}</code> is <code>/usr/hdp/current/hbase-master/bin/</code> in the case of a HDP install.</p> <p>To use a different port use the <code>-p</code> option:</p> <pre><code>sudo {HBASE_BIN/hbase-daemon.sh start rest -p 60080\n</code></pre>"},{"location":"service_hbase/#configure-sandbox-port-mapping-for-virtualbox","title":"Configure Sandbox port mapping for VirtualBox","text":"<ol> <li>Select the VM</li> <li>Select menu Machine&gt;Settings...</li> <li>Select tab Network</li> <li>Select Adapter 1</li> <li>Press Port Forwarding button</li> <li>Press Plus button to insert new rule: Name=HBASE REST, Host Port=60080, Guest Port=60080</li> <li>Press OK to close the rule window</li> <li>Press OK to Network window save the changes</li> </ol>"},{"location":"service_hbase/#hbase-restart","title":"HBase Restart","text":"<p>If it becomes necessary to restart HBase you can log into the hosts running HBase and use these steps.</p> <pre><code>sudo {HBASE_BIN}/hbase-daemon.sh stop rest\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh stop regionserver\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh stop master\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh stop zookeeper\n\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh start regionserver\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh start master\nsudo -u hbase {HBASE_BIN}/hbase-daemon.sh start zookeeper\nsudo {HBASE_BIN}/hbase-daemon.sh start rest -p 60080\n</code></pre> <p>Where <code>{HBASE_BIN}</code> is <code>/usr/hdp/current/hbase-master/bin/</code> in the case of a HDP Sandbox install.</p>"},{"location":"service_hbase/#hbase-client-dsl","title":"HBase client DSL","text":"<p>For more details about client DSL usage please look at the chapter about the client DSL in this guide.</p> <p>After launching the shell, execute the following command to be able to use the snippets below. <code>import org.apache.knox.gateway.shell.hbase.HBase;</code></p>"},{"location":"service_hbase/#systemversion-query-software-version","title":"systemVersion() - Query Software Version.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).systemVersion().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#clusterversion-query-storage-cluster-version","title":"clusterVersion() - Query Storage Cluster Version.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).clusterVersion().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#status-query-storage-cluster-status","title":"status() - Query Storage Cluster Status.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).status().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#tablelist-query-table-list","title":"table().list() - Query Table List.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example</li> <li><code>HBase.session(session).table().list().now().string</code></li> </ul>"},{"location":"service_hbase/#tablestring-tablenameschema-query-table-schema","title":"table(String tableName).schema() - Query Table Schema.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).table().schema().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamecreate-create-table-schema","title":"table(String tableName).create() - Create Table Schema.","text":"<ul> <li>Request<ul> <li>attribute(String name, Object value) - the table's attribute.</li> <li>family(String name) - starts family definition. Has sub requests:</li> <li>attribute(String name, Object value) - the family's attribute.</li> <li>endFamilyDef() - finishes family definition.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).create()    .attribute(\"tb_attr1\", \"value1\")    .attribute(\"tb_attr2\", \"value2\")    .family(\"family1\")        .attribute(\"fm_attr1\", \"value3\")        .attribute(\"fm_attr2\", \"value4\")    .endFamilyDef()    .family(\"family2\")    .family(\"family3\")    .endFamilyDef()    .attribute(\"tb_attr3\", \"value5\")    .now()</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenameupdate-update-table-schema","title":"table(String tableName).update() - Update Table Schema.","text":"<ul> <li>Request<ul> <li>family(String name) - starts family definition. Has sub requests:</li> <li>attribute(String name, Object value) - the family's attribute.</li> <li>endFamilyDef() - finishes family definition.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).update()      .family(\"family1\")          .attribute(\"fm_attr1\", \"new_value3\")      .endFamilyDef()      .family(\"family4\")          .attribute(\"fm_attr3\", \"value6\")      .endFamilyDef()      .now()```</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenameregions-query-table-metadata","title":"table(String tableName).regions() - Query Table Metadata.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).table(tableName).regions().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamedelete-delete-table","title":"table(String tableName).delete() - Delete Table.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).table(tableName).delete().now()</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamerowstring-rowidstore-cell-store","title":"table(String tableName).row(String rowId).store() - Cell Store.","text":"<ul> <li>Request<ul> <li>column(String family, String qualifier, Object value, Long time) - the data to store; \"qualifier\" may be \"null\"; \"time\" is optional.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).row(\"row_id_1\").store()      .column(\"family1\", \"col1\", \"col_value1\")      .column(\"family1\", \"col2\", \"col_value2\", 1234567890l)      .column(\"family2\", null, \"fam_value1\")      .now()</p> <p>HBase.session(session).table(tableName).row(\"row_id_2\").store()      .column(\"family1\", \"row2_col1\", \"row2_col_value1\")      .now()</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamerowstring-rowidquery-cell-or-row-query","title":"table(String tableName).row(String rowId).query() - Cell or Row Query.","text":"<ul> <li>rowId is optional. Querying with null or empty rowId will select all rows.</li> <li>Request<ul> <li>column(String family, String qualifier) - the column to select; \"qualifier\" is optional.</li> <li>startTime(Long) - the lower bound for filtration by time.</li> <li>endTime(Long) - the upper bound for filtration by time.</li> <li>times(Long startTime, Long endTime) - the lower and upper bounds for filtration by time.</li> <li>numVersions(Long) - the maximum number of versions to return.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).row(\"row_id_1\")      .query()      .now().string</p> <p>HBase.session(session).table(tableName).row().query().now().string</p> <p>HBase.session(session).table(tableName).row().query()      .column(\"family1\", \"row2_col1\")      .column(\"family2\")      .times(0, Long.MAX_VALUE)      .numVersions(1)      .now().string</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamerowstring-rowiddelete-row-column-or-cell-delete","title":"table(String tableName).row(String rowId).delete() - Row, Column, or Cell Delete.","text":"<ul> <li>Request<ul> <li>column(String family, String qualifier) - the column to delete; \"qualifier\" is optional.</li> <li>time(Long) - the upper bound for time filtration.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).row(\"row_id_1\")      .delete()      .column(\"family1\", \"col1\")      .now()```</p> <p>HBase.session(session).table(tableName).row(\"row_id_1\")      .delete()      .column(\"family2\")      .time(Long.MAX_VALUE)      .now()```</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamescannercreate-scanner-creation","title":"table(String tableName).scanner().create() - Scanner Creation.","text":"<ul> <li>Request<ul> <li>startRow(String) - the lower bound for filtration by row id.</li> <li>endRow(String) - the upper bound for filtration by row id.</li> <li>rows(String startRow, String endRow) - the lower and upper bounds for filtration by row id.</li> <li>column(String family, String qualifier) - the column to select; \"qualifier\" is optional.</li> <li>batch(Integer) - the batch size.</li> <li>startTime(Long) - the lower bound for filtration by time.</li> <li>endTime(Long) - the upper bound for filtration by time.</li> <li>times(Long startTime, Long endTime) - the lower and upper bounds for filtration by time.</li> <li>filter(String) - the filter XML definition.</li> <li>maxVersions(Integer) - the maximum number of versions to return.</li> </ul> </li> <li>Response<ul> <li>scannerId : String - the scanner ID of the created scanner. Consumes body.</li> </ul> </li> <li> <p>Example</p> <p>HBase.session(session).table(tableName).scanner().create()      .column(\"family1\", \"col2\")      .column(\"family2\")      .startRow(\"row_id_1\")      .endRow(\"row_id_2\")      .batch(1)      .startTime(0)      .endTime(Long.MAX_VALUE)      .filter(\"\")      .maxVersions(100)      .now()```</p> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamescannerstring-scanneridgetnext-scanner-get-next","title":"table(String tableName).scanner(String scannerId).getNext() - Scanner Get Next.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).table(tableName).scanner(scannerId).getNext().now().string</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#tablestring-tablenamescannerstring-scanneriddelete-scanner-deletion","title":"table(String tableName).scanner(String scannerId).delete() - Scanner Deletion.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse</li> </ul> </li> <li>Example<ul> <li><code>HBase.session(session).table(tableName).scanner(scannerId).delete().now()</code></li> </ul> </li> </ul>"},{"location":"service_hbase/#hbase-via-client-dsl","title":"HBase via Client DSL","text":"<p>This example illustrates sequence of all basic HBase operations:  1. get system version 2. get cluster version 3. get cluster status 4. create the table 5. get list of tables 6. get table schema 7. update table schema 8. insert single row into table 9. query row by id 10. query all rows 11. delete cell from row 12. delete entire column family from row 13. get table regions 14. create scanner 15. fetch values using scanner 16. drop scanner 17. drop the table</p> <p>There are several ways to do this depending upon your preference.</p> <p>You can use the Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar samples/ExampleHBase.groovy\n</code></pre> <p>You can manually type in the KnoxShell DSL script into the interactive Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>Each line from the file below will need to be typed or copied into the interactive shell.</p> <pre><code>/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.knox.gateway.shell.hbase\n\nimport org.apache.knox.gateway.shell.Hadoop\n\nimport static java.util.concurrent.TimeUnit.SECONDS\n\ngateway = \"https://localhost:8443/gateway/sandbox\"\nusername = \"guest\"\npassword = \"guest-password\"\ntableName = \"test_table\"\n\nsession = Hadoop.login(gateway, username, password)\n\nprintln \"System version : \" + HBase.session(session).systemVersion().now().string\n\nprintln \"Cluster version : \" + HBase.session(session).clusterVersion().now().string\n\nprintln \"Status : \" + HBase.session(session).status().now().string\n\nprintln \"Creating table '\" + tableName + \"'...\"\n\nHBase.session(session).table(tableName).create()  \\\n    .attribute(\"tb_attr1\", \"value1\")  \\\n    .attribute(\"tb_attr2\", \"value2\")  \\\n    .family(\"family1\")  \\\n        .attribute(\"fm_attr1\", \"value3\")  \\\n        .attribute(\"fm_attr2\", \"value4\")  \\\n    .endFamilyDef()  \\\n    .family(\"family2\")  \\\n    .family(\"family3\")  \\\n    .endFamilyDef()  \\\n    .attribute(\"tb_attr3\", \"value5\")  \\\n    .now()\n\nprintln \"Done\"\n\nprintln \"Table List : \" + HBase.session(session).table().list().now().string\n\nprintln \"Schema for table '\" + tableName + \"' : \" + HBase.session(session)  \\\n    .table(tableName)  \\\n    .schema()  \\\n    .now().string\n\nprintln \"Updating schema of table '\" + tableName + \"'...\"\n\nHBase.session(session).table(tableName).update()  \\\n    .family(\"family1\")  \\\n        .attribute(\"fm_attr1\", \"new_value3\")  \\\n    .endFamilyDef()  \\\n    .family(\"family4\")  \\\n        .attribute(\"fm_attr3\", \"value6\")  \\\n    .endFamilyDef()  \\\n    .now()\n\nprintln \"Done\"\n\nprintln \"Schema for table '\" + tableName + \"' : \" + HBase.session(session)  \\\n    .table(tableName)  \\\n    .schema()  \\\n    .now().string\n\nprintln \"Inserting data into table...\"\n\nHBase.session(session).table(tableName).row(\"row_id_1\").store()  \\\n    .column(\"family1\", \"col1\", \"col_value1\")  \\\n    .column(\"family1\", \"col2\", \"col_value2\", 1234567890l)  \\\n    .column(\"family2\", null, \"fam_value1\")  \\\n    .now()\n\nHBase.session(session).table(tableName).row(\"row_id_2\").store()  \\\n    .column(\"family1\", \"row2_col1\", \"row2_col_value1\")  \\\n    .now()\n\nprintln \"Done\"\n\nprintln \"Querying row by id...\"\n\nprintln HBase.session(session).table(tableName).row(\"row_id_1\")  \\\n    .query()  \\\n    .now().string\n\nprintln \"Querying all rows...\"\n\nprintln HBase.session(session).table(tableName).row().query().now().string\n\nprintln \"Querying row by id with extended settings...\"\n\nprintln HBase.session(session).table(tableName).row().query()  \\\n    .column(\"family1\", \"row2_col1\")  \\\n    .column(\"family2\")  \\\n    .times(0, Long.MAX_VALUE)  \\\n    .numVersions(1)  \\\n    .now().string\n\nprintln \"Deleting cell...\"\n\nHBase.session(session).table(tableName).row(\"row_id_1\")  \\\n    .delete()  \\\n    .column(\"family1\", \"col1\")  \\\n    .now()\n\nprintln \"Rows after delete:\"\n\nprintln HBase.session(session).table(tableName).row().query().now().string\n\nprintln \"Extended cell delete\"\n\nHBase.session(session).table(tableName).row(\"row_id_1\")  \\\n    .delete()  \\\n    .column(\"family2\")  \\\n    .time(Long.MAX_VALUE)  \\\n    .now()\n\nprintln \"Rows after delete:\"\n\nprintln HBase.session(session).table(tableName).row().query().now().string\n\nprintln \"Table regions : \" + HBase.session(session).table(tableName)  \\\n    .regions()  \\\n    .now().string\n\nprintln \"Creating scanner...\"\n\nscannerId = HBase.session(session).table(tableName).scanner().create()  \\\n    .column(\"family1\", \"col2\")  \\\n    .column(\"family2\")  \\\n    .startRow(\"row_id_1\")  \\\n    .endRow(\"row_id_2\")  \\\n    .batch(1)  \\\n    .startTime(0)  \\\n    .endTime(Long.MAX_VALUE)  \\\n    .filter(\"\")  \\\n    .maxVersions(100)  \\\n    .now().scannerId\n\nprintln \"Scanner id=\" + scannerId\n\nprintln \"Scanner get next...\"\n\nprintln HBase.session(session).table(tableName).scanner(scannerId)  \\\n    .getNext()  \\\n    .now().string\n\nprintln \"Dropping scanner with id=\" + scannerId\n\nHBase.session(session).table(tableName).scanner(scannerId).delete().now()\n\nprintln \"Done\"\n\nprintln \"Dropping table '\" + tableName + \"'...\"\n\nHBase.session(session).table(tableName).delete().now()\n\nprintln \"Done\"\n\nsession.shutdown(10, SECONDS)\n</code></pre>"},{"location":"service_hbase/#hbase-via-curl","title":"HBase via cURL","text":""},{"location":"service_hbase/#get-software-version","title":"Get software version","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\", \"application/json\" or \"application/x-protobuf\"</p> <pre><code>%  curl -ik -u guest:guest-password\\\n -H \"Accept:  application/json\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/version'\n</code></pre>"},{"location":"service_hbase/#get-version-information-regarding-the-hbase-cluster-backing-the-rest-api-instance","title":"Get version information regarding the HBase cluster backing the REST API instance","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\" or \"application/x-protobuf\"</p> <pre><code>%  curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/version/cluster'\n</code></pre>"},{"location":"service_hbase/#get-detailed-status-on-the-hbase-cluster-backing-the-rest-api-instance","title":"Get detailed status on the HBase cluster backing the REST API instance.","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\", \"application/json\" or \"application/x-protobuf\"</p> <pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/status/cluster'\n</code></pre>"},{"location":"service_hbase/#get-the-list-of-available-tables","title":"Get the list of available tables.","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\", \"application/json\" or \"application/x-protobuf\"</p> <pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase'\n</code></pre>"},{"location":"service_hbase/#create-table-with-two-column-families-using-xml-input","title":"Create table with two column families using xml input","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"   -H \"Content-Type: text/xml\"\\\n -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;TableSchema name=\"table1\"&gt;&lt;ColumnSchema name=\"family1\"/&gt;&lt;ColumnSchema name=\"family2\"/&gt;&lt;/TableSchema&gt;'\\\n -X PUT 'https://localhost:8443/gateway/sandbox/hbase/table1/schema'\n</code></pre>"},{"location":"service_hbase/#create-table-with-two-column-families-using-json-input","title":"Create table with two column families using JSON input","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: application/json\"  -H \"Content-Type: application/json\"\\\n -d '{\"name\":\"table2\",\"ColumnSchema\":[{\"name\":\"family3\"},{\"name\":\"family4\"}]}'\\\n -X PUT 'https://localhost:8443/gateway/sandbox/hbase/table2/schema'\n</code></pre>"},{"location":"service_hbase/#get-table-metadata","title":"Get table metadata","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/table1/regions'\n</code></pre>"},{"location":"service_hbase/#insert-single-row-table","title":"Insert single row table","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Content-Type: text/xml\"\\\n -H \"Accept: text/xml\"\\\n -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;CellSet&gt;&lt;Row key=\"cm93MQ==\"&gt;&lt;Cell column=\"ZmFtaWx5MTpjb2wx\" &gt;dGVzdA==&lt;/Cell&gt;&lt;/Row&gt;&lt;/CellSet&gt;'\\\n -X POST 'https://localhost:8443/gateway/sandbox/hbase/table1/row1'\n</code></pre>"},{"location":"service_hbase/#insert-multiple-rows-into-table","title":"Insert multiple rows into table","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Content-Type: text/xml\"\\\n -H \"Accept: text/xml\"\\\n -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;CellSet&gt;&lt;Row key=\"cm93MA==\"&gt;&lt;Cell column=\" ZmFtaWx5Mzpjb2x1bW4x\" &gt;dGVzdA==&lt;/Cell&gt;&lt;/Row&gt;&lt;Row key=\"cm93MQ==\"&gt;&lt;Cell column=\" ZmFtaWx5NDpjb2x1bW4x\" &gt;dGVzdA==&lt;/Cell&gt;&lt;/Row&gt;&lt;/CellSet&gt;'\\\n -X POST 'https://localhost:8443/gateway/sandbox/hbase/table2/false-row-key'\n</code></pre>"},{"location":"service_hbase/#get-all-data-from-table","title":"Get all data from table","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\", \"application/json\" or \"application/x-protobuf\"</p> <pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/table1/*'\n</code></pre>"},{"location":"service_hbase/#execute-cell-or-row-query","title":"Execute cell or row query","text":"<p>Set Accept Header to \"text/plain\", \"text/xml\", \"application/json\" or \"application/x-protobuf\"</p> <pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/table1/row1/family1:col1'\n</code></pre>"},{"location":"service_hbase/#delete-entire-row-from-table","title":"Delete entire row from table","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X DELETE 'https://localhost:8443/gateway/sandbox/hbase/table2/row0'\n</code></pre>"},{"location":"service_hbase/#delete-column-family-from-row","title":"Delete column family from row","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X DELETE 'https://localhost:8443/gateway/sandbox/hbase/table2/row0/family3'\n</code></pre>"},{"location":"service_hbase/#delete-specific-column-from-row","title":"Delete specific column from row","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X DELETE 'https://localhost:8443/gateway/sandbox/hbase/table2/row0/family3'\n</code></pre>"},{"location":"service_hbase/#create-scanner","title":"Create scanner","text":"<p>Scanner URL will be in Location response header</p> <pre><code>curl -ik -u guest:guest-password\\\n -H \"Content-Type: text/xml\"\\\n -d '&lt;Scanner batch=\"1\"/&gt;'\\\n -X PUT 'https://localhost:8443/gateway/sandbox/hbase/table1/scanner'\n</code></pre>"},{"location":"service_hbase/#get-the-values-of-the-next-cells-found-by-the-scanner","title":"Get the values of the next cells found by the scanner","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: application/json\"\\\n -X GET 'https://localhost:8443/gateway/sandbox/hbase/table1/scanner/13705290446328cff5ed'\n</code></pre>"},{"location":"service_hbase/#delete-scanner","title":"Delete scanner","text":"<pre><code>curl -ik -u guest:guest-password\\\n -H \"Accept: text/xml\"\\\n -X DELETE 'https://localhost:8443/gateway/sandbox/hbase/table1/scanner/13705290446328cff5ed'\n</code></pre>"},{"location":"service_hbase/#delete-table","title":"Delete table","text":"<pre><code>curl -ik -u guest:guest-password\\\n -X DELETE 'https://localhost:8443/gateway/sandbox/hbase/table1/schema'\n</code></pre>"},{"location":"service_hbase/#hbase-rest-ha","title":"HBase REST HA","text":"<p>Please look at #[Default Service HA support] if you wish to explicitly list the URLs under the service definition.</p> <p>If you run the HBase REST Server from the HBase Region Server nodes, you can utilize more advanced HA support.  The HBase  REST Server does not register itself with ZooKeeper.  So the Knox HA component looks in ZooKeeper for instances of HBase Region  Servers and then performs a light weight ping for the presence of the REST Server on the same hosts.  The user should not supply URLs  in the service definition.  </p> <p>Note: Users of Ambari must manually startup the HBase REST Server.</p> <p>To enable HA functionality for HBase in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;ha&lt;/role&gt;\n    &lt;name&gt;HaProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;WEBHBASE&lt;/name&gt;\n        &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true;zookeeperEnsemble=machine1:2181,machine2:2181,machine3:2181&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section must match that of the service role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. In this case the name is 'WEBHBASE'.</p> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts - This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL will be tried again after the list is fetched again from Zookeeper (a refresh of the list is done at this point)</p> </li> <li> <p>failoverSleep - The amount of time in millis that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled - Flag to turn the particular service on or off for HA.</p> </li> <li> <p>zookeeperEnsemble - A comma separated list of host names (or IP addresses) of the ZooKeeper hosts that consist of the ensemble that the HBase  servers register their information with. </p> </li> </ul> <p>And for the service configuration itself the URLs need NOT be added to the list. For example:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;WEBHBASE&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>Please note that there is no <code>&lt;url&gt;</code> tag specified here as the URLs for the Kafka servers are obtained from ZooKeeper.</p>"},{"location":"service_hive/","title":"Hive","text":""},{"location":"service_hive/#hive","title":"Hive","text":"<p>The Hive wiki pages describe Hive installation and configuration processes. In sandbox configuration file for Hive is located at <code>/etc/hive/hive-site.xml</code>. Hive Server has to be started in HTTP mode. Note the properties shown below as they are related to configuration required by the gateway.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.http.port&lt;/name&gt;\n    &lt;value&gt;10001&lt;/value&gt;\n    &lt;description&gt;Port number when in HTTP mode.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.http.path&lt;/name&gt;\n    &lt;value&gt;cliservice&lt;/value&gt;\n    &lt;description&gt;Path component of URL endpoint when in HTTP mode.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.transport.mode&lt;/name&gt;\n    &lt;value&gt;http&lt;/value&gt;\n    &lt;description&gt;Server transport mode. \"binary\" or \"http\".&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;hive.server2.allow.user.substitution&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The gateway by default includes a sample topology descriptor file <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>. The value in this sample is configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;HIVE&lt;/role&gt;\n    &lt;url&gt;http://localhost:10001/cliservice&lt;/url&gt;\n    &lt;param&gt;\n        &lt;name&gt;replayBufferSize&lt;/name&gt;\n        &lt;value&gt;8&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/service&gt;\n</code></pre> <p>By default the gateway is configured to use the binary transport mode for Hive in the Sandbox.</p> <p>A default replayBufferSize of 8KB is shown in the sample topology file above.  This may need to be increased if your query size is larger.</p>"},{"location":"service_hive/#hive-jdbc-url-mapping","title":"Hive JDBC URL Mapping","text":"<p>| ------- | ------------------------------------------------------------------------------- | | Gateway | <code>jdbc:hive2://{gateway-host}:{gateway-port}/;ssl=true;sslTrustStore={gateway-trust-store-path};trustStorePassword={gateway-trust-store-password};transportMode=http;httpPath={gateway-path}/{cluster-name}/hive</code> | | Cluster | <code>http://{hive-host}:{hive-port}/{hive-path}</code> |</p>"},{"location":"service_hive/#hive-examples","title":"Hive Examples","text":"<p>This guide provides detailed examples for how to do some basic interactions with Hive via the Apache Knox Gateway.</p>"},{"location":"service_hive/#hive-setup","title":"Hive Setup","text":"<ol> <li>Make sure you are running the correct version of Hive to ensure JDBC/Thrift/HTTP support.</li> <li>Make sure Hive Server is running on the correct port.</li> <li>Make sure Hive Server is running in HTTP mode.</li> <li>Client side (JDBC):<ol> <li>Hive JDBC in HTTP mode depends on following minimal libraries set to run successfully(must be in the classpath):<ul> <li>hive-jdbc-0.14.0-standalone.jar;</li> <li>commons-logging-1.1.3.jar;</li> </ul> </li> <li>Connection URL has to be the following: <code>jdbc:hive2://{gateway-host}:{gateway-port}/;ssl=true;sslTrustStore={gateway-trust-store-path};trustStorePassword={gateway-trust-store-password};transportMode=http;httpPath={gateway-path}/{cluster-name}/hive</code></li> <li>Look at https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DDLOperations for examples.    Hint: For testing it would be better to execute <code>set hive.security.authorization.enabled=false</code> as the first statement.    Hint: Good examples of Hive DDL/DML can be found here http://gettingstarted.hadooponazure.com/hw/hive.html</li> </ol> </li> </ol>"},{"location":"service_hive/#customization","title":"Customization","text":"<p>This example may need to be tailored to the execution environment. In particular host name, host port, user name, user password and context path may need to be changed to match your environment. In particular there is one example file in the distribution that may need to be customized. Take a moment to review this file. All of the values that may need to be customized can be found together at the top of the file.</p> <ul> <li>samples/hive/java/jdbc/sandbox/HiveJDBCSample.java</li> </ul>"},{"location":"service_hive/#client-jdbc-example","title":"Client JDBC Example","text":"<p>Sample example for creating new table, loading data into it from the file system local to the Hive server and querying data from that table.</p>"},{"location":"service_hive/#java","title":"Java","text":"<pre><code>import java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\n\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\n\npublic class HiveJDBCSample {\n\n  public static void main( String[] args ) {\n    Connection connection = null;\n    Statement statement = null;\n    ResultSet resultSet = null;\n\n    try {\n      String user = \"guest\";\n      String password = user + \"-password\";\n      String gatewayHost = \"localhost\";\n      int gatewayPort = 8443;\n      String trustStore = \"/usr/lib/knox/data/security/keystores/gateway.jks\";\n      String trustStorePassword = \"knoxsecret\";\n      String contextPath = \"gateway/sandbox/hive\";\n      String connectionString = String.format( \"jdbc:hive2://%s:%d/;ssl=true;sslTrustStore=%s;trustStorePassword=%s?hive.server2.transport.mode=http;hive.server2.thrift.http.path=/%s\", gatewayHost, gatewayPort, trustStore, trustStorePassword, contextPath );\n\n      // load Hive JDBC Driver\n      Class.forName( \"org.apache.hive.jdbc.HiveDriver\" );\n\n      // configure JDBC connection\n      connection = DriverManager.getConnection( connectionString, user, password );\n\n      statement = connection.createStatement();\n\n      // disable Hive authorization - it could be omitted if Hive authorization\n      // was configured properly\n      statement.execute( \"set hive.security.authorization.enabled=false\" );\n\n      // create sample table\n      statement.execute( \"CREATE TABLE logs(column1 string, column2 string, column3 string, column4 string, column5 string, column6 string, column7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\" );\n\n      // load data into Hive from file /tmp/log.txt which is placed on the local file system\n      statement.execute( \"LOAD DATA LOCAL INPATH '/tmp/log.txt' OVERWRITE INTO TABLE logs\" );\n\n      resultSet = statement.executeQuery( \"SELECT * FROM logs\" );\n\n      while ( resultSet.next() ) {\n        System.out.println( resultSet.getString( 1 ) + \" --- \" + resultSet.getString( 2 ) + \" --- \" + resultSet.getString( 3 ) + \" --- \" + resultSet.getString( 4 ) );\n      }\n    } catch ( ClassNotFoundException ex ) {\n      Logger.getLogger( HiveJDBCSample.class.getName() ).log( Level.SEVERE, null, ex );\n    } catch ( SQLException ex ) {\n      Logger.getLogger( HiveJDBCSample.class.getName() ).log( Level.SEVERE, null, ex );\n    } finally {\n      if ( resultSet != null ) {\n        try {\n          resultSet.close();\n        } catch ( SQLException ex ) {\n          Logger.getLogger( HiveJDBCSample.class.getName() ).log( Level.SEVERE, null, ex );\n        }\n      }\n      if ( statement != null ) {\n        try {\n          statement.close();\n        } catch ( SQLException ex ) {\n          Logger.getLogger( HiveJDBCSample.class.getName() ).log( Level.SEVERE, null, ex );\n        }\n      }\n      if ( connection != null ) {\n        try {\n          connection.close();\n        } catch ( SQLException ex ) {\n          Logger.getLogger( HiveJDBCSample.class.getName() ).log( Level.SEVERE, null, ex );\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"service_hive/#groovy","title":"Groovy","text":"<p>Make sure that <code>{GATEWAY_HOME/ext}</code> directory contains the following libraries for successful execution:</p> <ul> <li>hive-jdbc-0.14.0-standalone.jar;</li> <li>commons-logging-1.1.3.jar;</li> </ul> <p>There are several ways to execute this sample depending upon your preference.</p> <p>You can use the Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar samples/hive/groovy/jdbc/sandbox/HiveJDBCSample.groovy\n</code></pre> <p>You can manually type in the KnoxShell DSL script into the interactive Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>Each line from the file below will need to be typed or copied into the interactive shell.</p> <pre><code>import java.sql.DriverManager\n\nuser = \"guest\";\npassword = user + \"-password\";\ngatewayHost = \"localhost\";\ngatewayPort = 8443;\ntrustStore = \"/usr/lib/knox/data/security/keystores/gateway.jks\";\ntrustStorePassword = \"knoxsecret\";\ncontextPath = \"gateway/sandbox/hive\";\nconnectionString = String.format( \"jdbc:hive2://%s:%d/;ssl=true;sslTrustStore=%s;trustStorePassword=%s?hive.server2.transport.mode=http;hive.server2.thrift.http.path=/%s\", gatewayHost, gatewayPort, trustStore, trustStorePassword, contextPath );\n\n// Load Hive JDBC Driver\nClass.forName( \"org.apache.hive.jdbc.HiveDriver\" );\n\n// Configure JDBC connection\nconnection = DriverManager.getConnection( connectionString, user, password );\n\nstatement = connection.createStatement();\n\n// Disable Hive authorization - This can be omitted if Hive authorization is configured properly\nstatement.execute( \"set hive.security.authorization.enabled=false\" );\n\n// Create sample table\nstatement.execute( \"CREATE TABLE logs(column1 string, column2 string, column3 string, column4 string, column5 string, column6 string, column7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\" );\n\n// Load data into Hive from file /tmp/log.txt which is placed on the local file system\nstatement.execute( \"LOAD DATA LOCAL INPATH '/tmp/sample.log' OVERWRITE INTO TABLE logs\" );\n\nresultSet = statement.executeQuery( \"SELECT * FROM logs\" );\n\nwhile ( resultSet.next() ) {\n  System.out.println( resultSet.getString( 1 ) + \" --- \" + resultSet.getString( 2 ) );\n}\n\nresultSet.close();\nstatement.close();\nconnection.close();\n</code></pre> <p>Examples use 'log.txt' with content:</p> <pre><code>2012-02-03 18:35:34 SampleClass6 [INFO] everything normal for id 577725851\n2012-02-03 18:35:34 SampleClass4 [FATAL] system problem at id 1991281254\n2012-02-03 18:35:34 SampleClass3 [DEBUG] detail for id 1304807656\n2012-02-03 18:35:34 SampleClass3 [WARN] missing id 423340895\n2012-02-03 18:35:34 SampleClass5 [TRACE] verbose detail for id 2082654978\n2012-02-03 18:35:34 SampleClass0 [ERROR] incorrect id  1886438513\n2012-02-03 18:35:34 SampleClass9 [TRACE] verbose detail for id 438634209\n2012-02-03 18:35:34 SampleClass8 [DEBUG] detail for id 2074121310\n2012-02-03 18:35:34 SampleClass0 [TRACE] verbose detail for id 1505582508\n2012-02-03 18:35:34 SampleClass0 [TRACE] verbose detail for id 1903854437\n2012-02-03 18:35:34 SampleClass7 [DEBUG] detail for id 915853141\n2012-02-03 18:35:34 SampleClass3 [TRACE] verbose detail for id 303132401\n2012-02-03 18:35:34 SampleClass6 [TRACE] verbose detail for id 151914369\n2012-02-03 18:35:34 SampleClass2 [DEBUG] detail for id 146527742\n...\n</code></pre> <p>Expected output:</p> <pre><code>2012-02-03 --- 18:35:34 --- SampleClass6 --- [INFO]\n2012-02-03 --- 18:35:34 --- SampleClass4 --- [FATAL]\n2012-02-03 --- 18:35:34 --- SampleClass3 --- [DEBUG]\n2012-02-03 --- 18:35:34 --- SampleClass3 --- [WARN]\n2012-02-03 --- 18:35:34 --- SampleClass5 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass0 --- [ERROR]\n2012-02-03 --- 18:35:34 --- SampleClass9 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass8 --- [DEBUG]\n2012-02-03 --- 18:35:34 --- SampleClass0 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass0 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass7 --- [DEBUG]\n2012-02-03 --- 18:35:34 --- SampleClass3 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass6 --- [TRACE]\n2012-02-03 --- 18:35:34 --- SampleClass2 --- [DEBUG]\n...\n</code></pre>"},{"location":"service_hive/#hiveserver2-ha","title":"HiveServer2 HA","text":"<p>Knox provides basic failover functionality for calls made to Hive Server when more than one HiveServer2 instance is installed in the cluster and registered with the same ZooKeeper ensemble. The HA functionality in this case fetches the HiveServer2 URL information from a ZooKeeper ensemble, so the user need only supply the necessary ZooKeeper configuration and not the Hive connection URLs.</p> <p>To enable HA functionality for Hive in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;ha&lt;/role&gt;\n    &lt;name&gt;HaProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;HIVE&lt;/name&gt;\n        &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true;zookeeperEnsemble=machine1:2181,machine2:2181,machine3:2181;zookeeperNamespace=hiveserver2&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section must match that of the service role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. In this case the name is 'HIVE'.</p> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts - This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL will be tried again after the list is fetched again from Zookeeper (a refresh of the list is done at this point)</p> </li> <li> <p>failoverSleep - The amount of time in millis that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled - Flag to turn the particular service on or off for HA.</p> </li> <li> <p>zookeeperEnsemble - A comma separated list of host names (or IP addresses) of the zookeeper hosts that consist of the ensemble that the Hive servers register their information with. This value can be obtained from Hive's config file hive-site.xml as the value for the parameter 'hive.zookeeper.quorum'.</p> </li> <li> <p>zookeeperNamespace - This is the namespace under which HiveServer2 information is registered in the Zookeeper ensemble. This value can be obtained from Hive's config file hive-site.xml as the value for the parameter 'hive.server2.zookeeper.namespace'.</p> </li> </ul> <p>And for the service configuration itself the URLs need not be added to the list. For example.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;HIVE&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>Please note that there is no <code>&lt;url&gt;</code> tag specified here as the URLs for the Hive servers are obtained from Zookeeper.</p>"},{"location":"service_kafka/","title":"Kafka","text":""},{"location":"service_kafka/#kafka","title":"Kafka","text":"<p>Knox provides gateway functionality to Kafka when used with the Confluent Kafka REST Proxy. The Kafka REST APIs allow the user to view the status  of the cluster, perform administrative actions and produce messages.</p> <p>Note: Consumption of messages via Knox at this time is not supported.</p> <p>The docs for the Confluent Kafka REST Proxy can be found here: http://docs.confluent.io/current/kafka-rest/docs/index.html</p> <p>To enable this functionality, a topology file needs to have the following configuration:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;KAFKA&lt;/role&gt;\n    &lt;url&gt;http://&lt;kafka-rest-host&gt;:&lt;kafka-rest-port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default Kafka REST Proxy port is 8082. If it is configured to some other port, that configuration can be found in  <code>kafka-rest.properties</code> under the property <code>listeners</code>.</p>"},{"location":"service_kafka/#kafka-url-mapping","title":"Kafka URL Mapping","text":"<p>For Kafka URLs, the mapping of Knox Gateway accessible URLs to direct Kafka URLs is the following.</p> <p>| ------- | ------------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/kafka</code> | | Cluster | <code>http://{kakfa-rest-host}:{kafka-rest-port}}</code>                               |</p>"},{"location":"service_kafka/#kafka-examples-via-curl","title":"Kafka Examples via cURL","text":"<p>Some of the various calls that can be made and examples using curl are listed below.</p> <pre><code># 0. Getting topic info\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/kafka/topics'\n\n# 1. Publish message to topic\n\ncurl -ikv -u guest:guest-password -X POST 'https://localhost:8443/gateway/sandbox/kafka/topics/TOPIC1' -H 'Content-Type: application/vnd.kafka.json.v2+json' -H 'Accept: application/vnd.kafka.v2+json' --data '\"records\":[{\"value\":{\"foo\":\"bar\"}}]}'\n</code></pre>"},{"location":"service_kafka/#kafka-ha","title":"Kafka HA","text":"<p>Knox provides basic failover functionality for calls made to Kafka. Since the Confluent Kafka REST Proxy does not register itself with ZooKeeper, the HA component looks in ZooKeeper for instances of Kafka and then performs a light weight ping for the presence of the REST Proxy on the same hosts. As such the Kafka REST Proxy must be installed on the same host as Kafka. The user should not supply URLs in the service definition.  </p> <p>Note: Users of Ambari must manually startup the Confluent Kafka REST Proxy.</p> <p>To enable HA functionality for Kafka in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;ha&lt;/role&gt;\n    &lt;name&gt;HaProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;KAFKA&lt;/name&gt;\n        &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true;zookeeperEnsemble=machine1:2181,machine2:2181,machine3:2181&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section must match that of the service role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. In this case the name is 'KAFKA'.</p> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts - This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL will be tried again after the list is fetched again from Zookeeper (a refresh of the list is done at this point)</p> </li> <li> <p>failoverSleep - The amount of time in millis that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled - Flag to turn the particular service on or off for HA.</p> </li> <li> <p>zookeeperEnsemble - A comma separated list of host names (or IP addresses) of the ZooKeeper hosts that consist of the ensemble that the Kafka servers register their information with. </p> </li> </ul> <p>And for the service configuration itself the URLs need NOT be added to the list. For example:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;KAFKA&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>Please note that there is no <code>&lt;url&gt;</code> tag specified here as the URLs for the Kafka servers are obtained from ZooKeeper.</p>"},{"location":"service_livy/","title":"Livy","text":""},{"location":"service_livy/#livy-server","title":"Livy Server","text":"<p>Knox provides proxied access to Livy server for submitting Spark jobs. The gateway can be used to provide authentication and encryption for clients to servers like Livy.</p>"},{"location":"service_livy/#gateway-configuration","title":"Gateway configuration","text":"<p>The Gateway can be configured for Livy by modifying the topology XML file and providing a new service XML file.</p> <p>In the topology XML file, add the following with the correct hostname:</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;LIVYSERVER&lt;/role&gt;\n  &lt;url&gt;http://&lt;livy-server&gt;:8998&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>Livy server will use proxyUser to run the Spark session. To avoid that a user can  provide here any user (e.g. a more privileged), Knox will need to rewrite the  JSON body to replace what so ever is the value of proxyUser is with the username of the authenticated user.</p> <pre><code>{  \n  \"driverMemory\":\"2G\",\n  \"executorCores\":4,\n  \"executorMemory\":\"8G\",\n  \"proxyUser\":\"bernhard\",\n  \"conf\":{  \n    \"spark.master\":\"yarn-cluster\",\n    \"spark.jars.packages\":\"com.databricks:spark-csv_2.10:1.5.0\"\n  }\n}\n</code></pre> <p>The above is an example request body to be used to create a Spark session via Livy server and illustrates the \"proxyUser\" that requires rewrite.</p>"},{"location":"service_oozie/","title":"Oozie","text":""},{"location":"service_oozie/#oozie","title":"Oozie","text":"<p>Oozie is a Hadoop component that provides complex job workflows to be submitted and managed. Please refer to the latest Oozie documentation for details.</p> <p>In order to make Oozie accessible via the gateway there are several important Hadoop configuration settings. These all relate to the network endpoint exposed by various Hadoop services.</p> <p>The HTTP endpoint at which Oozie is running can be found via the <code>oozie.base.url property</code> in the <code>oozie-site.xml</code> file. In a Sandbox installation this can typically be found in <code>/etc/oozie/conf/oozie-site.xml</code>.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;oozie.base.url&lt;/name&gt;\n    &lt;value&gt;http://sandbox.hortonworks.com:11000/oozie&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The RPC address at which the Resource Manager exposes the JOBTRACKER endpoint can be found via the <code>yarn.resourcemanager.address</code> in the <code>yarn-site.xml</code> file. In a Sandbox installation this can typically be found in <code>/etc/hadoop/conf/yarn-site.xml</code>.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:8050&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The RPC address at which the Name Node exposes its RPC endpoint can be found via the <code>dfs.namenode.rpc-address</code> in the <code>hdfs-site.xml</code> file. In a Sandbox installation this can typically be found in <code>/etc/hadoop/conf/hdfs-site.xml</code>.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;dfs.namenode.rpc-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:8020&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>If HDFS has been configured to be in High Availability mode (HA), then instead of the RPC address mentioned above for the Name Node, look up and use the logical name of the service found via <code>dfs.nameservices</code> in <code>hdfs-site.xml</code>. For example,</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;dfs.nameservices&lt;/name&gt;\n    &lt;value&gt;ha-service&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Please note, only one of the URLs, either the RPC endpoint or the HA service name should be used as the NAMENODE HDFS URL in the gateway topology file.</p> <p>The information above must be provided to the gateway via a topology descriptor file. These topology descriptor files are placed in <code>{GATEWAY_HOME}/deployments</code>. An example that is setup for the default configuration of the Sandbox is <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>. These values will need to be changed for non-default Sandbox or other Hadoop cluster configuration.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;NAMENODE&lt;/role&gt;\n    &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;JOBTRACKER&lt;/role&gt;\n    &lt;url&gt;rpc://localhost:8050&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;OOZIE&lt;/role&gt;\n    &lt;url&gt;http://localhost:11000/oozie&lt;/url&gt;\n    &lt;param&gt;\n        &lt;name&gt;replayBufferSize&lt;/name&gt;\n        &lt;value&gt;8&lt;/value&gt;\n    &lt;/param&gt;\n&lt;/service&gt;\n</code></pre> <p>A default replayBufferSize of 8KB is shown in the sample topology file above.  This may need to be increased if your request size is larger.</p>"},{"location":"service_oozie/#oozie-url-mapping","title":"Oozie URL Mapping","text":"<p>For Oozie URLs, the mapping of Knox Gateway accessible URLs to direct Oozie URLs is simple.</p> <p>| ------- | --------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/oozie</code> | | Cluster | <code>http://{oozie-host}:{oozie-port}/oozie}</code>                                   |</p>"},{"location":"service_oozie/#oozie-request-changes","title":"Oozie Request Changes","text":"<p>TODO - In some cases the Oozie requests needs to be slightly different when made through the gateway. These changes are required in order to protect the client from knowing the internal structure of the Hadoop cluster.</p>"},{"location":"service_oozie/#oozie-example-via-client-dsl","title":"Oozie Example via Client DSL","text":"<p>This example will also submit the familiar WordCount Java MapReduce job to the Hadoop cluster via the gateway using the KnoxShell DSL. However in this case the job will be submitted via a Oozie workflow. There are several ways to do this depending upon your preference.</p> <p>You can use the \"embedded\" Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar samples/ExampleOozieWorkflow.groovy\n</code></pre> <p>You can manually type in the KnoxShell DSL script into the \"embedded\" Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>Each line from the file <code>samples/ExampleOozieWorkflow.groovy</code> will need to be typed or copied into the interactive shell.</p>"},{"location":"service_oozie/#oozie-example-via-curl","title":"Oozie Example via cURL","text":"<p>The example below illustrates the sequence of curl commands that could be used to run a \"word count\" map reduce job via an Oozie workflow.</p> <p>It utilizes the hadoop-examples.jar from a Hadoop install for running a simple word count job. A copy of that jar has been included in the samples directory for convenience.</p> <p>In addition a workflow definition and configuration file is required. These have not been included but are available for download. Download workflow-definition.xml and workflow-configuration.xml and store them in the <code>{GATEWAY_HOME}</code> directory. Review the contents of workflow-configuration.xml to ensure that it matches your environment.</p> <p>Take care to follow the instructions below where replacement values are required. These replacement values are identified with <code>{ }</code> markup.</p> <pre><code># 0. Optionally cleanup the test directory in case a previous example was run without cleaning up.\ncurl -i -k -u guest:guest-password -X DELETE \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example?op=DELETE&amp;recursive=true'\n\n# 1. Create the inode for workflow definition file in /user/guest/example\ncurl -i -k -u guest:guest-password -X PUT \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/workflow.xml?op=CREATE'\n\n# 2. Upload the workflow definition file.  This file can be found in {GATEWAY_HOME}/templates\ncurl -i -k -u guest:guest-password -T workflow-definition.xml -X PUT \\\n    '{Value Location header from command above}'\n\n# 3. Create the inode for hadoop-examples.jar in /user/guest/example/lib\ncurl -i -k -u guest:guest-password -X PUT \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/lib/hadoop-examples.jar?op=CREATE'\n\n# 4. Upload hadoop-examples.jar to /user/guest/example/lib.  Use a hadoop-examples.jar from a Hadoop install.\ncurl -i -k -u guest:guest-password -T samples/hadoop-examples.jar -X PUT \\\n    '{Value Location header from command above}'\n\n# 5. Create the inode for a sample input file readme.txt in /user/guest/example/input.\ncurl -i -k -u guest:guest-password -X PUT \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/input/README?op=CREATE'\n\n# 6. Upload readme.txt to /user/guest/example/input.  Use the readme.txt in {GATEWAY_HOME}.\n# The sample below uses this README file found in {GATEWAY_HOME}.\ncurl -i -k -u guest:guest-password -T README -X PUT \\\n    '{Value of Location header from command above}'\n\n# 7. Submit the job via Oozie\n# Take note of the Job ID in the JSON response as this will be used in the next step.\ncurl -i -k -u guest:guest-password -H Content-Type:application/xml -T workflow-configuration.xml \\\n    -X POST 'https://localhost:8443/gateway/sandbox/oozie/v1/jobs?action=start'\n\n# 8. Query the job status via Oozie.\ncurl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/oozie/v1/job/{Job ID from JSON body}'\n\n# 9. List the contents of the output directory /user/guest/example/output\ncurl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/output?op=LISTSTATUS'\n\n# 10. Optionally cleanup the test directory\ncurl -i -k -u guest:guest-password -X DELETE \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example?op=DELETE&amp;recursive=true'\n</code></pre>"},{"location":"service_oozie/#oozie-client-dsl","title":"Oozie Client DSL","text":""},{"location":"service_oozie/#submit-submit-a-workflow-job","title":"submit() - Submit a workflow job.","text":"<ul> <li>Request<ul> <li>text (String) - XML formatted workflow configuration string.</li> <li>file (String) - A filename containing XML formatted workflow configuration.</li> <li>action (String) - The initial action to take on the job.  Optional: Default is \"start\".</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>Workflow.submit(session).file(localFile).action(\"start\").now()</code></li> </ul> </li> </ul>"},{"location":"service_oozie/#status-query-the-status-of-a-workflow-job","title":"status() - Query the status of a workflow job.","text":"<ul> <li>Request<ul> <li>jobId (String) - The job ID to check. This is the ID received when the job was created.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>Workflow.status(session).jobId(jobId).now().string</code></li> </ul> </li> </ul>"},{"location":"service_oozie/#oozie-ha","title":"Oozie HA","text":"<p>Please look at #[Default Service HA support]</p>"},{"location":"service_service_test/","title":"Service Test","text":""},{"location":"service_service_test/#service-test-api","title":"Service Test API","text":"<p>The gateway supports a Service Test API that can be used to test Knox's ability to connect to each of the different Hadoop services via a simple HTTP GET request. To be able to access this API, one must add the following lines into the topology for which you wish to run the service test.</p> <pre><code>&lt;service&gt;\n  &lt;role&gt;SERVICE-TEST&lt;/role&gt;\n&lt;/service&gt;\n</code></pre> <p>After adding the above to a topology, you can make a cURL request with the following structure</p> <pre><code>curl -i -k \"https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/service-test?username=guest&amp;password=guest-password\"\n</code></pre> <p>An alternate method of providing credentials:</p> <pre><code>curl -i -k -u guest:guest-password https://{gateway-hostname}:{gateway-port}/gateway/{topology-name}/service-test\n</code></pre> <p>Below is an example response. The gateway is also capable of returning XML if specified in the request's \"Accept\" HTTP header.</p> <pre><code>{\n    \"serviceTestWrapper\": {\n     \"Tests\": {\n      \"ServiceTest\": [\n       {\n        \"serviceName\": \"WEBHDFS\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/webhdfs/v1/?op=LISTSTATUS\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHCAT\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/templeton/v1/status\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHCAT\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/templeton/v1/version\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHCAT\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/templeton/v1/version/hive\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHCAT\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/templeton/v1/version/hadoop\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"OOZIE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/oozie/v1/admin/build-version\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"OOZIE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/oozie/v1/admin/status\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"OOZIE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/oozie/versions\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHBASE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/hbase/version\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHBASE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/hbase/version/cluster\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHBASE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/hbase/status/cluster\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"WEBHBASE\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/hbase\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"RESOURCEMANAGER\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/resourcemanager/v1/{topology-name}/info\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"RESOURCEMANAGER\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/resourcemanager/v1/{topology-name}/metrics\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"RESOURCEMANAGER\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/resourcemanager/v1/{topology-name}/apps\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"FALCON\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/falcon/api/admin/stack\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"FALCON\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/falcon/api/admin/version\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"FALCON\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/falcon/api/metadata/lineage/serialize\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"FALCON\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/falcon/api/metadata/lineage/vertices/all\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"FALCON\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/falcon/api/metadata/lineage/edges/all\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"STORM\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/storm/api/v1/cluster/configuration\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"STORM\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/storm/api/v1/cluster/summary\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"STORM\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/storm/api/v1/supervisor/summary\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       },\n       {\n        \"serviceName\": \"STORM\",\n        \"requestURL\": \"http://{gateway-host}:{gateway-port}/gateway/{topology-name}/storm/api/v1/topology/summary\",\n        \"responseContent\": \"Content-Length:0,Content-Type: application/json;charset=utf-8\",\n        \"httpCode\": 200,\n        \"message\": \"Request sucessful.\"\n       }\n      ]\n     },\n     \"messages\": {\n      \"message\": [\n\n      ]\n     }\n    }\n}\n</code></pre> <p>We can see that this service-test makes HTTP requests to each of the services through Knox using the specified topology. The test will only make calls to those services that have entries within the topology file.</p>"},{"location":"service_service_test/#adding-and-changing-test-urls","title":"Adding and Changing test URLs","text":"<p>URLs for each service are stored in <code>{GATEWAY_HOME}/data/services/{service-name}/{service-version}/service.xml</code>. Each <code>&lt;testURL&gt;</code> element represents a service resource that will be tested if the service is set up in the topology. You can add or remove these from the <code>service.xml</code> file. Just note if you add URLs there is no guarantee in the order they will be tested. All default URLs have been tested and work on various clusters. If a new URL is added and doesn't respond in a way the user expects then it is up to the user to determine whether the URL is correct or not.</p>"},{"location":"service_service_test/#some-important-things-to-note","title":"Some important things to note:","text":"<ul> <li>In the first cURL request, the quotes are necessary around the URL or else a command line terminal will not include the <code>&amp;password</code> query parameter in the request.</li> <li>This API call does not require any credentials to receive a response from Knox, but expect to receive 401 responses from each of the services if none are provided.</li> </ul>"},{"location":"service_solr/","title":"Solr","text":""},{"location":"service_solr/#solr","title":"Solr","text":"<p>Knox provides gateway functionality to Solr with support for versions 5.5+ and 6+. The Solr REST APIs allow the user to view the status  of the collections, perform administrative actions and query collections.</p> <p>See the Solr Quickstart (http://lucene.apache.org/solr/quickstart.html) section of the Solr documentation for examples of the Solr REST API.</p> <p>Since Knox provides an abstraction over Solr and ZooKeeper, the use of the SolrJ CloudSolrClient is no longer supported.  You should replace  instances of CloudSolrClient with HttpSolrClient.</p> <p>Note: Updates to Solr via Knox require a POST operation require the use of preemptive authentication which is not directly supported by the  SolrJ API at this time.</p> <p>To enable this functionality, a topology file needs to have the following configuration:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;SOLR&lt;/role&gt;\n    &lt;version&gt;6.0.0&lt;/version&gt;\n    &lt;url&gt;http://&lt;solr-host&gt;:&lt;solr-port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default Solr port is 8983. Adjust the version specified to either '5.5.0 or '6.0.0'.</p> <p>For Solr 5.5.0 you also need to change the role name to <code>SOLRAPI</code> like this:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;SOLRAPI&lt;/role&gt;\n    &lt;version&gt;5.5.0&lt;/version&gt;\n    &lt;url&gt;http://&lt;solr-host&gt;:&lt;solr-port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_solr/#solr-url-mapping","title":"Solr URL Mapping","text":"<p>For Solr URLs, the mapping of Knox Gateway accessible URLs to direct Solr URLs is the following.</p> <p>| ------- | ------------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/solr</code> | | Cluster | <code>http://{solr-host}:{solr-port}/solr</code>                               |</p>"},{"location":"service_solr/#solr-examples-via-curl","title":"Solr Examples via cURL","text":"<p>Some of the various calls that can be made and examples using curl are listed below.</p> <pre><code># 0. Query collection\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/solr/select?q=*:*&amp;wt=json'\n\n# 1. Query cluster status\n\ncurl -ikv -u guest:guest-password -X POST 'https://localhost:8443/gateway/sandbox/solr/admin/collections?action=CLUSTERSTATUS'\n</code></pre>"},{"location":"service_solr/#solr-ha","title":"Solr HA","text":"<p>Knox provides basic failover functionality for calls made to Solr Cloud when more than one Solr instance is installed in the cluster and registered with the same ZooKeeper ensemble. The HA functionality in this case fetches the Solr URL information from a ZooKeeper ensemble, so the user need only supply the necessary ZooKeeper configuration and not the Solr connection URLs.</p> <p>To enable HA functionality for Solr Cloud in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;ha&lt;/role&gt;\n    &lt;name&gt;HaProvider&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;\n        &lt;name&gt;SOLR&lt;/name&gt;\n        &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true;zookeeperEnsemble=machine1:2181,machine2:2181,machine3:2181&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section must match that of the service role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. In this case the name is 'SOLR'.</p> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts - This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL will be tried again after the list is fetched again from ZooKeeper (a refresh of the list is done at this point)</p> </li> <li> <p>failoverSleep - The amount of time in millis that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled - Flag to turn the particular service on or off for HA.</p> </li> <li> <p>zookeeperEnsemble - A comma separated list of host names (or IP addresses) of the zookeeper hosts that consist of the ensemble that the Solr servers register their information with. </p> </li> </ul> <p>And for the service configuration itself the URLs need NOT be added to the list. For example.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;SOLR&lt;/role&gt;\n    &lt;version&gt;6.0.0&lt;/version&gt;\n&lt;/service&gt;\n</code></pre> <p>Please note that there is no <code>&lt;url&gt;</code> tag specified here as the URLs for the Solr servers are obtained from ZooKeeper.</p>"},{"location":"service_ssl_certificate_trust/","title":"SSL Certificate Trust","text":""},{"location":"service_ssl_certificate_trust/#tlsssl-certificate-trust","title":"TLS/SSL Certificate Trust","text":"<p>When the Gateway dispatches requests to a configured service using TLS/SSL, that service's certificate  must be trusted inorder for the connection to succeed.  To do this, the Gateway checks  a configured trust store for the service's certificate or the certificate of the CA that issued that  certificate. </p> <p>If not explicitly set, the Gateway will use its configured identity keystore as the trust store. By default, this keystore is located at <code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code>; however,  a custom identity keystore may be set in the gateway-site.xml file. See <code>gateway.tls.keystore.password.alias</code>, <code>gateway.tls.keystore.path</code>,  and <code>gateway.tls.keystore.type</code>. </p> <p>The trust store is configured at the Gatway-level.  There is no support to set a different trust store per service. To use a specific trust store, the following configuration elements may be set in the  gateway-site.xml file:</p> Configuration Element Description gateway.httpclient.truststore.path Fully qualified path to the trust store to use. Default is the keystore used to hold the Gateway's identity.  See <code>gateway.tls.keystore.path</code>. gateway.httpclient.truststore.type Keystore type of the trust store. Default is JKS. gateway.httpclient.truststore.password.alias Alias for the password to the trust store. <p>If <code>gateway.httpclient.truststore.path</code> is not set, the keystore used to hold the Gateway's identity  will be used as the trust store. </p> <p>However, if <code>gateway.httpclient.truststore.path</code> is set, it is expected that  <code>gateway.httpclient.truststore.type</code> and <code>gateway.httpclient.truststore.password.alias</code> are set appropriately. If <code>gateway.httpclient.truststore.type</code> is not set, the Gateway will assume the trust  store is a JKS file. If <code>gateway.httpclient.truststore.password.alias</code> is not set, the Gateway will assume the alias name is \"gateway-httpclient-truststore-password\".  In any case, if the  trust store password is different from the Gateway's master secret then it can be set using</p> <pre><code>knoxcli.sh create-alias {password-alias} --value {pwd}\n</code></pre> <p>If a password is not found using the provided (or default) alias name, then the Gateway's master secret  will be used.</p> <p>All topologies deployed within the Gateway instance will use the configured trust store to verify a  service's identity.  </p>"},{"location":"service_ssl_certificate_trust/#tlsssl-certificate-trust_1","title":"TLS/SSL Certificate Trust","text":"<p>When the Gateway dispatches requests to a configured service using TLS/SSL, that service's certificate  must be trusted inorder for the connection to succeed.  To do this, the Gateway checks  a configured trust store for the service's certificate or the certificate of the CA that issued that  certificate. </p> <p>If not explicitly set, the Gateway will use its configured identity keystore as the trust store. By default, this keystore is located at <code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code>; however,  a custom identity keystore may be set in the gateway-site.xml file. See <code>gateway.tls.keystore.password.alias</code>, <code>gateway.tls.keystore.path</code>,  and <code>gateway.tls.keystore.type</code>. </p> <p>The trust store is configured at the Gatway-level.  There is no support to set a different trust store per service. To use a specific trust store, the following configuration elements may be set in the  gateway-site.xml file:</p> Configuration Element Description gateway.httpclient.truststore.path Fully qualified path to the trust store to use. Default is the keystore used to hold the Gateway's identity.  See <code>gateway.tls.keystore.path</code>. gateway.httpclient.truststore.type Keystore type of the trust store. Default is JKS. gateway.httpclient.truststore.password.alias Alias for the password to the trust store. <p>If <code>gateway.httpclient.truststore.path</code> is not set, the keystore used to hold the Gateway's identity  will be used as the trust store. </p> <p>However, if <code>gateway.httpclient.truststore.path</code> is set, it is expected that  <code>gateway.httpclient.truststore.type</code> and <code>gateway.httpclient.truststore.password.alias</code> are set appropriately. If <code>gateway.httpclient.truststore.type</code> is not set, the Gateway will assume the trust  store is a JKS file. If <code>gateway.httpclient.truststore.password.alias</code> is not set, the Gateway will assume the alias name is \"gateway-httpclient-truststore-password\".  In any case, if the  trust store password is different from the Gateway's master secret then it can be set using</p> <pre><code>knoxcli.sh create-alias {password-alias} --value {pwd}\n</code></pre> <p>If a password is not found using the provided (or default) alias name, then the Gateway's master secret  will be used.</p> <p>All topologies deployed within the Gateway instance will use the configured trust store to verify a  service's identity.  </p>"},{"location":"service_ssl_certificate_trust/#tlsssl-certificate-trust_2","title":"TLS/SSL Certificate Trust","text":"<p>When the Gateway dispatches requests to a configured service using TLS/SSL, that service's certificate  must be trusted inorder for the connection to succeed.  To do this, the Gateway checks  a configured trust store for the service's certificate or the certificate of the CA that issued that  certificate. </p> <p>If not explicitly set, the Gateway will use its configured identity keystore as the trust store. By default, this keystore is located at <code>{GATEWAY_HOME}/data/security/keystores/gateway.jks</code>; however,  a custom identity keystore may be set in the gateway-site.xml file. See <code>gateway.tls.keystore.password.alias</code>, <code>gateway.tls.keystore.path</code>,  and <code>gateway.tls.keystore.type</code>. </p> <p>The trust store is configured at the Gatway-level.  There is no support to set a different trust store per service. To use a specific trust store, the following configuration elements may be set in the  gateway-site.xml file:</p> Configuration Element Description gateway.httpclient.truststore.path Fully qualified path to the trust store to use. Default is the keystore used to hold the Gateway's identity.  See <code>gateway.tls.keystore.path</code>. gateway.httpclient.truststore.type Keystore type of the trust store. Default is JKS. gateway.httpclient.truststore.password.alias Alias for the password to the trust store. <p>If <code>gateway.httpclient.truststore.path</code> is not set, the keystore used to hold the Gateway's identity  will be used as the trust store. </p> <p>However, if <code>gateway.httpclient.truststore.path</code> is set, it is expected that  <code>gateway.httpclient.truststore.type</code> and <code>gateway.httpclient.truststore.password.alias</code> are set appropriately. If <code>gateway.httpclient.truststore.type</code> is not set, the Gateway will assume the trust  store is a JKS file. If <code>gateway.httpclient.truststore.password.alias</code> is not set, the Gateway will assume the alias name is \"gateway-httpclient-truststore-password\".  In any case, if the  trust store password is different from the Gateway's master secret then it can be set using</p> <pre><code>knoxcli.sh create-alias {password-alias} --value {pwd}\n</code></pre> <p>If a password is not found using the provided (or default) alias name, then the Gateway's master secret  will be used.</p> <p>All topologies deployed within the Gateway instance will use the configured trust store to verify a  service's identity.  </p>"},{"location":"service_storm/","title":"Storm","text":""},{"location":"service_storm/#storm","title":"Storm","text":"<p>Storm is a distributed realtime computation system. Storm exposes REST APIs for UI functionality that can be used for retrieving metrics data and configuration information as well as management operations such as starting or stopping topologies.</p> <p>The docs for this can be found here</p> <p>https://github.com/apache/storm/blob/master/docs/STORM-UI-REST-API.md</p> <p>To enable this functionality, a topology file needs to have the following configuration:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;STORM&lt;/role&gt;\n    &lt;url&gt;http://&lt;hostname&gt;:&lt;port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default UI daemon port is 8744. If it is configured to some other port, that configuration can be found in <code>storm.yaml</code> as the value for the property <code>ui.port</code>.</p> <p>In addition to the storm service configuration above, a STORM-LOGVIEWER service must be configured if the log files are to be retrieved through Knox. The value of the port for the logviewer can be found by the property <code>logviewer.port</code> also in the file <code>storm.yaml</code>.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;STORM-LOGVIEWER&lt;/role&gt;\n    &lt;url&gt;http://&lt;hostname&gt;:&lt;port&gt;&lt;/url&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_storm/#storm-url-mapping","title":"Storm URL Mapping","text":"<p>For Storm URLs, the mapping of Knox Gateway accessible URLs to direct Storm URLs is the following.</p> <p>| ------- | ------------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/storm</code> | | Cluster | <code>http://{storm-host}:{storm-port}</code>                                      |</p> <p>For the log viewer the mapping is as follows</p> <p>| ------- | ------------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/storm/logviewer</code> | | Cluster | <code>http://{storm-logviewer-host}:{storm-logviewer-port}</code>                                      |</p>"},{"location":"service_storm/#storm-examples","title":"Storm Examples","text":"<p>Some of the various calls that can be made and examples using curl are listed below.</p> <pre><code># 0. Getting cluster configuration\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/cluster/configuration'\n\n# 1. Getting cluster summary information\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/cluster/summary'\n\n# 2. Getting supervisor summary information\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/supervisor/summary'\n\n# 3. topologies summary information\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/topology/summary'\n\n# 4. Getting specific topology information. Substitute {id} with the topology id.\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/topology/{id}'\n\n# 5. To get component level information. Substitute {id} with the topology id and {component} with the component id e.g. 'spout'\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/storm/api/v1/topology/{id}/component/{component}'\n</code></pre> <p>The following POST operations all require a 'x-csrf-token' header along with other information that can be stored in a cookie file. In particular the 'ring-session' header and 'JSESSIONID'.</p> <pre><code># 6. To activate a topology. Substitute {id} with the topology id and {token-value} with the x-csrf-token value.\n\ncurl -ik -b ~/cookiejar.txt -c ~/cookiejar.txt -u guest:guest-password -H 'x-csrf-token:{token-value}' -X POST \\\n http://localhost:8744/api/v1/topology/{id}/activate\n\n# 7. To de-activate a topology. Substitute {id} with the topology id and {token-value} with the x-csrf-token value.\n\ncurl -ik -b ~/cookiejar.txt -c ~/cookiejar.txt -u guest:guest-password -H 'x-csrf-token:{token-value}' -X POST \\\n http://localhost:8744/api/v1/topology/{id}/deactivate\n\n# 8. To rebalance a topology. Substitute {id} with the topology id and {token-value} with the x-csrf-token value.\n\ncurl -ik -b ~/cookiejar.txt -c ~/cookiejar.txt -u guest:guest-password -H 'x-csrf-token:{token-value}' -X POST \\\n http://localhost:8744/api/v1/topology/{id}/rebalance/0\n\n# 9. To kill a topology. Substitute {id} with the topology id and {token-value} with the x-csrf-token value.\n\ncurl -ik -b ~/cookiejar.txt -c ~/cookiejar.txt -u guest:guest-password -H 'x-csrf-token:{token-value}' -X POST \\\n http://localhost:8744/api/v1/topology/{id}/kill/0\n</code></pre>"},{"location":"service_webhcat/","title":"WebHCat","text":""},{"location":"service_webhcat/#webhcat","title":"WebHCat","text":"<p>WebHCat (also called Templeton) is a related but separate service from HiveServer2. As such it is installed and configured independently. The WebHCat wiki pages describe this processes. In sandbox this configuration file for WebHCat is located at <code>/etc/hadoop/hcatalog/webhcat-site.xml</code>. Note the properties shown below as they are related to configuration required by the gateway.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;templeton.port&lt;/name&gt;\n    &lt;value&gt;50111&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>Also important is the configuration of the JOBTRACKER RPC endpoint. For Hadoop 2 this can be found in the <code>yarn-site.xml</code> file. In Sandbox this file can be found at <code>/etc/hadoop/conf/yarn-site.xml</code>. The property <code>yarn.resourcemanager.address</code> within that file is relevant for the gateway's configuration.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:8050&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>See #[WebHDFS] for details about locating the Hadoop configuration for the NAMENODE endpoint.</p> <p>The gateway by default includes a sample topology descriptor file <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>. The values in this sample are configured to work with an installed Sandbox VM.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;NAMENODE&lt;/role&gt;\n    &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;JOBTRACKER&lt;/role&gt;\n    &lt;url&gt;rpc://localhost:8050&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;WEBHCAT&lt;/role&gt;\n    &lt;url&gt;http://localhost:50111/templeton&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The URLs provided for the role NAMENODE and JOBTRACKER do not result in an endpoint being exposed by the gateway. This information is only required so that other URLs can be rewritten that reference the appropriate RPC address for Hadoop services. This prevents clients from needing to be aware of the internal cluster details. Note that for Hadoop 2 the JOBTRACKER RPC endpoint is provided by the Resource Manager component.</p> <p>By default the gateway is configured to use the HTTP endpoint for WebHCat in the Sandbox. This could alternatively be configured to use the HTTPS endpoint by providing the correct address.</p>"},{"location":"service_webhcat/#webhcat-url-mapping","title":"WebHCat URL Mapping","text":"<p>For WebHCat URLs, the mapping of Knox Gateway accessible URLs to direct WebHCat URLs is simple.</p> <p>| ------- | ------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/templeton</code> | | Cluster | <code>http://{webhcat-host}:{webhcat-port}/templeton}</code>                               |</p>"},{"location":"service_webhcat/#webhcat-via-curl","title":"WebHCat via cURL","text":"<p>Users can use cURL to directly invoke the REST APIs via the gateway. For the full list of available REST calls look at the WebHCat documentation. This is a simple curl command to test the connection:</p> <pre><code>curl -i -k -u guest:guest-password 'https://localhost:8443/gateway/sandbox/templeton/v1/status'\n</code></pre>"},{"location":"service_webhcat/#webhcat-example","title":"WebHCat Example","text":"<p>This example will submit the familiar WordCount Java MapReduce job to the Hadoop cluster via the gateway using the KnoxShell DSL. There are several ways to do this depending upon your preference.</p> <p>You can use the \"embedded\" Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar samples/ExampleWebHCatJob.groovy\n</code></pre> <p>You can manually type in the KnoxShell DSL script into the \"embedded\" Groovy interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>Each line from the file <code>samples/ExampleWebHCatJob.groovy</code> would then need to be typed or copied into the interactive shell.</p>"},{"location":"service_webhcat/#webhcat-client-dsl","title":"WebHCat Client DSL","text":""},{"location":"service_webhcat/#submitjava-submit-a-java-mapreduce-job","title":"submitJava() - Submit a Java MapReduce job.","text":"<ul> <li>Request<ul> <li>jar (String) - The remote file name of the JAR containing the app to execute.</li> <li>app (String) - The app name to execute. This is wordcount for example not the class name.</li> <li>input (String) - The remote directory name to use as input for the job.</li> <li>output (String) - The remote directory name to store output from the job.</li> </ul> </li> <li>Response<ul> <li>jobId : String - The job ID of the submitted job.  Consumes body.</li> </ul> </li> <li> <p>Example</p> <p>Job.submitJava(session)     .jar(remoteJarName)     .app(appName)     .input(remoteInputDir)     .output(remoteOutputDir)     .now()     .jobId</p> </li> </ul>"},{"location":"service_webhcat/#submitpig-submit-a-pig-job","title":"submitPig() - Submit a Pig job.","text":"<ul> <li>Request<ul> <li>file (String) - The remote file name of the pig script.</li> <li>arg (String) - An argument to pass to the script.</li> <li>statusDir (String) - The remote directory to store status output.</li> </ul> </li> <li>Response<ul> <li>jobId : String - The job ID of the submitted job.  Consumes body.</li> </ul> </li> <li>Example<ul> <li><code>Job.submitPig(session).file(remotePigFileName).arg(\"-v\").statusDir(remoteStatusDir).now()</code></li> </ul> </li> </ul>"},{"location":"service_webhcat/#submithive-submit-a-hive-job","title":"submitHive() - Submit a Hive job.","text":"<ul> <li>Request<ul> <li>file (String) - The remote file name of the hive script.</li> <li>arg (String) - An argument to pass to the script.</li> <li>statusDir (String) - The remote directory to store status output.</li> </ul> </li> <li>Response<ul> <li>jobId : String - The job ID of the submitted job.  Consumes body.</li> </ul> </li> <li>Example<ul> <li><code>Job.submitHive(session).file(remoteHiveFileName).arg(\"-v\").statusDir(remoteStatusDir).now()</code></li> </ul> </li> </ul>"},{"location":"service_webhcat/#submitsqoop-job-api","title":"submitSqoop Job API","text":"<p>Using the Knox DSL, you can now easily submit and monitor Apache Sqoop jobs. The WebHCat Job class now supports the <code>submitSqoop</code> command.</p> <pre><code>Job.submitSqoop(session)\n    .command(\"import --connect jdbc:mysql://hostname:3306/dbname ... \")\n    .statusDir(remoteStatusDir)\n    .now().jobId\n</code></pre> <p>The <code>submitSqoop</code> command supports the following arguments:</p> <ul> <li>command (String) - The sqoop command string to execute.</li> <li>files (String) - Comma separated files to be copied to the templeton controller job.</li> <li>optionsfile (String) - The remote file which contain Sqoop command need to run.</li> <li>libdir (String) - The remote directory containing jdbc jar to include with sqoop lib</li> <li>statusDir (String) - The remote directory to store status output.</li> </ul> <p>A complete example is available here: https://cwiki.apache.org/confluence/display/KNOX/2016/11/08/Running+SQOOP+job+via+KNOX+Shell+DSL</p>"},{"location":"service_webhcat/#queryqueue-return-a-list-of-all-job-ids-registered-to-the-user","title":"queryQueue() - Return a list of all job IDs registered to the user.","text":"<ul> <li>Request<ul> <li>No request parameters.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>Job.queryQueue(session).now().string</code></li> </ul> </li> </ul>"},{"location":"service_webhcat/#querystatus-check-the-status-of-a-job-and-get-related-job-information-given-its-job-id","title":"queryStatus() - Check the status of a job and get related job information given its job ID.","text":"<ul> <li>Request<ul> <li>jobId (String) - The job ID to check. This is the ID received when the job was created.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>Job.queryStatus(session).jobId(jobId).now().string</code></li> </ul> </li> </ul>"},{"location":"service_webhcat/#webhcat-ha","title":"WebHCat HA","text":"<p>Please look at #[Default Service HA support]</p>"},{"location":"service_webhdfs/","title":"WebHDFS","text":""},{"location":"service_webhdfs/#webhdfs","title":"WebHDFS","text":"<p>REST API access to HDFS in a Hadoop cluster is provided by WebHDFS or HttpFS. Both services provide the same API. The WebHDFS REST API documentation is available online. WebHDFS must be enabled in the <code>hdfs-site.xml</code> configuration file and exposes the API on each NameNode and DataNode. HttpFS however is a separate server to be configured and started separately. In the sandbox this configuration file is located at <code>/etc/hadoop/conf/hdfs-site.xml</code>. Note the properties shown below as they are related to configuration required by the gateway. Some of these represent the default values and may not actually be present in <code>hdfs-site.xml</code>.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;dfs.namenode.rpc-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:8020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:50070&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;dfs.https.namenode.https-address&lt;/name&gt;\n    &lt;value&gt;sandbox.hortonworks.com:50470&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>The values above need to be reflected in each topology descriptor file deployed to the gateway. The gateway by default includes a sample topology descriptor file <code>{GATEWAY_HOME}/deployments/sandbox.xml</code>. The values in this sample are configured to work with an installed Sandbox VM.</p> <p>Please also note that the port changed from 50070 to 9870 in Hadoop 3.0.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;NAMENODE&lt;/role&gt;\n    &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n&lt;/service&gt;\n&lt;service&gt;\n    &lt;role&gt;WEBHDFS&lt;/role&gt;\n    &lt;url&gt;http://localhost:50070/webhdfs&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The URL provided for the role NAMENODE does not result in an endpoint being exposed by the gateway. This information is only required so that other URLs can be rewritten that reference the Name Node's RPC address. This prevents clients from needing to be aware of the internal cluster details.</p> <p>By default the gateway is configured to use the HTTP endpoint for WebHDFS in the Sandbox. This could alternatively be configured to use the HTTPS endpoint by providing the correct address.</p>"},{"location":"service_webhdfs/#hdfs-namenode-federation","title":"HDFS NameNode Federation","text":"<p>NameNode federation introduces some additional complexity when determining to which URL(s) Knox should proxy HDFS-related requests.</p> <p>The HDFS core-site.xml configuration includes additional properties, which represent options in terms of the NameNode endpoints.</p> Property Name Description Example Value dfs.internal.nameservices The list of defined namespaces ns1,ns2 <p>For each value enumerated by dfs.internal.nameservices, there is another property defined, for specifying the associated NameNode names.</p> Property Name Description Example Value dfs.ha.namenodes.ns1 The NameNode identifiers associated with the ns1 namespace nn1,nn2 dfs.ha.namenodes.ns2 The NameNode identifiers associated with the ns2 namespace nn3,nn4 <p>For each namenode name enumerated by each of these properties, there are other properties defined, for specifying the associated host addresses.</p> Property Name Description Example Value dfs.namenode.http-address.ns1.nn1 The HTTP host address of nn1 NameNode in the ns1 namespace host1:50070 dfs.namenode.https-address.ns1.nn1 The HTTPS host address of nn1 NameNode in the ns1 namespace host1:50470 dfs.namenode.http-address.ns1.nn2 The HTTP host address of nn2 NameNode in the ns1 namespace host2:50070 dfs.namenode.https-address.ns1.nn2 The HTTPS host address of nn2 NameNode in the ns1 namespace host2:50470 dfs.namenode.http-address.ns2.nn3 The HTTP host address of nn3 NameNode in the ns2 namespace host3:50070 dfs.namenode.https-address.ns2.nn3 The HTTPS host address of nn3 NameNode in the ns2 namespace host3:50470 dfs.namenode.http-address.ns2.nn4 The HTTP host address of nn4 NameNode in the ns2 namespace host4:50070 dfs.namenode.https-address.ns2.nn4 The HTTPS host address of nn4 NameNode in the ns2 namespace host4:50470 <p>So, if Knox should proxy the NameNodes associated with ns1, and the configuration does not dictate HTTPS, then the WEBHDFS service must contain URLs based on the values of dfs.namenode.http-address.ns1.nn1 and dfs.namenode.http-address.ns1.nn2. Likewise, if Knox should proxy the NameNodes associated with ns2, the WEBHDFS service must contain URLs based on the values of dfs.namenode.http-address.ns2.nn3 and dfs.namenode.http-address.ns2.nn3.</p> <p>Fortunately, for Ambari-managed clusters, descriptors and service discovery can handle this complexity for administrators. In the descriptor, the service can be declared without any endpoints, and the desired namespace can be specified to disambiguate which endpoint(s) should be proxied by way of a parameter named discovery-namespace.</p> <pre><code>\"services\": [\n  {\n    \"name\": \"WEBHDFS\",\n    \"params\": {\n      \"discovery-nameservice\": \"ns2\"\n    }\n  },\n</code></pre> <p>If no namespace is specified, then the default namespace will be applied. This default namespace is derived from the value of the property named fs.defaultFS defined in the HDFS core-site.xml configuration.</p> <p></p>"},{"location":"service_webhdfs/#webhdfs-url-mapping","title":"WebHDFS URL Mapping","text":"<p>For Name Node URLs, the mapping of Knox Gateway accessible WebHDFS URLs to direct WebHDFS URLs is simple.</p> Type URL Gateway <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/webhdfs</code> Cluster <code>http://{webhdfs-host}:50070/webhdfs</code> <p>However, there is a subtle difference to URLs that are returned by WebHDFS in the Location header of many requests. Direct WebHDFS requests may return Location headers that contain the address of a particular DataNode. The gateway will rewrite these URLs to ensure subsequent requests come back through the gateway and internal cluster details are protected.</p> <p>A WebHDFS request to the NameNode to retrieve a file will return a URL of the form below in the Location header.</p> <pre><code>http://{datanode-host}:{data-node-port}/webhdfs/v1/{path}?...\n</code></pre> <p>Note that this URL contains the network location of a DataNode. The gateway will rewrite this URL to look like the URL below.</p> <pre><code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/webhdfs/data/v1/{path}?_={encrypted-query-parameters}\n</code></pre> <p>The <code>{encrypted-query-parameters}</code> will contain the <code>{datanode-host}</code> and <code>{datanode-port}</code> information. This information along with the original query parameters are encrypted so that the internal Hadoop details are protected.</p>"},{"location":"service_webhdfs/#webhdfs-examples","title":"WebHDFS Examples","text":"<p>The examples below upload a file, download the file and list the contents of the directory.</p>"},{"location":"service_webhdfs/#webhdfs-via-client-dsl","title":"WebHDFS via client DSL","text":"<p>You can use the Groovy example scripts and interpreter provided with the distribution.</p> <pre><code>java -jar bin/shell.jar samples/ExampleWebHdfsPutGet.groovy\njava -jar bin/shell.jar samples/ExampleWebHdfsLs.groovy\n</code></pre> <p>You can manually type the client DSL script into the KnoxShell interactive Groovy interpreter provided with the distribution. The command below starts the KnoxShell in interactive mode.</p> <pre><code>java -jar bin/shell.jar\n</code></pre> <p>Each line below could be typed or copied into the interactive shell and executed. This is provided as an example to illustrate the use of the client DSL.</p> <pre><code>// Import the client DSL and a useful utilities for working with JSON.\nimport org.apache.knox.gateway.shell.Hadoop\nimport org.apache.knox.gateway.shell.hdfs.Hdfs\nimport groovy.json.JsonSlurper\n\n// Setup some basic config.\ngateway = \"https://localhost:8443/gateway/sandbox\"\nusername = \"guest\"\npassword = \"guest-password\"\n\n// Start the session.\nsession = Hadoop.login( gateway, username, password )\n\n// Cleanup anything leftover from a previous run.\nHdfs.rm( session ).file( \"/user/guest/example\" ).recursive().now()\n\n// Upload the README to HDFS.\nHdfs.put( session ).file( \"README\" ).to( \"/user/guest/example/README\" ).now()\n\n// Download the README from HDFS.\ntext = Hdfs.get( session ).from( \"/user/guest/example/README\" ).now().string\nprintln text\n\n// List the contents of the directory.\ntext = Hdfs.ls( session ).dir( \"/user/guest/example\" ).now().string\njson = (new JsonSlurper()).parseText( text )\nprintln json.FileStatuses.FileStatus.pathSuffix\n\n// Cleanup the directory.\nHdfs.rm( session ).file( \"/user/guest/example\" ).recursive().now()\n\n// Clean the session.\nsession.shutdown()\n</code></pre>"},{"location":"service_webhdfs/#webhdfs-via-curl","title":"WebHDFS via cURL","text":"<p>Users can use cURL to directly invoke the REST APIs via the gateway.</p>"},{"location":"service_webhdfs/#optionally-cleanup-the-sample-directory-in-case-a-previous-example-was-run-without-cleaning-up","title":"Optionally cleanup the sample directory in case a previous example was run without cleaning up.","text":"<pre><code>curl -i -k -u guest:guest-password -X DELETE \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example?op=DELETE&amp;recursive=true'\n</code></pre>"},{"location":"service_webhdfs/#register-the-name-for-a-sample-file-readme-in-userguestexample","title":"Register the name for a sample file README in /user/guest/example.","text":"<pre><code>curl -i -k -u guest:guest-password -X PUT \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/README?op=CREATE'\n</code></pre>"},{"location":"service_webhdfs/#upload-readme-to-userguestexample-use-the-readme-in-gateway_home","title":"Upload README to /user/guest/example.  Use the README in {GATEWAY_HOME}.","text":"<pre><code>curl -i -k -u guest:guest-password -T README -X PUT \\\n    '{Value of Location header from command above}'\n</code></pre>"},{"location":"service_webhdfs/#list-the-contents-of-the-directory-userguestexample","title":"List the contents of the directory /user/guest/example.","text":"<pre><code>curl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example?op=LISTSTATUS'\n</code></pre>"},{"location":"service_webhdfs/#request-the-content-of-the-readme-file-in-userguestexample","title":"Request the content of the README file in /user/guest/example.","text":"<pre><code>curl -i -k -u guest:guest-password -X GET \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example/README?op=OPEN'\n</code></pre>"},{"location":"service_webhdfs/#read-the-content-of-the-file","title":"Read the content of the file.","text":"<pre><code>curl -i -k -u guest:guest-password -X GET \\\n    '{Value of Location header from command above}'\n</code></pre>"},{"location":"service_webhdfs/#optionally-cleanup-the-example-directory","title":"Optionally cleanup the example directory.","text":"<pre><code>curl -i -k -u guest:guest-password -X DELETE \\\n    'https://localhost:8443/gateway/sandbox/webhdfs/v1/user/guest/example?op=DELETE&amp;recursive=true'\n</code></pre>"},{"location":"service_webhdfs/#webhdfs-client-dsl","title":"WebHDFS client DSL","text":""},{"location":"service_webhdfs/#get-get-a-file-from-hdfs-open","title":"get() - Get a file from HDFS (OPEN).","text":"<ul> <li>Request<ul> <li>from( String name ) - The full name of the file in HDFS.</li> <li>file( String name ) - The name of a local file to create with the content. If this isn't specified the file content must be read from the response.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> <li>If file parameter specified content will be streamed to file.</li> </ul> </li> <li>Example<ul> <li><code>Hdfs.get( session ).from( \"/user/guest/example/README\" ).now().string</code></li> </ul> </li> </ul>"},{"location":"service_webhdfs/#ls-query-the-contents-of-a-directory-liststatus","title":"ls() - Query the contents of a directory (LISTSTATUS)","text":"<ul> <li>Request<ul> <li>dir( String name ) - The full name of the directory in HDFS.</li> </ul> </li> <li>Response<ul> <li>BasicResponse</li> </ul> </li> <li>Example<ul> <li><code>Hdfs.ls( session ).dir( \"/user/guest/example\" ).now().string</code></li> </ul> </li> </ul>"},{"location":"service_webhdfs/#mkdir-create-a-directory-in-hdfs-mkdirs","title":"mkdir() - Create a directory in HDFS (MKDIRS)","text":"<ul> <li>Request<ul> <li>dir( String name ) - The full name of the directory to create in HDFS.</li> <li>perm( String perm ) - The permissions for the directory (e.g. 644).  Optional: default=\"777\"</li> </ul> </li> <li>Response<ul> <li>EmptyResponse - Implicit close().</li> </ul> </li> <li>Example<ul> <li><code>Hdfs.mkdir( session ).dir( \"/user/guest/example\" ).now()</code></li> </ul> </li> </ul>"},{"location":"service_webhdfs/#put-write-a-file-into-hdfs-create","title":"put() - Write a file into HDFS (CREATE)","text":"<ul> <li>Request<ul> <li>text( String text ) - Text to upload to HDFS.  Takes precedence over file if both present.</li> <li>file( String name ) - The name of a local file to upload to HDFS.</li> <li>to( String name ) - The fully qualified name to create in HDFS.</li> </ul> </li> <li>Response<ul> <li>EmptyResponse - Implicit close().</li> </ul> </li> <li>Example<ul> <li><code>Hdfs.put( session ).file( README ).to( \"/user/guest/example/README\" ).now()</code></li> </ul> </li> </ul>"},{"location":"service_webhdfs/#rm-delete-a-file-or-directory-delete","title":"rm() - Delete a file or directory (DELETE)","text":"<ul> <li>Request<ul> <li>file( String name ) - The fully qualified file or directory name in HDFS.</li> <li>recursive( Boolean recursive ) - Delete directory and all of its contents if True.  Optional: default=False</li> </ul> </li> <li>Response<ul> <li>BasicResponse - Implicit close().</li> </ul> </li> <li>Example<ul> <li><code>Hdfs.rm( session ).file( \"/user/guest/example\" ).recursive().now()</code></li> </ul> </li> </ul>"},{"location":"service_webhdfs/#webhdfs-ha","title":"WebHDFS HA","text":"<p>Knox provides basic failover functionality for REST API calls made to WebHDFS when HDFS HA has been  configured and enabled.</p> <p>To enable HA functionality for WebHDFS in Knox the following configuration has to be added to the topology file.</p> <pre><code>&lt;provider&gt;\n   &lt;role&gt;ha&lt;/role&gt;\n   &lt;name&gt;HaProvider&lt;/name&gt;\n   &lt;enabled&gt;true&lt;/enabled&gt;\n   &lt;param&gt;\n       &lt;name&gt;WEBHDFS&lt;/name&gt;\n       &lt;value&gt;maxFailoverAttempts=3;failoverSleep=1000;enabled=true&lt;/value&gt;\n   &lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>The role and name of the provider above must be as shown. The name in the 'param' section must match that of the service  role name that is being configured for HA and the value in the 'param' section is the configuration for that particular service in HA mode. In this case the name is 'WEBHDFS'.</p> <p>The various configuration parameters are described below:</p> <ul> <li> <p>maxFailoverAttempts -  This is the maximum number of times a failover will be attempted. The failover strategy at this time is very simplistic in that the next URL in the list of URLs provided for the service is used and the one that failed is put at the bottom  of the list. If the list is exhausted and the maximum number of attempts is not reached then the first URL that failed  will be tried again (the list will start again from the original top entry).</p> </li> <li> <p>failoverSleep -  The amount of time in milliseconds that the process will wait or sleep before attempting to failover.</p> </li> <li> <p>enabled -  Flag to turn the particular service on or off for HA.</p> </li> </ul> <p>And for the service configuration itself the additional URLs should be added to the list. The active  URL (at the time of configuration) should ideally be added to the top of the list.</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;WEBHDFS&lt;/role&gt;\n    &lt;url&gt;http://{host1}:50070/webhdfs&lt;/url&gt;\n    &lt;url&gt;http://{host2}:50070/webhdfs&lt;/url&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"service_yarn/","title":"YARN","text":""},{"location":"service_yarn/#yarn","title":"Yarn","text":"<p>Knox provides gateway functionality for the REST APIs of the ResourceManager. The ResourceManager REST APIs allow the user to get information about the cluster - status on the cluster, metrics on the cluster, scheduler information, information about nodes in the cluster, and information about applications on the cluster. Also as of Hadoop version 2.5.0, the user can submit a new application as well as kill it (or get state) using the 'Writable' APIs.</p> <p>The docs for this can be found here</p> <p>http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html</p> <p>To enable this functionality, a topology file needs to have the following configuration:</p> <pre><code>&lt;service&gt;\n    &lt;role&gt;RESOURCEMANAGER&lt;/role&gt;\n    &lt;url&gt;http://&lt;hostname&gt;:&lt;port&gt;/ws&lt;/url&gt;\n&lt;/service&gt;\n</code></pre> <p>The default resource manager http port is 8088. If it is configured to some other port, that configuration can be found in <code>yarn-site.xml</code> under the property <code>yarn.resourcemanager.webapp.address</code>.</p>"},{"location":"service_yarn/#yarn-url-mapping","title":"Yarn URL Mapping","text":"<p>For Yarn URLs, the mapping of Knox Gateway accessible URLs to direct Yarn URLs is the following.</p> <p>| ------- | ------------------------------------------------------------------------------------- | | Gateway | <code>https://{gateway-host}:{gateway-port}/{gateway-path}/{cluster-name}/resourcemanager</code> | | Cluster | <code>http://{yarn-host}:{yarn-port}/ws}</code>                                      |</p>"},{"location":"service_yarn/#yarn-examples-via-curl","title":"Yarn Examples via cURL","text":"<p>Some of the various calls that can be made and examples using curl are listed below.</p> <pre><code># 0. Getting cluster info\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster'\n\n# 1. Getting cluster metrics\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/metrics'\n\nTo get the same information in an xml format\n\ncurl -ikv -u guest:guest-password -H Accept:application/xml -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/metrics'\n\n# 2. Getting scheduler information\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/scheduler'\n\n# 3. Getting all the applications listed and their information\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps'\n\n# 4. Getting applications statistics\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/appstatistics'\n\nAlso query params can be used as below to filter the results\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/appstatistics?states=accepted,running,finished&amp;applicationTypes=mapreduce'\n\n# 5. To get a specific application (please note, replace the application id with a real value)\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/{application_id}'\n\n# 6. To get the attempts made for a particular application\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/{application_id}/appattempts'\n\n# 7. To get information about the various nodes\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/nodes'\n\nAlso to get a specific node, use an id obtained in the response from above (the node id is scrambled) and issue the following\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/nodes/{node_id}'\n\n# 8. To create a new Application\n\ncurl -ikv -u guest:guest-password -X POST 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/new-application'\n\nAn application id is returned from the request above and this can be used to submit an application.\n\n# 9. To submit an application, put together a request containing the application id received in the above response (please refer to Yarn REST\nAPI documentation).\n\ncurl -ikv -u guest:guest-password -T request.json -H Content-Type:application/json -X POST 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps'\n\nHere the request is saved in a file called request.json\n\n#10. To get application state\n\ncurl -ikv -u guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/{application_id}/state'\n\ncurl -ikv -u guest:guest-password -H Content-Type:application/json -X PUT -T state-killed.json 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/application_1409008107556_0007/state'\n\n# 11. To kill an application that is running issue the below command with the application id of the application that is to be killed.\nThe contents of the state-killed.json file are :\n\n{\n  \"state\":\"KILLED\"\n}\n\n\ncurl -ikv -u guest:guest-password -H Content-Type:application/json -X PUT -T state-killed.json 'https://localhost:8443/gateway/sandbox/resourcemanager/v1/cluster/apps/{application_id}/state'\n</code></pre> <p>Note: The sensitive data like nodeHttpAddress, nodeHostName, id will be hidden.</p>"},{"location":"sse-support/","title":"SSE Support","text":""},{"location":"sse-support/#server-sent-events-support","title":"Server-Sent Events Support","text":""},{"location":"sse-support/#introduction","title":"Introduction","text":"<p>Server-Sent Events (SSE) allows a server to push updates to the browser over a single longlived HTTP connection. The media type is test/event-stream.</p>"},{"location":"sse-support/#configuration","title":"Configuration","text":"<p>Asynchronous behaviour and therefore SSE support is not enabled by default. Modify the <code>gateway.servlet.async.supported</code> property to <code>true</code> in <code>&lt;KNOX-HOME&gt;/conf/gateway-site.xml</code> file.</p> <pre><code>  &lt;property&gt;\n      &lt;name&gt;gateway.servlet.async.supported&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n      &lt;description&gt;Enable/Disable async support.&lt;/description&gt;\n  &lt;/property&gt;\n</code></pre> <p>Service and rewrite rules need the be updated as well. Both Ha and non Ha configurations work.</p>"},{"location":"sse-support/#example","title":"Example","text":"<p>In the following sample configuration we assume that the backend SSE service URL is http://myhost:7435. And <code>gateway.servlet.async.supported</code> property is set to <code>true</code> as shown above.</p>"},{"location":"sse-support/#rewrite","title":"Rewrite","text":"<p>Example code snippet from <code>&lt;KNOX-HOME&gt;/data/services/{myservice}/{version}/rewrite.xml</code> where myservice = sservice and version = 0.1</p> <pre><code>  &lt;rules&gt;\n      &lt;rule dir=\"IN\" name=\"SSERVICE/sservice/inbound\" pattern=\"*://*:*/**/sservice/*\"&gt;\n        &lt;rewrite template=\"{$serviceUrl[SSERVICE]}/sse\"/&gt;\n      &lt;/rule&gt;\n  &lt;/rules&gt;\n</code></pre>"},{"location":"sse-support/#service","title":"Service","text":"<p>Example code snippet from <code>&lt;KNOX-HOME&gt;/data/services/{myservice}/{version}/service.xml</code> where myservice = sservice and version = 0.1</p> <pre><code>  &lt;service role=\"SSERVICE\" name=\"sservice\" version=\"0.1\"&gt;\n    &lt;routes&gt;\n      &lt;route path=\"/sservice/**\"&gt;&lt;/route&gt;\n    &lt;/routes&gt;\n    &lt;dispatch classname=\"org.apache.knox.gateway.sse.SSEDispatch\" ha-classname=\"org.apache.knox.gateway.ha.dispatch.SSEHaDispatch\"/&gt;\n  &lt;/service&gt;\n</code></pre>"},{"location":"sse-support/#topology","title":"Topology","text":"<p>Finally, update the topology file at <code>&lt;KNOX-HOME&gt;/conf/{topology}.xml</code>  with the backend service URL</p> <pre><code>  &lt;service&gt;\n    &lt;role&gt;SSERVICE&lt;/role&gt;\n    &lt;url&gt;http://myhost:7435&lt;/url&gt;\n  &lt;/service&gt;\n</code></pre>"},{"location":"webshell/","title":"Webshell","text":""},{"location":"webshell/#webshell","title":"Webshell","text":""},{"location":"webshell/#introduction","title":"Introduction","text":"<p>This feature enables shell access to the machine running Apache Knox. Users can SSO into Knox and then access shell using the Knox WebShell URL on knox homepage. There are some out of band configuration changes that are required for the feature to work. </p>"},{"location":"webshell/#prerequisite","title":"Prerequisite","text":"<p>This feature works only on *nix systems given it relies on sudoers file. Make sure following prerequisite are met before turning on the feature.</p> <ul> <li>Make sure Knox process user (user under which knox process runs) exists on local machine.</li> <li>Make sure the user used to login exists on local machine.</li> <li>Make sure you have proper rights to create/update sudoers file descibed in \"Configuration\" section below.</li> </ul>"},{"location":"webshell/#configuration","title":"Configuration","text":"<p>Webshell is not turned on by default. To enable Webshell following properties needs to be changed in <code>gateway-site.xml</code></p> <pre><code>property&gt;\n    &lt;name&gt;gateway.websocket.feature.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Enable/Disable websocket feature.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;gateway.webshell.feature.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Enable/Disable webshell feature.&lt;/description&gt;\n&lt;/property&gt;\n&lt;!-- in case JWT cookie validation for websockets is needed --&gt;\n&lt;property&gt;\n    &lt;name&gt;gateway.websocket.JWT.validation.feature.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Enable/Disable websocket JWT validation feature.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Create a sudoers file <code>/etc/sudoers.d/knox</code> (assuming, Apache Knox process is running as user <code>knox</code>) with mappings for all the users that need WebShell acess on the machine running Apache Knox. </p> <p>e.g. the following settings in <code>sudoers</code> file let's user <code>sam</code> and <code>knoxui</code> login to WebShell. Further restrictions on user <code>sam</code> and <code>knoxui</code> can be applied in <code>sudoers</code> file. More info: https://linux.die.net/man/5/sudoers. Here users <code>sam</code> and <code>knoxui</code> are SSO users that login using Knox authentication providers such as LDAP, PAM etc.</p> <pre><code>Defaults env_keep += JAVA_HOME\nDefaults always_set_home\nknox ALL=(sam:ALL) NOPASSWD: /bin/bash\nknox ALL=(knoxui:ALL) NOPASSWD: /bin/bash\n</code></pre>"},{"location":"websocket-support/","title":"WebSocket Support","text":""},{"location":"websocket-support/#websocket-support","title":"WebSocket Support","text":""},{"location":"websocket-support/#introduction","title":"Introduction","text":"<p>WebSocket is a communication protocol that allows full duplex communication over a single TCP connection. Knox provides out-of-the-box support for the WebSocket protocol, currently only text messages are supported.</p>"},{"location":"websocket-support/#configuration","title":"Configuration","text":"<p>By default WebSocket functionality is disabled, it can be easily enabled by changing the <code>gateway.websocket.feature.enabled</code> property to <code>true</code> in <code>&lt;KNOX-HOME&gt;/conf/gateway-site.xml</code> file.  </p> <pre><code>  &lt;property&gt;\n      &lt;name&gt;gateway.websocket.feature.enabled&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n      &lt;description&gt;Enable/Disable websocket feature.&lt;/description&gt;\n  &lt;/property&gt;\n</code></pre> <p>Service and rewrite rules need to changed accordingly to match the appropriate websocket context.</p>"},{"location":"websocket-support/#example","title":"Example","text":"<p>In the following sample configuration we assume that the backend WebSocket URL is ws://myhost:9999/ws. And 'gateway.websocket.feature.enabled' property is set to 'true' as shown above.</p>"},{"location":"websocket-support/#rewrite","title":"rewrite","text":"<p>Example code snippet from <code>&lt;KNOX-HOME&gt;/data/services/{myservice}/{version}/rewrite.xml</code> where myservice = websocket and version = 0.6.0</p> <pre><code>  &lt;rules&gt;\n    &lt;rule dir=\"IN\" name=\"WEBSOCKET/ws/inbound\" pattern=\"*://*:*/**/ws\"&gt;\n      &lt;rewrite template=\"{$serviceUrl[WEBSOCKET]}/ws\"/&gt;\n    &lt;/rule&gt;\n  &lt;/rules&gt;\n</code></pre>"},{"location":"websocket-support/#service","title":"service","text":"<p>Example code snippet from <code>&lt;KNOX-HOME&gt;/data/services/{myservice}/{version}/service.xml</code> where myservice = websocket and version = 0.6.0</p> <pre><code>  &lt;service role=\"WEBSOCKET\" name=\"websocket\" version=\"0.6.0\"&gt;\n    &lt;policies&gt;\n          &lt;policy role=\"webappsec\"/&gt;\n          &lt;policy role=\"authentication\" name=\"Anonymous\"/&gt;\n          &lt;policy role=\"rewrite\"/&gt;\n          &lt;policy role=\"authorization\"/&gt;\n    &lt;/policies&gt;\n    &lt;routes&gt;\n      &lt;route path=\"/ws\"&gt;\n          &lt;rewrite apply=\"WEBSOCKET/ws/inbound\" to=\"request.url\"/&gt;\n      &lt;/route&gt;\n    &lt;/routes&gt;\n  &lt;/service&gt;\n</code></pre>"},{"location":"websocket-support/#topology","title":"topology","text":"<p>Finally, update the topology file at <code>&lt;KNOX-HOME&gt;/conf/{topology}.xml</code>  with the backend service URL</p> <pre><code>  &lt;service&gt;\n      &lt;role&gt;WEBSOCKET&lt;/role&gt;\n      &lt;url&gt;ws://myhost:9999/ws&lt;/url&gt;\n  &lt;/service&gt;\n</code></pre>"},{"location":"x-forwarded-headers/","title":"X-Forwarded Headers","text":""},{"location":"x-forwarded-headers/#x-forwarded-headers-support","title":"X-Forwarded-* Headers Support","text":"<p>Out-of-the-box Knox provides support for some <code>X-Forwarded-*</code> headers through the use of a Servlet Filter. Specifically the headers handled/populated by Knox are:</p> <ul> <li>X-Forwarded-For</li> <li>X-Forwarded-Proto</li> <li>X-Forwarded-Port</li> <li>X-Forwarded-Host</li> <li>X-Forwarded-Server</li> <li>X-Forwarded-Context</li> </ul> <p>This functionality can be turned off by a configuration setting in the file gateway-site.xml and redeploying the necessary topology/topologies.</p> <p>The setting is (under the 'configuration' tag) :</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;gateway.xforwarded.enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>If this setting is absent, the default behavior is that the <code>X-Forwarded-*</code> header support is on or in other words, <code>gateway.xforwarded.enabled</code> is set to <code>true</code> by default.</p>"},{"location":"x-forwarded-headers/#header-population","title":"Header population","text":"<p>The following are the various rules for population of these headers:</p>"},{"location":"x-forwarded-headers/#x-forwarded-for","title":"X-Forwarded-For","text":"<p>This header represents a list of client IP addresses. If the header is already present Knox adds a comma separated value to the list. The value added is the client's IP address as Knox sees it. This value is added to the end of the list.</p>"},{"location":"x-forwarded-headers/#x-forwarded-proto","title":"X-Forwarded-Proto","text":"<p>The protocol used in the client request. If this header is passed into Knox its value is maintained, otherwise Knox will populate the header with the value 'https' if the request is a secure one or 'http' otherwise.</p>"},{"location":"x-forwarded-headers/#x-forwarded-port","title":"X-Forwarded-Port","text":"<p>The port used in the client request. If this header is passed into Knox its value is maintained, otherwise Knox will populate the header with the value of the port that the request was made coming into Knox.</p>"},{"location":"x-forwarded-headers/#x-forwarded-host","title":"X-Forwarded-Host","text":"<p>Represents the original host requested by the client in the Host HTTP request header. The value passed into Knox is maintained by Knox. If no value is present, Knox populates the header with the value of the HTTP Host header.</p>"},{"location":"x-forwarded-headers/#x-forwarded-server","title":"X-Forwarded-Server","text":"<p>The hostname of the server Knox is running on.</p>"},{"location":"x-forwarded-headers/#x-forwarded-context","title":"X-Forwarded-Context","text":"<p>This header value contains the context path of the request to Knox.</p>"},{"location":"dev-guide/admin-ui/","title":"Admin ui","text":""},{"location":"dev-guide/admin-ui/#introduction","title":"Introduction","text":"<p>The Admin UI is a work in progress. It has started with viewpoint of being a simple web interface for   Admin API functions but will hopefully grow into being able to also provide visibility into the gateway  in terms of logs and metrics.</p>"},{"location":"dev-guide/admin-ui/#source-and-binaries","title":"Source and Binaries","text":"<p>The Admin UI application follows the architecture of a hosted application in Knox. To that end it needs to be  packaged up in the gateway-applications module in the source tree so that in the installation it can wind up here</p> <p><code>&lt;GATEWAY_HOME&gt;/data/applications/admin-ui</code></p> <p>However since the application is built using angular and various node modules the source tree is not something we want to place into the gateway-applications module. Instead we will place the production 'binaries' in gateway-applications  and have the source in a module called 'gateway-admin-ui'.</p> <p>To work with the angular application you need to install some prerequisite tools. </p> <p>The main tool needed is the angular cli and while installing that you  will get its dependencies which should fulfill any other requirements Prerequisites</p>"},{"location":"dev-guide/admin-ui/#manager-topology","title":"Manager Topology","text":"<p>The Admin UI is deployed to a fixed topology. The topology file can be found under</p> <p><code>&lt;GATEWAY_HOME&gt;/conf/topologies/manager.xml</code></p> <p>The topology hosts an instance of the Admin API for the UI to use. The reason for this is that the existing Admin API needs  to have a different security model from that used by the Admin UI. The key components of this topology are:</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;webappsec&lt;/role&gt;\n    &lt;name&gt;WebAppSec&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;csrf.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;csrf.customHeader&lt;/name&gt;&lt;value&gt;X-XSRF-Header&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;csrf.methodsToIgnore&lt;/name&gt;&lt;value&gt;GET,OPTIONS,HEAD&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;xframe-options.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n    &lt;param&gt;&lt;name&gt;strict.transport.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n</code></pre> <p>and </p> <pre><code>&lt;application&gt;\n    &lt;role&gt;admin-ui&lt;/role&gt;\n&lt;/application&gt;\n</code></pre>"},{"location":"dev-guide/book/","title":"Overview","text":""},{"location":"dev-guide/book/#apache-knox-gateway-21x-developers-guide","title":"Apache Knox Gateway 2.1.x Developer's Guide","text":""},{"location":"dev-guide/book/#overview","title":"Overview","text":"<p>Apache Knox gateway is a specialized reverse proxy gateway for various Hadoop REST APIs. However, the gateway is built entirely upon a fairly generic framework. This framework is used to \"plug-in\" all of the behavior that makes it specific to Hadoop in general and any particular Hadoop REST API. It would be equally as possible to create a customized reverse proxy for other non-Hadoop HTTP endpoints. This approach is taken to ensure that the Apache Knox gateway can scale with the rapidly evolving Hadoop ecosystem.</p> <p>Throughout this guide we will be using a publicly available REST API to demonstrate the development of various extension mechanisms. http://openweathermap.org/</p>"},{"location":"dev-guide/book/#architecture-overview","title":"Architecture Overview","text":"<p>The gateway itself is a layer over an embedded Jetty JEE server. At the very highest level the gateway processes requests by using request URLs to lookup specific JEE Servlet Filter chain that is used to process the request. The gateway framework provides extensible mechanisms to assemble chains of custom filters that support secured access to services.</p> <p>The gateway has two primary extensibility mechanisms: Service and Provider. The Service extensibility framework provides a way to add support for new HTTP/REST endpoints. For example, the support for WebHdfs is plugged into the Knox gateway as a Service. The Provider extensibility framework allows adding new features to the gateway that can be used across Services. An example of a Provider is an authentication provider. Providers can also expose APIs that other service and provider extensions can utilize.</p> <p>Service and Provider integrations interact with the gateway framework in two distinct phases: Deployment and Runtime. The gateway framework can be thought of as a layer over the JEE Servlet framework. Specifically all runtime processing within the gateway is performed by JEE Servlet Filters. The two phases interact with this JEE Servlet Filter based model in very different ways. The first phase, Deployment, is responsible for converting fairly simple to understand configuration called topology into JEE WebArchive (WAR) based implementation details. The second phase, Runtime, is the processing of requests via a set of Filters configured in the WAR.</p> <p>From an \"ethos\" perspective, Service and Provider extensions should attempt to incur complexity associated with configuration in the deployment phase. This should allow for very streamlined request processing that is very high performance and easily testable. The preference at runtime, in OO style, is for small classes that perform a specific function. The ideal set of implementation classes are then assembled by the Service and Provider plugins during deployment.</p> <p>A second critical design consideration is streaming. The processing infrastructure is build around JEE Servlet Filters as they provide a natural streaming interception model. All Provider implementations should make every attempt to maintaining this streaming characteristic.</p>"},{"location":"dev-guide/book/#project-overview","title":"Project Overview","text":"<p>The table below describes the purpose of the current modules in the project. Of particular importance are the root pom.xml and the gateway-release module. The root pom.xml is critical because this is where all dependency version must be declared. There should be no dependency version information in module pom.xml files. The gateway-release module is critical because the dependencies declared there essentially define the classpath of the released gateway server. This is also true of the other -release modules in the project.</p> File/Module Description LICENSE The license for all source files in the release. NOTICE Attributions required by dependencies. README A brief overview of the Knox project. CHANGES A description of the changes for each release. ISSUES The knox issues for the current release. gateway-util-common Common low level utilities used by many modules. gateway-util-launcher The launcher framework. gateway-util-urltemplate The i18n logging and resource framework. gateway-i18n The URL template and rewrite utilities gateway-i18n-logging-log4j The integration of i18n logging with log4j. gateway-i18n-logging-sl4j The integration of i18n logging with sl4j. gateway-spi The SPI for service and provider extensions. gateway-provider-identity-assertion-common The identity assertion provider base gateway-provider-identity-assertion-concat An identity assertion provider that facilitates prefix and suffix concatenation. gateway-provider-identity-assertion-pseudo The default identity assertion provider. gateway-provider-jersey The jersey display provider. gateway-provider-rewrite The URL rewrite provider. gateway-provider-rewrite-func-hostmap-static Host mapping function extension to rewrite. gateway-provider-rewrite-func-service-registry Service registry function extension to rewrite. gateway-provider-rewrite-step-secure-query Crypto step extension to rewrite. gateway-provider-security-authz-acls Service level authorization. gateway-provider-security-jwt JSON Web Token utilities. gateway-provider-security-preauth Preauthenticated SSO header support. gateway-provider-security-shiro Shiro authentiation integration. gateway-provider-security-webappsec Filters to prevent common webapp security issues. gateway-service-as The implementation of the Access service POC. gateway-service-definitions The implementation of the Service definition and rewrite files. gateway-service-hbase The implementation of the HBase service. gateway-service-hive The implementation of the Hive service. gateway-service-oozie The implementation of the Oozie service. gateway-service-tgs The implementation of the Ticket Granting service POC. gateway-service-webhdfs The implementation of the WebHdfs service. gateway-discovery-ambari The Ambari service URL discovery implementation. gateway-service-remoteconfig The implementation of the RemoteConfigurationRegistryClientService. gateway-server The implementation of the Knox gateway server. gateway-shell The implementation of the Knox Groovy shell. gateway-test-ldap Pulls in all of the dependencies of the test LDAP server. gateway-server-launcher The launcher definition for the gateway. gateway-shell-launcher The launcher definition for the shell. knox-cli-launcher A module to pull in all of the dependencies of the CLI. gateway-test-ldap-launcher The launcher definition for the test LDAP server. gateway-release The definition of the gateway binary release. Contains content and dependencies to be included in binary gateway package. gateway-test-utils Various utilities used in unit and system tests. gateway-test The functional tests. pom.xml The top level pom. build.xml A collection of utility for building and releasing."},{"location":"dev-guide/book/#development-processes","title":"Development Processes","text":"<p>The project uses Maven in general with a few convenience Ant targets.</p> <p>Building the project can be built via Maven or Ant.  The two commands below are equivalent.</p> <pre><code>mvn clean install\nant\n</code></pre> <p>A more complete build can be done that builds and generates the unsigned ZIP release artifacts. You will find these in the target/{version} directory (e.g. target/0.XX.0-SNAPSHOT).</p> <pre><code>mvn -Ppackage clean install\nant release\n</code></pre> <p>There are a few other Ant targets that are especially convenient for testing.</p> <p>This command installs the gateway into the {{{install}}} directory of the project. Note that this command does not first build the project.</p> <pre><code>ant install-test-home\n</code></pre> <p>This command starts the gateway and LDAP servers installed by the command above into a test GATEWAY_HOME (i.e. install). Note that this command does not first install the test home.</p> <pre><code>ant start-test-servers\n</code></pre> <p>So putting things together the following Ant command will build a release, install it and start the servers ready for manual testing.</p> <pre><code>ant release install-test-home start-test-servers\n</code></pre>"},{"location":"dev-guide/book/#docker-image","title":"Docker Image","text":"<p>Apache Knox ships with a <code>docker</code> Maven module that will build a Docker image. To build the Knox Docker image, you must have Docker running on your machine. The following Maven command will build Knox and package it into a Docker image.</p> <pre><code>mvn -Ppackage,release,docker clean package\n</code></pre> <p>This will build 2 Docker images:</p> <ul> <li><code>apache/knox:gateway-2.1.0-SNAPSHOT</code></li> <li><code>apache/knox:ldap-2.1.0-SNAPSHOT</code></li> </ul> <p>The <code>gateway</code> image will use an entrypoint to start Knox Gateway. The <code>ldap</code> image will use an entrypoint to start Knox Demo LDAP.</p> <p>An example of using the Docker images would be the following:</p> <pre><code>docker run -d --name knox-ldap -p 33389:33389 apache/knox:ldap-2.1.0-SNAPSHOT\ndocker run -d --name knox-gateway -p 8443:8443 apache/knox:gateway-2.1.0-SNAPSHOT\n</code></pre> <p>Using docker-compose that would look like this:</p> <pre><code>docker-compose -f gateway-docker/src/main/resources/docker-compose.yml up\n</code></pre> <p>The images are designed to be a base that can be built on to add your own providers, descriptors, and topologies as necessary.</p>"},{"location":"dev-guide/book/#behavior","title":"Behavior","text":"<p>There are two distinct phases in the behavior of the gateway. These are the deployment and runtime phases. The deployment phase is responsible for converting topology descriptors into an executable JEE style WAR. The runtime phase is the processing of requests via WAR created during the deployment phase.</p> <p>The deployment phase is arguably the more complex of the two phases. This is because runtime relies on well known JEE constructs while deployment introduces new framework concepts. The base concept of the deployment framework is that of a \"contributor\". In the framework, contributors are pluggable component responsible for generating JEE WAR artifacts from topology files.</p>"},{"location":"dev-guide/book/#deployment-behavior","title":"Deployment Behavior","text":"<p>The goal of the deployment phase is to take easy to understand topology descriptions and convert them into optimized runtime artifacts. Our goal is not only should the topology descriptors be easy to understand, but have them be easy for a management system (e.g. Ambari) to generate. Think of deployment as compiling an assembly descriptor into a JEE WAR. WARs are then deployed to an embedded JEE container (i.e. Jetty).</p> <p>Consider the results of starting the gateway the first time. There are two sets of files that are relevant for deployment. The first is the topology file <code>&lt;GATEWAY_HOME&gt;/conf/topologies/sandbox.xml</code>. This second set is the WAR structure created during the deployment of the topology file.</p> <pre><code>data/deployments/sandbox.war.143bfef07f0/WEB-INF\n  web.xml\n  gateway.xml\n  shiro.ini\n  rewrite.xml\n  hostmap.txt\n</code></pre> <p>Notice that the directory <code>sandbox.war.143bfef07f0</code> is an \"unzipped\" representation of a JEE WAR file. This specifically means that it contains a <code>WEB-INF</code> directory which contains a <code>web.xml</code> file. For the curious the strange number (i.e. 143bfef07f0) in the name of the WAR directory is an encoded timestamp. This is the timestamp of the topology file (i.e. sandbox.xml) at the time the deployment occurred. This value is used to determine when topology files have changed and redeployment is required.</p> <p>Here is a brief overview of the purpose of each file in the WAR structure.</p> <p>web.xml : A standard JEE WAR descriptor. In this case a build-in GatewayServlet is mapped to the url pattern /*.</p> <p>gateway.xml : The configuration file for the GatewayServlet. Defines the filter chain that will be applied to each service's various URLs.</p> <p>shiro.ini : The configuration file for the Shiro authentication provider's filters. This information is derived from the information in the provider section of the topology file.</p> <p>rewrite.xml : The configuration file for the rewrite provider's filter. This captures all of the rewrite rules for the services. These rules are contributed by the contributors for each service.</p> <p>hostmap.txt : The configuration file the hostmap provider's filter. This information is derived from the information in the provider section of the topology file.</p> <p>The deployment framework follows \"visitor\" style patterns. Each topology file is parsed and the various constructs within it are \"visited\". The appropriate contributor for each visited construct is selected by the framework. The contributor is then passed the contrust from the topology file and asked to update the JEE WAR artifacts. Each contributor is free to inspect and modify any portion of the WAR artifacts.</p> <p>The diagram below provides an overview of the deployment processing. Detailed descriptions of each step follow the diagram.</p> <pre><code>@startuml\n!include deployment-overview.puml\n@enduml\n</code></pre> <ol> <li> <p>The gateway server loads a topology file from conf/topologies into an internal structure.</p> </li> <li> <p>The gateway server delegates to a deployment factory to create the JEE WAR structure.</p> </li> <li> <p>The deployment factory first creates a basic WAR structure with WEB-INF/web.xml.</p> </li> <li> <p>Each provider and service in the topology is visited and the appropriate deployment contributor invoked. Each contributor is passed the appropriate information from the topology and modifies the WAR structure.</p> </li> <li> <p>A complete WAR structure is returned to the gateway service.</p> </li> <li> <p>The gateway server uses internal container APIs to dynamically deploy the WAR.</p> </li> </ol> <p>The Java method below is the actual code from the DeploymentFactory that implements this behavior. You will note the initialize, contribute, finalize sequence. Each contributor is given three opportunities to interact with the topology and archive. This allows the various contributors to interact if required. For example, the service contributors use the deployment descriptor added to the WAR by the rewrite provider.</p> <pre><code>public static WebArchive createDeployment( GatewayConfig config, Topology topology ) {\n  Map&lt;String,List&lt;ProviderDeploymentContributor&gt;&gt; providers;\n  Map&lt;String,List&lt;ServiceDeploymentContributor&gt;&gt; services;\n  DeploymentContext context;\n\n  providers = selectContextProviders( topology );\n  services = selectContextServices( topology );\n  context = createDeploymentContext( config, topology.getName(), topology, providers, services );\n\n  initialize( context, providers, services );\n  contribute( context, providers, services );\n  finalize( context, providers, services );\n\n  return context.getWebArchive();\n}\n</code></pre> <p>Below is a diagram that provides more detail. This diagram focuses on the interactions between the deployment factory and the service deployment contributors. Detailed description of each step follow the diagram.</p> <pre><code>@startuml\n!include deployment-service.puml\n@enduml\n</code></pre> <ol> <li> <p>The gateway server loads global configuration (i.e. /conf/gateway-site.xml <li> <p>The gateway server loads a topology descriptor file.</p> </li> <li> <p>The gateway server delegates to the deployment factory to create a deployable WAR structure.</p> </li> <li> <p>The deployment factory creates a runtime descriptor to configure that gateway servlet.</p> </li> <li> <p>The deployment factory creates a basic WAR structure and adds the gateway servlet runtime descriptor to it.</p> </li> <li> <p>The deployment factory creates a deployment context object and adds the WAR structure to it.</p> </li> <li> <p>For each service defined in the topology descriptor file the appropriate service deployment contributor is selected and invoked. The correct service deployment contributor is determined by matching the role of a service in the topology descriptor to a value provided by the getRole() method of the ServiceDeploymentContributor interface. The initializeContribution method from each service identified in the topology is called. Each service deployment contributor is expected to setup any runtime artifacts in the WAR that other services or provides may need.</p> </li> <li> <p>The contributeService method from each service identified in the topology is called. This is where the service deployment contributors will modify any runtime descriptors.</p> </li> <li> <p>One of they ways that a service deployment contributor can modify the runtime descriptors is by asking the framework to contribute filters. This is how services are loosely coupled to the providers of features. For example a service deployment contributor might ask the framework to contribute the filters required for authorization. The deployment framework will then delegate to the correct provider deployment contributor to add filters for that feature.</p> </li> <li> <p>Finally the finalizeContribution method for each service is invoked. This provides an opportunity to react to anything done via the contributeService invocations and tie up any loose ends.</p> </li> <li> <p>The populated WAR is returned to the gateway server.</p> </li> <p>The following diagram will provide expanded detail on the behavior of provider deployment contributors. Much of the beginning and end of the sequence shown overlaps with the service deployment sequence above. Those steps (i.e. 1-6, 17) will not be described below for brevity. The remaining steps have detailed descriptions following the diagram.</p> <pre><code>@startuml\n!include deployment-provider.puml\n@enduml\n</code></pre> <ol> <li> <p>For each provider the appropriate provider deployment contributor is selected and invoked. The correct service deployment contributor is determined by first matching the role of a provider in the topology descriptor to a value provided by the getRole() method of the ProviderDeploymentContributor interface. If this is ambiguous, the name from the topology is used match the value provided by the getName() method of the ProviderDeploymentContributor interface. The initializeContribution method from each provider identified in the topology is called. Each provider deployment contributor is expected to setup any runtime artifacts in the WAR that other services or provides may need. Note: In addition, others provider not explicitly referenced in the topology may have their initializeContribution method called. If this is the case only one default instance for each role declared vis the getRole() method will be used. The method used to determine the default instance is non-deterministic so it is best to select a particular named instance of a provider for each role.</p> </li> <li> <p>Each provider deployment contributor will typically add any runtime deployment descriptors it requires for operation. These descriptors are added to the WAR structure within the deployment context.</p> </li> <li> <p>The contributeProvider method of each configured or default provider deployment contributor is invoked.</p> </li> <li> <p>Each provider deployment contributor populates any runtime deployment descriptors based on information in the topology.</p> </li> <li> <p>Provider deployment contributors are never asked to contribute to the deployment directly. Instead a service deployment contributor will ask to have a particular provider role (e.g. authentication) contribute to the deployment.</p> </li> <li> <p>A service deployment contributor asks the framework to contribute filters for a given provider role.</p> </li> <li> <p>The framework selects the appropriate provider deployment contributor and invokes its contributeFilter method.</p> </li> <li> <p>During this invocation the provider deployment contributor populate populate service specific information. In particular it will add filters to the gateway servlet's runtime descriptor by adding JEE Servlet Filters. These filters will be added to the resources (or URLs) identified by the service deployment contributor.</p> </li> <li> <p>The finalizeContribute method of all referenced and default provider deployment contributors is invoked.</p> </li> <li> <p>The provider deployment contributor is expected to perform any final modifications to the runtime descriptors in the WAR structure.</p> </li> </ol>"},{"location":"dev-guide/book/#runtime-behavior","title":"Runtime Behavior","text":"<p>The runtime behavior of the gateway is somewhat simpler as it more or less follows well known JEE models. There is one significant wrinkle. The filter chains are managed within the GatewayServlet as opposed to being managed by the JEE container. This is the result of an early decision made in the project. The intention is to allow more powerful URL matching than is provided by the JEE Servlet mapping mechanisms.</p> <p>The diagram below provides a high-level overview of the runtime processing. An explanation for each step is provided after the diagram.</p> <pre><code>@startuml\n!include runtime-overview.puml\n@enduml\n</code></pre> <ol> <li> <p>A REST client makes a HTTP request that is received by the embedded JEE container.</p> </li> <li> <p>A filter chain is looked up in a map of URLs to filter chains.</p> </li> <li> <p>The filter chain, which is itself a filter, is invoked.</p> </li> <li> <p>Each filter invokes the filters that follow it in the chain. The request and response objects can be wrapped in typically JEE Filter fashion. Filters may not continue chain processing and return if that is appropriate.</p> </li> <li> <p>Eventually the end of the last filter in the chain is invoked. Typically this is a special \"dispatch\" filter that is responsible for dispatching the request to the ultimate endpoint. Dispatch filters are also responsible for reading the response.</p> </li> <li> <p>The response may be in the form of a number of content types (e.g. application/json, text/xml).</p> </li> <li> <p>The response entity is streamed through the various response wrappers added by the filters. These response wrappers may rewrite various portions of the headers and body as per their configuration.</p> </li> <li> <p>The return of the response entity to the client is ultimately \"pulled through\" the filter response wrapper by the container.</p> </li> <li> <p>The response entity is returned original client.</p> </li> </ol> <p>This diagram provides a more detailed breakdown of the request processing. Again, descriptions of each step follow the diagram.</p> <pre><code>@startuml\n!include runtime-request-processing.puml\n@enduml\n</code></pre> <ol> <li> <p>A REST client makes a HTTP request that is received by the embedded JEE container.</p> </li> <li> <p>The embedded container looks up the servlet mapped to the URL and invokes the service method. This our case the GatewayServlet is mapped to / and therefore receives all requests for a given topology. Keep in mind that the WAR itself is deployed on a root context path that typically contains a level for the gateway and the name of the topology. This means that there is a single GatewayServlet per topology and it is effectivly mapped to //. <li> <p>The GatewayServlet holds a single reference to a GatewayFilter which is a specialized JEE Servlet Filter. This choice was made to allow the GatewayServlet to dynamically deploy modified topologies. This is done by building a new GatewayFilter instance and replacing the old in an atomic fashion.</p> </li> <li> <p>The GatewayFilter contains another layer of URL mapping as defined in the gateway.xml runtime descriptor. The various service deployment contributor added these mappings at deployment time. Each service may add a number of different sub-URLs depending in their requirements. These sub-URLs will all be mapped to independently configured filter chains.</p> </li> <li> <p>The GatewayFilter invokes the doFilter method on the selected chain.</p> </li> <li> <p>The chain invokes the doFilter method of the first filter in the chain.</p> </li> <li> <p>Each filter in the chain continues processing by invoking the doFilter on the next filter in the chain. Ultimately a dispatch filter forward the request to the real service instead of invoking another filter. This is sometimes referred to as pivoting.</p> </li>"},{"location":"dev-guide/book/#gateway-servlet-gateway-filter","title":"Gateway Servlet &amp; Gateway Filter","text":"<p>TODO</p> <pre><code>&lt;web-app&gt;\n\n  &lt;servlet&gt;\n    &lt;servlet-name&gt;sample&lt;/servlet-name&gt;\n    &lt;servlet-class&gt;org.apache.knox.gateway.GatewayServlet&lt;/servlet-class&gt;\n    &lt;init-param&gt;\n      &lt;param-name&gt;gatewayDescriptorLocation&lt;/param-name&gt;\n      &lt;param-value&gt;gateway.xml&lt;/param-value&gt;\n    &lt;/init-param&gt;\n  &lt;/servlet&gt;\n\n  &lt;servlet-mapping&gt;\n    &lt;servlet-name&gt;sandbox&lt;/servlet-name&gt;\n    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;\n  &lt;/servlet-mapping&gt;\n\n  &lt;listener&gt;\n    &lt;listener-class&gt;org.apache.knox.gateway.services.GatewayServicesContextListener&lt;/listener-class&gt;\n  &lt;/listener&gt;\n\n  ...\n\n&lt;/web-app&gt;\n</code></pre> <pre><code>&lt;gateway&gt;\n\n  &lt;resource&gt;\n    &lt;role&gt;WEATHER&lt;/role&gt;\n    &lt;pattern&gt;/weather/**?**&lt;/pattern&gt;\n\n    &lt;filter&gt;\n      &lt;role&gt;authentication&lt;/role&gt;\n      &lt;name&gt;sample&lt;/name&gt;\n      &lt;class&gt;...&lt;/class&gt;\n    &lt;/filter&gt;\n\n    &lt;filter&gt;...&lt;/filter&gt;*\n\n  &lt;/resource&gt;\n\n&lt;/gateway&gt;\n</code></pre> <pre><code>@Test\npublic void testDevGuideSample() throws Exception {\n  Template pattern, input;\n  Matcher&lt;String&gt; matcher;\n  Matcher&lt;String&gt;.Match match;\n\n  // GET http://api.openweathermap.org/data/2.5/weather?q=Palo+Alto\n  pattern = Parser.parse( \"/weather/**?**\" );\n  input = Parser.parse( \"/weather/2.5?q=Palo+Alto\" );\n\n  matcher = new Matcher&lt;String&gt;();\n  matcher.add( pattern, \"fake-chain\" );\n  match = matcher.match( input );\n\n  assertThat( match.getValue(), is( \"fake-chain\") );\n}\n</code></pre>"},{"location":"dev-guide/book/#extension-logistics","title":"Extension Logistics","text":"<p>There are a number of extension points available in the gateway: services, providers, rewrite steps and functions, etc. All of these use the Java ServiceLoader mechanism for their discovery. There are two ways to make these extensions available on the class path at runtime. The first way to add a new module to the project and have the extension \"built-in\". The second is to add the extension to the class path of the server after it is installed. Both mechanism are described in more detail below.</p>"},{"location":"dev-guide/book/#service-loaders","title":"Service Loaders","text":"<p>Extensions are discovered via Java's Service Loader mechanism. There are good tutorials available for learning more about this. The basics come down to two things.</p> <ol> <li> <p>Implement the service contract interface (e.g. ServiceDeploymentContributor, ProviderDeploymentContributor)</p> </li> <li> <p>Create a file in META-INF/services of the JAR that will contain the extension. This file will be named as the fully qualified name of the contract interface (e.g. org.apache.knox.gateway.deploy.ProviderDeploymentContributor). The contents of the file will be the fully qualified names of any implementation of that contract interface in that JAR.</p> </li> </ol> <p>One tip is to include a simple test with each of you extension to ensure that it will be properly discovered. This is very helpful in situations where a refactoring fails to change the a class in the META-INF/services files. An example of one such test from the project is shown below.</p> <pre><code>  @Test\n  public void testServiceLoader() throws Exception {\n    ServiceLoader loader = ServiceLoader.load( ProviderDeploymentContributor.class );\n    Iterator iterator = loader.iterator();\n    assertThat( \"Service iterator empty.\", iterator.hasNext() );\n    while( iterator.hasNext() ) {\n      Object object = iterator.next();\n      if( object instanceof ShiroDeploymentContributor ) {\n        return;\n      }\n    }\n    fail( \"Failed to find \" + ShiroDeploymentContributor.class.getName() + \" via service loader.\" );\n  }\n</code></pre>"},{"location":"dev-guide/book/#class-path","title":"Class Path","text":"<p>One way to extend the functionality of the server without having to recompile is to add the extension JARs to the servers class path. As an extensible server this is made straight forward but it requires some understanding of how the server's classpath is setup. In the  directory there are four class path related directories (i.e. bin, lib, dep, ext). <p>The bin directory contains very small \"launcher\" jars that contain only enough code to read configuration and setup a class path. By default the configuration of a launcher is embedded with the launcher JAR but it may also be extracted into a .cfg file. In that file you will see how the class path is defined.</p> <pre><code>class.path=../lib/*.jar,../dep/*.jar;../ext;../ext/*.jar\n</code></pre> <p>The paths are all relative to the directory that contains the launcher JAR.</p> <p>../lib/*.jar : These are the \"built-in\" jars that are part of the project itself. Information is provided elsewhere in this document for how to integrate a built-in extension.</p> <p>../dep/*.jar : These are the JARs for all of the external dependencies of the project. This separation between the generated JARs and dependencies help keep licensing issues straight.</p> <p>../ext : This directory is for post-install extensions and is empty by default. Including the directory (vs *.jar) allows for individual classes to be placed in this directory.</p> <p>../ext/*.jar : This would pick up all extension JARs placed in the ext directory.</p> <p>Note that order is significant.  The lib JARs take precedence over dep JARs and they take precedence over ext classes and JARs.</p>"},{"location":"dev-guide/book/#maven-module","title":"Maven Module","text":"<p>Integrating an extension into the project follows well established Maven patterns for adding modules. Below are several points that are somewhat unique to the Knox project.</p> <ol> <li> <p>Add the module to the root pom.xml file's  list. Take care to ensure that the module is in the correct place in the list based on its dependencies. Note: In general modules should not have non-test dependencies on gateway-server but rather gateway-spi <li> <p>Any new dependencies must be represented in the root pom.xml file's  section. The required version of the dependencies will be declared there. The new sub-module's pom.xml file must not include dependency version information. This helps prevent dependency version conflict issues. <li> <p>If the extension is to be \"built into\" the released gateway server it needs to be added as a dependency to the gateway-release module. This is done by adding to the  section of the gateway-release's pom.xml file. If this isn't done the JARs for the module will not be automatically packaged into the release artifacts. This can be useful while an extension is under development but not yet ready for inclusion in the release. <p>More detailed examples of adding both a service and a provider extension are provided in subsequent sections.</p>"},{"location":"dev-guide/book/#services","title":"Services","text":"<p>Services are extensions that are responsible for converting information in the topology file to runtime descriptors. Typically services do not require their own runtime descriptors. Rather, they modify either the gateway runtime descriptor (i.e. gateway.xml) or descriptors of other providers (e.g. rewrite.xml).</p> <p>The service provider interface for a Service is ServiceDeploymentContributor and is shown below.</p> <pre><code>package org.apache.knox.gateway.deploy;\nimport org.apache.knox.gateway.topology.Service;\npublic interface ServiceDeploymentContributor {\n  String getRole();\n  void initializeContribution( DeploymentContext context );\n  void contributeService( DeploymentContext context, Service service ) throws Exception;\n  void finalizeContribution( DeploymentContext context );\n}\n</code></pre> <p>Each service provides an implementation of this interface that is discovered via the ServerLoader mechanism previously described. The meaning of this is best understood in the context of the structure of the topology file. A fragment of a topology file is shown below.</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        ....\n    &lt;/gateway&gt;\n    &lt;service&gt;\n        &lt;role&gt;WEATHER&lt;/role&gt;\n        &lt;url&gt;http://api.openweathermap.org/data&lt;/url&gt;\n    &lt;/service&gt;\n    ....\n&lt;/topology&gt;\n</code></pre> <p>With these two things a more detailed description of the purpose of each ServiceDeploymentContributor method should be helpful.</p> <p>String getRole(); : This is the value the framework uses to associate a given <code>&lt;service&gt;&lt;role&gt;</code> with a particular ServiceDeploymentContributor implementation. See below how the example WeatherDeploymentContributor implementation returns the role WEATHER that matches the value in the topology file. This will result in the WeatherDeploymentContributor's methods being invoked when a WEATHER service is encountered in the topology file.</p> <pre><code>public class WeatherDeploymentContributor extends ServiceDeploymentContributorBase {\n  private static final String ROLE = \"WEATHER\";\n  @Override\n  public String getRole() {\n    return ROLE;\n  }\n  ...\n}\n</code></pre> <p>void initializeContribution( DeploymentContext context ); : In this method a contributor would create, initialize and add any descriptors it was responsible for to the deployment context. For the weather service example this isn't required so the empty method isn't shown here.</p> <p>void contributeService( DeploymentContext context, Service service ) throws Exception; : In this method a service contributor typically add and configures any features it requires. This method will be dissected in more detail below.</p> <p>void finalizeContribution( DeploymentContext context ); : In this method a contributor would finalize any descriptors it was responsible for to the deployment context. For the weather service example this isn't required so the empty method isn't shown here.</p>"},{"location":"dev-guide/book/#service-contribution-behavior","title":"Service Contribution Behavior","text":"<p>In order to understand the job of the ServiceDeploymentContributor a few runtime descriptors need to be introduced.</p> <p>Gateway Runtime Descriptor: WEB-INF/gateway.xml : This runtime descriptor controls the behavior of the GatewayFilter. It defines a mapping between resources (i.e. URL patterns) and filter chains. The sample gateway runtime descriptor helps illustrate.</p> <pre><code>&lt;gateway&gt;\n  &lt;resource&gt;\n    &lt;role&gt;WEATHER&lt;/role&gt;\n    &lt;pattern&gt;/weather/**?**&lt;/pattern&gt;\n    &lt;filter&gt;\n      &lt;role&gt;authentication&lt;/role&gt;\n      &lt;name&gt;sample&lt;/name&gt;\n      &lt;class&gt;...&lt;/class&gt;\n    &lt;/filter&gt;\n    &lt;filter&gt;...&lt;/filter&gt;*\n    ...\n  &lt;/resource&gt;\n&lt;/gateway&gt;\n</code></pre> <p>Rewrite Provider Runtime Descriptor: WEB-INF/rewrite.xml : The rewrite provider runtime descriptor controls the behavior of the rewrite filter. Each service contributor is responsible for adding the rules required to control the URL rewriting required by that service. Later sections will provide more detail about the capabilities of the rewrite provider.</p> <pre><code>&lt;rules&gt;\n  &lt;rule dir=\"IN\" name=\"WEATHER/openweathermap/inbound/versioned/file\"\n      pattern=\"*://*:*/**/weather/{version}?{**}\"&gt;\n    &lt;rewrite template=\"{$serviceUrl[WEATHER]}/{version}/weather?{**}\"/&gt;\n  &lt;/rule&gt;\n&lt;/rules&gt;\n</code></pre> <p>With these two descriptors in mind a detailed breakdown of the WeatherDeploymentContributor's contributeService method will make more sense. At a high level the important concept is that contributeService is invoked by the framework for each  in the topology file. <pre><code>public class WeatherDeploymentContributor extends ServiceDeploymentContributorBase {\n  ...\n  @Override\n  public void contributeService( DeploymentContext context, Service service ) throws Exception {\n    contributeResources( context, service );\n    contributeRewriteRules( context );\n  }\n\n  private void contributeResources( DeploymentContext context, Service service ) throws URISyntaxException {\n    ResourceDescriptor resource = context.getGatewayDescriptor().addResource();\n    resource.role( service.getRole() );\n    resource.pattern( \"/weather/**?**\" );\n    addAuthenticationFilter( context, service, resource );\n    addRewriteFilter( context, service, resource );\n    addDispatchFilter( context, service, resource );\n  }\n\n  private void contributeRewriteRules( DeploymentContext context ) throws IOException {\n    UrlRewriteRulesDescriptor allRules = context.getDescriptor( \"rewrite\" );\n    UrlRewriteRulesDescriptor newRules = loadRulesFromClassPath();\n    allRules.addRules( newRules );\n  }\n\n  ...\n}\n</code></pre> <p>The DeploymentContext parameter contains information about the deployment as well as the WAR structure being created via deployment. The Service parameter is the object representation of the  element in the topology file. Details about particularly important lines follow the code block. <p>ResourceDescriptor resource = context.getGatewayDescriptor().addResource(); : Obtains a reference to the gateway runtime descriptor and adds a new resource element. Note that many of the APIs in the deployment framework follow a fluent vs bean style.</p> <p>resource.role( service.getRole() ); : Sets the role for a particular resource. Many of the filters may need access to this role information in order to make runtime decisions.</p> <p>resource.pattern( \"/weather/?\" ); : Sets the URL pattern to which the filter chain that will follow will be mapped within the GatewayFilter.</p> <p>add*Filter( context, service, resource ); : These are taken from a base class. A representation of the implementation of that method from the base class is shown below. Notice how this essentially delegates back to the framework to add the filters required by a particular provider role (e.g. \"rewrite\").</p> <pre><code>  protected void addRewriteFilter( DeploymentContext context, Service service, ResourceDescriptor resource ) {\n    context.contributeFilter( service, resource, \"rewrite\", null, null );\n  }\n</code></pre> <p>UrlRewriteRulesDescriptor allRules = context.getDescriptor( \"rewrite\" ); : Here the rewrite provider runtime descriptor is obtained by name from the deployment context. This does represent a tight coupling in this case between this service and the default rewrite provider. The rewrite provider however is unlikely to be related with alternate implementations.</p> <p>UrlRewriteRulesDescriptor newRules = loadRulesFromClassPath(); : This is convenience method for loading partial rewrite descriptor information from the classpath. Developing and maintaining these rewrite rules is far easier as an external resource. The rewrite descriptor API could however have been used to achieve the same result.</p> <p>allRules.addRules( newRules ); : Here the rewrite rules for the weather service are merged into the larger set of rewrite rules.</p> <pre><code>&lt;project&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;parent&gt;\n        &lt;groupId&gt;org.apache.knox&lt;/groupId&gt;\n        &lt;artifactId&gt;gateway&lt;/artifactId&gt;\n        &lt;version&gt;2.1.0-SNAPSHOT&lt;/version&gt;\n    &lt;/parent&gt;\n\n    &lt;artifactId&gt;gateway-service-weather&lt;/artifactId&gt;\n    &lt;name&gt;gateway-service-weather&lt;/name&gt;\n    &lt;description&gt;A sample extension to the gateway for a weather REST API.&lt;/description&gt;\n\n    &lt;licenses&gt;\n        &lt;license&gt;\n            &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt;\n            &lt;url&gt;https://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt;\n            &lt;distribution&gt;repo&lt;/distribution&gt;\n        &lt;/license&gt;\n    &lt;/licenses&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;${gateway-group}&lt;/groupId&gt;\n            &lt;artifactId&gt;gateway-spi&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;${gateway-group}&lt;/groupId&gt;\n            &lt;artifactId&gt;gateway-provider-rewrite&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\n        ... Test Dependencies ...\n\n    &lt;/dependencies&gt;\n\n&lt;/project&gt;\n</code></pre>"},{"location":"dev-guide/book/#service-definition-files","title":"Service Definition Files","text":"<p>As of release 0.6.0, the gateway now also supports a declarative way of plugging-in a new Service. A Service can be defined with a combination of two files, these are:</p> <pre><code>service.xml\nrewrite.xml\n</code></pre> <p>The rewrite.xml file contains the rewrite rules as defined in other sections of this guide, and the service.xml file contains the various routes (paths) to be provided by the Service and the rewrite rule bindings to those paths. This will be described in further detail in this section.</p> <p>While the service.xml file is absolutely required, the rewrite.xml file in theory is optional (though it is highly unlikely that no rewrite rules are needed).</p> <p>To add a new service, simply add a service.xml and rewrite.xml file in an appropriate directory (see #[Service Definition Directory Structure]) in the module gateway-service-definitions to make the new service part of the Knox build.</p>"},{"location":"dev-guide/book/#servicexml","title":"service.xml","text":"<p>Below is a sample of a very simple service.xml file, taking the same weather api example.</p> <pre><code>&lt;service role=\"WEATHER\" name=\"weather\" version=\"0.1.0\"&gt;\n    &lt;routes&gt;\n        &lt;route path=\"/weather/**?**\"/&gt;\n    &lt;/routes&gt;\n&lt;/service&gt;\n</code></pre> <p>service : The root tag is 'service' that has the three required attributes: role, name and version. These three values disambiguate this service definition from others. To ensure the exact same service definition is being used in a topology file, all values should be specified,</p> <pre><code>&lt;topology&gt;\n    &lt;gateway&gt;\n        ....\n    &lt;/gateway&gt;\n    &lt;service&gt;\n        &lt;role&gt;WEATHER&lt;/role&gt;\n        &lt;name&gt;weather&lt;/name&gt;\n        &lt;version&gt;0.1.0&lt;/version&gt;\n        &lt;url&gt;http://api.openweathermap.org/data&lt;/url&gt;\n        &lt;dispatch&gt;\n            &lt;contributor-name&gt;custom-client&lt;/contributor-name&gt;\n            &lt;ha-contributor-name&gt;ha-client&lt;/ha-contributor-name&gt;\n            &lt;classname&gt;org.apache.knox.gateway.dispatch.PassAllHeadersDispatch&lt;/classname&gt;\n            &lt;ha-classname&gt;&lt;/ha-classname&gt;\n            &lt;http-client-factory&gt;&lt;/http-client-factory&gt;\n            &lt;use-two-way-ssl&gt;false&lt;/use-two-way-ssl&gt;\n        &lt;/dispatch&gt;\n    &lt;/service&gt;\n    ....\n&lt;/topology&gt;\n</code></pre> <p>If only role is specified in the topology file (the only required element other than url) then the first service definition of that role found will be used with the highest version of that role and name. Similarly if only the version is omitted from the topology specification of the service, the service definition of the highest version will be used. It is therefore important to specify a version for a service if it is desired that a topology be locked down to a specific version of a service.</p> <p>The optional <code>dispatch</code> element can be used to override the dispatch specified in service.xml file for the service, this can be useful in cases where you want a specific topology to override the dispatch, which is useful for topology based federation from one Knox instance to another. <code>dispatch\\classname</code> is the most commonly used option, but other options are available if one wants more fine grained control over topology dispatch override. </p> <p>routes : Wrapper element for one or more routes.</p> <p>route : A route specifies the path that the service is routing as well as any rewrite bindings or policy bindings. Another child element that may be used here is a dispatch element.</p> <p>rewrite : A rewrite rule or function that is to be applied to the path. A rewrite element contains a apply attribute that references the rewrite function or rule by name. Along with the apply attribute, a to attribute must be used. The to specifies what part of the request or response to rewrite. The valid values for the to attribute are:</p> <ul> <li>request.url</li> <li>request.headers</li> <li>request.cookies</li> <li>request.body</li> <li>response.headers</li> <li>response.cookies</li> <li>response.body</li> </ul> <p>Below is an example of a snippet from the WebHDFS service definition</p> <pre><code>    &lt;route path=\"/webhdfs/v1/**?**\"&gt;\n        &lt;rewrite apply=\"WEBHDFS/webhdfs/inbound/namenode/file\" to=\"request.url\"/&gt;\n        &lt;rewrite apply=\"WEBHDFS/webhdfs/outbound/namenode/headers\" to=\"response.headers\"/&gt;\n    &lt;/route&gt;\n</code></pre> <p>dispatch : The dispatch element can be used to plug-in a custom dispatch class. The interface for Dispatch can be found in the module gateway-spi, org.apache.knox.gateway.dispatch.Dispatch.</p> <p>This element can be used at the service level (i.e. as a child of the service tag) or at the route level. A dispatch specified at the route level takes precedence over a dispatch specified at the service level. By default the dispatch used is org.apache.knox.gateway.dispatch.DefaultDispatch.</p> <p>The dispatch tag has four attributes that can be specified.</p> <p>contributor-name : This attribute can be used to specify a deployment contributor to be invoked for a custom dispatch.</p> <p>classname : This attribute can be used to specify a custom dispatch class.</p> <p>ha-contributor-name : This attribute can be used to specify a deployment contributor to be invoked for custom HA dispatch functionality.</p> <p>ha-classname : This attribute can be used to specify a custom dispatch class with HA functionality.</p> <p>Only one of contributor-name or classname should be specified and one of ha-contributor-name or ha-classname should be specified.</p> <p>If providing a custom dispatch, either a jar should be provided, see #[Class Path] or a #[Maven Module] should be created.</p> <p>Check out #[ConfigurableDispatch] about configurable dispatch type. </p> <p>policies : This is a wrapper tag for policy elements and can be a child of the service tag or the route tag. Once again, just like with dispatch, the route level policies defined override the ones at the service level.</p> <p>This element can contain one or more policy elements. The order of the policy elements is important as that will be the order of execution.</p> <p>policy : At this time the policy element just has two attributes, role and name. These are used to execute a deployment contributor by that role and name. Therefore new policies must be added by using the deployment contributor mechanism.</p> <p>For example,</p> <pre><code>&lt;service role=\"FOO\" name=\"foo\" version=\"1.6.0\"&gt;\n    &lt;policies&gt;\n        &lt;policy role=\"webappsec\"/&gt;\n        &lt;policy role=\"authentication\"/&gt;\n        &lt;policy role=\"rewrite\"/&gt;\n        &lt;policy role=\"identity-assertion\"/&gt;\n        &lt;policy role=\"authorization\"/&gt;\n    &lt;/policies&gt;\n    &lt;routes&gt;\n        &lt;route path=\"/foo/?**\"&gt;\n            &lt;rewrite apply=\"FOO/foo/inbound\" to=\"request.url\"/&gt;\n            &lt;policies&gt;\n                &lt;policy role=\"webappsec\"/&gt;\n                &lt;policy role=\"federation\"/&gt;\n                &lt;policy role=\"identity-assertion\"/&gt;\n                &lt;policy role=\"authorization\"/&gt;\n                &lt;policy role=\"rewrite\"/&gt;\n            &lt;/policies&gt;\n            &lt;dispatch contributor-name=\"http-client\" /&gt;\n        &lt;/route&gt;\n    &lt;/routes&gt;\n    &lt;dispatch contributor-name=\"custom-client\" ha-contributor-name=\"ha-client\"/&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"dev-guide/book/#configurabledispatch","title":"ConfigurableDispatch","text":"<p>The <code>ConfigurableDispatch</code> allows service definition writers to:</p> <ul> <li>exclude certain header(s) from the outbound HTTP request</li> <li>exclude certain header(s) from the outbound HTTP response</li> <li>declares whether parameters should be URL-encoded or not</li> </ul> <p>This dispatch type can be set in <code>service.xml</code> as follows:</p> <pre><code>&lt;dispatch classname=\"org.apache.knox.gateway.dispatch.ConfigurableDispatch\" ha-classname=\"org.apache.knox.gateway.ha.dispatch.ConfigurableHADispatch\"&gt;\n  &lt;param&gt;\n    &lt;name&gt;requestExcludeHeaders&lt;/name&gt;\n    &lt;value&gt;Authorization,Content-Length&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;responseExcludeHeaders&lt;/name&gt;\n    &lt;value&gt;SET-COOKIE,WWW-AUTHENTICATE&lt;/value&gt;\n  &lt;/param&gt;\n  &lt;param&gt;\n    &lt;name&gt;removeUrlEncoding&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/param&gt;\n&lt;/dispatch&gt;\n</code></pre> <p>The default values of these parameters are:</p> <ul> <li><code>requestExcludeHeaders</code> = <code>Host,Authorization,Content-Length,Transfer-Encoding</code></li> <li><code>responseExcludeHeaders</code> = <code>SET-COOKIE,WWW-AUTHENTICATE</code></li> <li><code>removeUrlEncoding</code> = <code>false</code></li> </ul> <p>The <code>responseExcludeHeaders</code> handling allows excluding only certain directives of the <code>SET-COOKIE</code> HTTP header. The following sample shows how to ecxlude only the <code>HttpOnly</code> directive from <code>SET-COOKIE</code> and the <code>WWW-AUTHENTICATE</code> header entirely in the routbound response HTTP header:</p> <pre><code>&lt;dispatch classname=\"org.apache.knox.gateway.dispatch.ConfigurableDispatch\" ha-classname=\"org.apache.knox.gateway.ha.dispatch.ConfigurableHADispatch\"&gt;&gt;\n  &lt;param&gt;\n    &lt;name&gt;responseExcludeHeaders&lt;/name&gt;\n    &lt;value&gt;WWW-AUTHENTICATE,SET-COOKIE:HttpOnly&lt;/value&gt;\n  &lt;/param&gt;\n&lt;/dispatch&gt;\n</code></pre>"},{"location":"dev-guide/book/#rewritexml","title":"rewrite.xml","text":"<p>The rewrite.xml file that accompanies the service.xml file follows the same rules as described in the section #[Rewrite Provider].</p>"},{"location":"dev-guide/book/#service-definition-directory-structure","title":"Service Definition Directory Structure","text":"<p>On installation of the Knox gateway, the following directory structure can be found under ${GATEWAY_HOME}/data. This is a mirror of the directories and files under the module gateway-service-definitions.</p> <pre><code>services\n    |______ service name\n                    |______ version\n                                |______service.xml\n                                |______rewrite.xml\n</code></pre> <p>For example,</p> <pre><code>services\n    |______ webhdfs\n               |______ 2.4.0\n                         |______service.xml\n                         |______rewrite.xml\n</code></pre> <p>To test out a new service, you can just add the appropriate files (service.xml and rewrite.xml) in a directory under ${GATEWAY_HOME}/data/services. If you want to make the service contribution to the Knox build, they files need to go in the gateway-service-definitions module.</p>"},{"location":"dev-guide/book/#service-definition-runtime-behavior","title":"Service Definition Runtime Behavior","text":"<p>The runtime artifacts as well as the behavior does not change whether the service is plugged in via the deployment descriptors or through a service.xml file.</p>"},{"location":"dev-guide/book/#custom-dispatch-dependency-injection","title":"Custom Dispatch Dependency Injection","text":"<p>When writing a custom dispatch class, one often needs configuration or gateway services. A lightweight dependency injection system is used that can inject instances of classes or primitives available in the filter configuration's init params or as a servlet context attribute.</p> <p>Details of this can be found in the module gateway-util-configinjector and also an example use of it is in the class org.apache.knox.gateway.dispatch.DefaultDispatch. Look at the following method for example:</p> <pre><code> @Configure\n   protected void setReplayBufferSize(@Default(\"8\") int size) {\n      replayBufferSize = size;\n   }\n</code></pre>"},{"location":"dev-guide/book/#service-discovery","title":"Service Discovery","text":"<p>Knox supports the ability to dynamically determine endpoint URLs in topologies for supported Hadoop services. This functionality is currently only supported for Ambari-managed Hadoop clusters. There are a number of these services, which are officially supported, but this set can be extended by modifying the source or speciyfing external configuration.</p>"},{"location":"dev-guide/book/#service-url-definitions","title":"Service URL Definitions","text":"<p>The service discovery system determines service URLs by processing mappings of Hadoop service configuration properties and corresponding URL templates. The knowledge about converting these arbitrary service configuration properties into correct service endpoint URLs is defined in a configuration file internal to the Ambari service discovery module.</p>"},{"location":"dev-guide/book/#configuration-details","title":"Configuration Details","text":"<p>This internal configuration file ( ambari-service-discovery-url-mappings.xml ) in the gateway-discovery-ambari module is used to specify a URL template and the associated configuration properties to populate it. A limited degree of conditional logic is supported to accommodate things like http vs https configurations.</p> <p>The simplest way to describe its contents will be by examples.</p> <p>Example 1</p> <p>The simplest example of one such mapping involves a service component for which there is a single configuration property which specifies the complete URL</p> <pre><code>&lt;!-- This is the service mapping declaration. The name must match what is specified as a Knox topology service role --&gt;\n&lt;service name=\"OOZIE\"&gt;\n\n  &lt;!-- This is the URL pattern with palceholder(s) for values provided by properties --&gt;\n  &lt;url-pattern&gt;{OOZIE_URL}&lt;/url-pattern&gt;\n\n  &lt;properties&gt;\n\n    &lt;!-- This is a property, which in this simple case, matches a template placeholder --&gt;\n    &lt;property name=\"OOZIE_URL\"&gt;\n\n      &lt;!-- This is the component whose configuration will be used to lookup the value of the subsequent config property name --&gt;\n      &lt;component&gt;OOZIE_SERVER&lt;/component&gt;\n\n      &lt;!-- This is the name of the component config property whose value should be assigned to the OOZIE_URL value --&gt;\n      &lt;config-property&gt;oozie.base.url&lt;/config-property&gt;\n\n    &lt;/property&gt;\n  &lt;/properties&gt;\n&lt;/service&gt;\n</code></pre> <p>The OOZIE_SERVER component configuration is oozie-site If oozie-site.xml has the property named oozie.base.url with the value http://ooziehost:11000, then the resulting URL for the OOZIE service will be http://ooziehost:11000</p> <p>Example 2</p> <p>A slightly more complicated example involves a service component for which the complete URL is not described by a single detail, but rather multiple endpoint URL details</p> <pre><code>&lt;service name=\"WEBHCAT\"&gt;\n  &lt;url-pattern&gt;http://{HOST}:{PORT}/templeton&lt;/url-pattern&gt;\n  &lt;properties&gt;\n    &lt;property name=\"HOST\"&gt;\n      &lt;component&gt;WEBHCAT_SERVER&lt;/component&gt;\n\n      &lt;!-- This tells discovery to get the hostname for the WEBHCAT_SERVER component from Ambari --&gt;\n      &lt;hostname/&gt;\n\n    &lt;/property&gt;\n    &lt;property name=\"PORT\"&gt;\n      &lt;component&gt;WEBHCAT_SERVER&lt;/component&gt;\n\n      &lt;!-- This is the name of the component config property whose value should be assigned to --&gt;\n      &lt;!-- the PORT value --&gt;\n      &lt;config-property&gt;templeton.port&lt;/config-property&gt;\n\n    &lt;/property&gt;\n  &lt;/properties&gt;\n&lt;/service&gt;\n</code></pre> <p>Example 3</p> <p>An even more complicated example involves a service for which HTTPS is supported, and which employs the limited conditional logic support</p> <pre><code>&lt;service name=\"ATLAS\"&gt;\n  &lt;url-pattern&gt;{SCHEME}://{HOST}:{PORT}&lt;/url-pattern&gt;\n  &lt;properties&gt;\n\n    &lt;!-- Property for getting the ATLAS_SERVER component hostname from Ambari --&gt;\n    &lt;property name=\"HOST\"&gt;\n      &lt;component&gt;ATLAS_SERVER&lt;/component&gt;\n      &lt;hostname/&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Property for capturing whether TLS is enabled or not; This is not a template placeholder property --&gt;\n    &lt;property name=\"TLS_ENABLED\"&gt;\n      &lt;component&gt;ATLAS_SERVER&lt;/component&gt;\n      &lt;config-property&gt;atlas.enableTLS&lt;/config-property&gt;\n    &lt;/property&gt;\n    &lt;!-- Property for getting the http port ; also NOT a template placeholder property --&gt;\n    &lt;property name=\"HTTP_PORT\"&gt;\n      &lt;component&gt;ATLAS_SERVER&lt;/component&gt;\n      &lt;config-property&gt;atlas.server.http.port&lt;/config-property&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Property for getting the https port ; also NOT a template placeholder property --&gt;\n    &lt;property name=\"HTTPS_PORT\"&gt;\n      &lt;component&gt;ATLAS_SERVER&lt;/component&gt;\n      &lt;config-property&gt;atlas.server.https.port&lt;/config-property&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Template placeholder property, dependent on the TLS_ENABLED property value --&gt;\n    &lt;property name=\"PORT\"&gt;\n      &lt;config-property&gt;\n        &lt;if property=\"TLS_ENABLED\" value=\"true\"&gt;\n          &lt;then&gt;HTTPS_PORT&lt;/then&gt;\n          &lt;else&gt;HTTP_PORT&lt;/else&gt;\n        &lt;/if&gt;\n      &lt;/config-property&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Template placeholder property, dependent on the TLS_ENABLED property value --&gt;\n    &lt;property name=\"SCHEME\"&gt;\n      &lt;config-property&gt;\n        &lt;if property=\"TLS_ENABLED\" value=\"true\"&gt;\n          &lt;then&gt;https&lt;/then&gt;\n          &lt;else&gt;http&lt;/else&gt;\n        &lt;/if&gt;\n      &lt;/config-property&gt;\n    &lt;/property&gt;\n  &lt;/properties&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"dev-guide/book/#external-configuration","title":"External Configuration","text":"<p>The internal configuration for URL construction can be overridden or augmented by way of a configuration file in the gateway configuration directory, or an alternative file specified by a Java system property. This mechanism is useful for developing support for new services, for custom solutions, or any scenario for which rebuilding Knox is not desirable.</p> <p>The default file, for which Knox will search first, is {GATEWAY_HOME}/conf/ambari-discovery-url-mappings.xml</p> <p>If Knox doesn't find that file, it will check for a Java system property named org.apache.gateway.topology.discovery.ambari.config, whose value is the fully-qualified path to an XML file. This file's contents must adhere to the format outlined above.</p> <p>If this configuration exists, Knox will apply it as if it were part of the internal configuration.</p> <p>Example</p> <p>If Apache Solr weren't supported, then it could be added by creating the following definition in {GATEWAY_HOME}/conf/ambari-discovery-url-mappings.xml :</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;service-discovery-url-mappings&gt;\n  &lt;service name=\"SOLR\"&gt;\n    &lt;url-pattern&gt;http://{HOST}:{PORT}&lt;/url-pattern&gt;\n    &lt;properties&gt;\n      &lt;property name=\"HOST\"&gt;\n        &lt;component&gt;INFRA_SOLR&lt;/component&gt;\n        &lt;hostname/&gt;\n      &lt;/property&gt;\n      &lt;property name=\"PORT\"&gt;\n        &lt;component&gt;INFRA_SOLR&lt;/component&gt;\n        &lt;config-property&gt;infra_solr_port&lt;/config-property&gt;\n      &lt;/property&gt;\n    &lt;/properties&gt;\n  &lt;/service&gt;\n&lt;/service-discovery-url-mappings&gt;\n</code></pre> <p>N.B. Knox must be restarted for changes to this external configuration to be applied.</p>"},{"location":"dev-guide/book/#component-configuration-mapping","title":"Component Configuration Mapping","text":"<p>To support URL construction from service configuration files, Ambari service discovery requires knowledge of the service component types and their respective relationships to configuration types. This knowledge is defined in a configuration file internal to the Ambari service discovery module.</p>"},{"location":"dev-guide/book/#configuration-details_1","title":"Configuration Details","text":"<p>This internal configuration file ( ambari-service-discovery-component-config-mapping.properties ) in the gateway-discovery-ambari module is used to define the mapping of Hadoop service component names to the configuration type from which Knox will lookup property values.</p> <p>Example</p> <pre><code>NAMENODE=hdfs-site\nRESOURCEMANAGER=yarn-site\nHISTORYSERVER=mapred-site\nOOZIE_SERVER=oozie-site\nHIVE_SERVER=hive-site\nWEBHCAT_SERVER=webhcat-site\n</code></pre>"},{"location":"dev-guide/book/#external-configuration_1","title":"External Configuration","text":"<p>The internal configuration for component configuration mappings can be overridden or augmented by way of a configuration file in the gateway configuration directory, or an alternative file specified by a Java system property. This mechanism is useful for developing support for new services, for custom solutions, or any scenario for which rebuilding Knox is not desirable.</p> <p>The default file, for which Knox will search first, is {GATEWAY_HOME}/conf/ambari-discovery-component-config.properties If Knox doesn't find that file, it will check for a Java system property named org.apache.knox.gateway.topology.discovery.ambari.component.mapping, whose value is the fully-qualified path to a properties file. This file's contents must adhere to the format outlined above.</p> <p>If this configuration exists, Knox will apply it as if it were part of the internal configuration.</p> <p>Example</p> <p>Following the aforementioned SOLR example, Knox needs to know in which configuration file to find the INFRA_SOLR component configuration property, so the following property must be defined in {GATEWAY_HOME}/conf/ambari-discovery-component-config.properties :</p> <pre><code>INFRA_SOLR=infra-solr-env\n</code></pre> <p>This tells Knox to look for the infra_solr_port property in the infra-solr-env configuration.</p> <p>N.B. Knox must be restarted for changes to this external configuration to be applied.</p>"},{"location":"dev-guide/book/#validator","title":"Validator","text":"<p>Apache Knox provides preauth federation authentication where Knox supports two built-in validators for verifying incoming requests. In this section, we describe how to write a custom validator for this scenario. The provided validators include: </p> <ul> <li>preauth.default.validation: This default behavior does not perform any validation check. All requests will pass.</li> <li>preauth.ip.validation : This validation checks if a request is originated from an IP address which is configured in Knox service through property preauth.ip.addresses.</li> </ul> <p>However, these built-in validation choices may not fulfill the internal requirments of some organization. Therefore, Knox supports (since 0.12) a pluggble framework where anyone can include a custom validator. </p> <p>In essence, a user can add a custom validator by following these  steps. The corresponding code examples are incorporated after that:</p> <ol> <li>Create a separate Java package (e.g. com.company.knox.validator) in a new or existing Maven project.</li> <li>Create a new class (e.g. CustomValidator) that implements org.apache.knox.gateway.preauth.filter.PreAuthValidator.</li> <li>The class should implement the method String getName() that may returns a string constant. The step-9  will need this user defined string constant.</li> <li>The class should implement the method boolean validate(HttpServletRequest httpRequest, FilterConfig filterConfig). This is the key method which will validate the request based on 'httpRequest' and 'filterConfig'. In most common cases, user may need to use HTTP headers value to validate. For example, client can get a token from an authentication service and pass it as HTTP header. This validate method needs to extract that header and verify the token. In some instance, the server may need to contact the same authentication service to validate.</li> <li>Create a text file src/resources/META-INF/services and add fully qualified name of your custom validator class (e.g. com.company.knox.validator.CustomValidator).</li> <li>You may need to include the packages \"org.apache.knox.gateway-provider-security-preauth\"  of version 0.12+ and  \"javax.servlet.javax.servlet-api\" of version 3.1.0+ in pom.xml.</li> <li>Build your custom jar.</li> <li>Deploy the jar in $GATEWAY_HOME/ext directory.</li> <li>Add/modify a parameter called preauth.validation.method with the name of validator used in step #3. Optionally, you may add any new parameter that may be required only for your CustomValidator.</li> </ol> <p>Validator Class (Step 2-4) </p> <pre><code>package com.company.knox.validator;\n\nimport org.apache.knox.gateway.preauth.filter.PreAuthValidationException;\nimport org.apache.knox.gateway.preauth.filter.PreAuthValidator;\nimport com.google.common.base.Strings;\n\nimport javax.servlet.FilterConfig;\nimport javax.servlet.http.HttpServletRequest;\n\npublic class CustomValidator extends PreAuthValidator {\n  //Any string constant value should work for these 3 variables\n  //This string will be used in 'services' file.\n  public static final String CUSTOM_VALIDATOR_NAME = \"fooValidator\"; \n  //Optional: User may want to pass soemthign through HTTP header. (per client request)\n  public static final String CUSTOM_TOKEN_HEADER_NAME = \"foo_claim\";\n\n\n  /**\n   * @param httpRequest\n   * @param filterConfig\n   * @return\n   * @throws PreAuthValidationException\n   */\n  @Override\n  public boolean validate(HttpServletRequest httpRequest, FilterConfig filterConfig) throws PreAuthValidationException {\n    String claimToken = httpRequest.getHeader(CUSTOM_TOKEN_HEADER_NAME);\n    if (!Strings.isNullOrEmpty(claimToken)) {\n      return checkCustomeToken(claimToken); //to be implemented\n    } else {\n      log.warn(\"Claim token was empty for header name '\" + CUSTOM_TOKEN_HEADER_NAME + \"'\");\n      return false;\n    }\n  }\n\n  /**\n   * Define unique validator name\n   *\n   * @return\n   */\n  @Override\n  public String getName() {\n    return CUSTOM_VALIDATOR_NAME;\n  }\n}\n</code></pre> <p>META-INF/services contents (Step-5)</p> <p><code>com.company.knox.validator.CustomValidator</code></p> <p>POM file (Step-6)</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;\n    &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.knox&lt;/groupId&gt;\n    &lt;artifactId&gt;gateway-test-utils&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.knox&lt;/groupId&gt;\n    &lt;artifactId&gt;gateway-provider-security-preauth&lt;/artifactId&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Deploy Custom Jar (Step-7-8)</p> <p>Build the jar (e.g. customValidation.jar) using 'mvn clean package' <code>cp customValidation.jar $GATEWAY_HOME/ext/</code></p> <p>Topology Config (Step-9)</p> <pre><code>&lt;provider&gt;\n    &lt;role&gt;federation&lt;/role&gt;\n    &lt;name&gt;HeaderPreAuth&lt;/name&gt;\n    &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;param&gt;&lt;name&gt;preauth.validation.method&lt;/name&gt;\n    &lt;!--Same as CustomeValidator.CUSTOM_VALIDATOR_NAME   -&gt;\n    &lt;value&gt;fooValidator&lt;/value&gt;&lt;/param&gt;\n&lt;/provider&gt;\n</code></pre>"},{"location":"dev-guide/book/#providers","title":"Providers","text":"<pre><code>public interface ProviderDeploymentContributor {\n  String getRole();\n  String getName();\n\n  void initializeContribution( DeploymentContext context );\n  void contributeProvider( DeploymentContext context, Provider provider );\n  void contributeFilter(\n      DeploymentContext context,\n      Provider provider,\n      Service service,\n      ResourceDescriptor resource,\n      List&lt;FilterParamDescriptor&gt; params );\n\n  void finalizeContribution( DeploymentContext context );\n}\n</code></pre> <pre><code>&lt;project&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;parent&gt;\n        &lt;groupId&gt;org.apache.knox&lt;/groupId&gt;\n        &lt;artifactId&gt;gateway&lt;/artifactId&gt;\n        &lt;version&gt;2.1.0-SNAPSHOT&lt;/version&gt;\n    &lt;/parent&gt;\n\n    &lt;artifactId&gt;gateway-provider-security-authn-sample&lt;/artifactId&gt;\n    &lt;name&gt;gateway-provider-security-authn-sample&lt;/name&gt;\n    &lt;description&gt;A simple sample authorization provider.&lt;/description&gt;\n\n    &lt;licenses&gt;\n        &lt;license&gt;\n            &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt;\n            &lt;url&gt;https://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt;\n            &lt;distribution&gt;repo&lt;/distribution&gt;\n        &lt;/license&gt;\n    &lt;/licenses&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;${gateway-group}&lt;/groupId&gt;\n            &lt;artifactId&gt;gateway-spi&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n\n&lt;/project&gt;\n</code></pre>"},{"location":"dev-guide/book/#deployment-context","title":"Deployment Context","text":"<pre><code>package org.apache.knox.gateway.deploy;\n\nimport ...\n\npublic interface DeploymentContext {\n\n  GatewayConfig getGatewayConfig();\n\n  Topology getTopology();\n\n  WebArchive getWebArchive();\n\n  WebAppDescriptor getWebAppDescriptor();\n\n  GatewayDescriptor getGatewayDescriptor();\n\n  void contributeFilter(\n      Service service,\n      ResourceDescriptor resource,\n      String role,\n      String name,\n      List&lt;FilterParamDescriptor&gt; params );\n\n  void addDescriptor( String name, Object descriptor );\n\n  &lt;T&gt; T getDescriptor( String name );\n\n}\n</code></pre> <pre><code>public class Topology {\n\n  public URI getUri() {...}\n  public void setUri( URI uri ) {...}\n\n  public String getName() {...}\n  public void setName( String name ) {...}\n\n  public long getTimestamp() {...}\n  public void setTimestamp( long timestamp ) {...}\n\n  public Collection&lt;Service&gt; getServices() {...}\n  public Service getService( String role, String name ) {...}\n  public void addService( Service service ) {...}\n\n  public Collection&lt;Provider&gt; getProviders() {...}\n  public Provider getProvider( String role, String name ) {...}\n  public void addProvider( Provider provider ) {...}\n}\n</code></pre> <pre><code>public interface GatewayDescriptor {\n  List&lt;GatewayParamDescriptor&gt; params();\n  GatewayParamDescriptor addParam();\n  GatewayParamDescriptor createParam();\n  void addParam( GatewayParamDescriptor param );\n  void addParams( List&lt;GatewayParamDescriptor&gt; params );\n\n  List&lt;ResourceDescriptor&gt; resources();\n  ResourceDescriptor addResource();\n  ResourceDescriptor createResource();\n  void addResource( ResourceDescriptor resource );\n}\n</code></pre>"},{"location":"dev-guide/book/#gateway-services","title":"Gateway Services","text":"<p>TODO - Describe the service registry and other global services.</p>"},{"location":"dev-guide/book/#standard-providers","title":"Standard Providers","text":""},{"location":"dev-guide/book/#rewrite-provider","title":"Rewrite Provider","text":"<p>gateway-provider-rewrite org.apache.knox.gateway.filter.rewrite.api.UrlRewriteRulesDescriptor</p> <pre><code>&lt;rules&gt;\n  &lt;rule\n      dir=\"IN\"\n      name=\"WEATHER/openweathermap/inbound/versioned/file\"\n      pattern=\"*://*:*/**/weather/{version}?{**}\"&gt;\n    &lt;rewrite template=\"{$serviceUrl[WEATHER]}/{version}/weather?{**}\"/&gt;\n  &lt;/rule&gt;\n&lt;/rules&gt;\n</code></pre> <pre><code>&lt;rules&gt;\n  &lt;filter name=\"WEBHBASE/webhbase/status/outbound\"&gt;\n    &lt;content type=\"*/json\"&gt;\n      &lt;apply path=\"$[LiveNodes][*][name]\" rule=\"WEBHBASE/webhbase/address/outbound\"/&gt;\n    &lt;/content&gt;\n    &lt;content type=\"*/xml\"&gt;\n      &lt;apply path=\"/ClusterStatus/LiveNodes/Node/@name\" rule=\"WEBHBASE/webhbase/address/outbound\"/&gt;\n    &lt;/content&gt;\n  &lt;/filter&gt;\n&lt;/rules&gt;\n</code></pre> <pre><code>@Test\npublic void testDevGuideSample() throws Exception {\n  URI inputUri, outputUri;\n  Matcher&lt;Void&gt; matcher;\n  Matcher&lt;Void&gt;.Match match;\n  Template input, pattern, template;\n\n  inputUri = new URI( \"http://sample-host:8443/gateway/topology/weather/2.5?q=Palo+Alto\" );\n\n  input = Parser.parse( inputUri.toString() );\n  pattern = Parser.parse( \"*://*:*/**/weather/{version}?{**}\" );\n  template = Parser.parse( \"http://api.openweathermap.org/data/{version}/weather?{**}\" );\n\n  matcher = new Matcher&lt;Void&gt;();\n  matcher.add( pattern, null );\n  match = matcher.match( input );\n\n  outputUri = Expander.expand( template, match.getParams(), null );\n\n  assertThat(\n      outputUri.toString(),\n      is( \"http://api.openweathermap.org/data/2.5/weather?q=Palo+Alto\" ) );\n}\n</code></pre> <pre><code>@Test\npublic void testDevGuideSampleWithEvaluator() throws Exception {\n  URI inputUri, outputUri;\n  Matcher&lt;Void&gt; matcher;\n  Matcher&lt;Void&gt;.Match match;\n  Template input, pattern, template;\n  Evaluator evaluator;\n\n  inputUri = new URI( \"http://sample-host:8443/gateway/topology/weather/2.5?q=Palo+Alto\" );\n  input = Parser.parse( inputUri.toString() );\n\n  pattern = Parser.parse( \"*://*:*/**/weather/{version}?{**}\" );\n  template = Parser.parse( \"{$serviceUrl[WEATHER]}/{version}/weather?{**}\" );\n\n  matcher = new Matcher&lt;Void&gt;();\n  matcher.add( pattern, null );\n  match = matcher.match( input );\n\n  evaluator = new Evaluator() {\n    @Override\n    public List&lt;String&gt; evaluate( String function, List&lt;String&gt; parameters ) {\n      return Arrays.asList( \"http://api.openweathermap.org/data\" );\n    }\n  };\n\n  outputUri = Expander.expand( template, match.getParams(), evaluator );\n\n  assertThat(\n      outputUri.toString(),\n      is( \"http://api.openweathermap.org/data/2.5/weather?q=Palo+Alto\" ) );\n}\n</code></pre>"},{"location":"dev-guide/book/#rewrite-filters","title":"Rewrite Filters","text":"<p>TODO - Cover the supported content types. TODO - Provide a XML and JSON \"properties\" example where one NVP is modified based on value of another name.</p> <pre><code>&lt;rules&gt;\n  &lt;filter name=\"WEBHBASE/webhbase/regions/outbound\"&gt;\n    &lt;content type=\"*/json\"&gt;\n      &lt;apply path=\"$[Region][*][location]\" rule=\"WEBHBASE/webhbase/address/outbound\"/&gt;\n    &lt;/content&gt;\n    &lt;content type=\"*/xml\"&gt;\n      &lt;apply path=\"/TableInfo/Region/@location\" rule=\"WEBHBASE/webhbase/address/outbound\"/&gt;\n    &lt;/content&gt;\n  &lt;/filter&gt;\n&lt;/rules&gt;\n</code></pre> <pre><code>&lt;gateway&gt;\n  ...\n  &lt;resource&gt;\n    &lt;role&gt;WEBHBASE&lt;/role&gt;\n    &lt;pattern&gt;/hbase/*/regions?**&lt;/pattern&gt;\n    ...\n    &lt;filter&gt;\n      &lt;role&gt;rewrite&lt;/role&gt;\n      &lt;name&gt;url-rewrite&lt;/name&gt;\n      &lt;class&gt;org.apache.knox.gateway.filter.rewrite.api.UrlRewriteServletFilter&lt;/class&gt;\n      &lt;param&gt;\n        &lt;name&gt;response.body&lt;/name&gt;\n        &lt;value&gt;WEBHBASE/webhbase/regions/outbound&lt;/value&gt;\n      &lt;/param&gt;\n    &lt;/filter&gt;\n    ...\n  &lt;/resource&gt;\n  ...\n&lt;/gateway&gt;\n</code></pre> <p>HBaseDeploymentContributor <pre><code>    params = new ArrayList&lt;FilterParamDescriptor&gt;();\n    params.add( regionResource.createFilterParam().name( \"response.body\" ).value( \"WEBHBASE/webhbase/regions/outbound\" ) );\n    addRewriteFilter( context, service, regionResource, params );\n</code></pre></p>"},{"location":"dev-guide/book/#rewrite-functions","title":"Rewrite Functions","text":"<p>TODO - Provide an lowercase function as an example.</p> <pre><code>&lt;rules&gt;\n  &lt;functions&gt;\n    &lt;hostmap config=\"/WEB-INF/hostmap.txt\"/&gt;\n  &lt;/functions&gt;\n  ...\n&lt;/rules&gt;\n</code></pre>"},{"location":"dev-guide/book/#rewrite-steps","title":"Rewrite Steps","text":"<p>TODO - Provide an lowercase step as an example.</p> <pre><code>&lt;rules&gt;\n  &lt;rule dir=\"OUT\" name=\"WEBHDFS/webhdfs/outbound/namenode/headers/location\"&gt;\n    &lt;match pattern=\"{scheme}://{host}:{port}/{path=**}?{**}\"/&gt;\n    &lt;rewrite template=\"{gateway.url}/webhdfs/data/v1/{path=**}?{scheme}?host={$hostmap(host)}?{port}?{**}\"/&gt;\n    &lt;encrypt-query/&gt;\n  &lt;/rule&gt;\n&lt;/rules&gt;\n</code></pre>"},{"location":"dev-guide/book/#identity-assertion-provider","title":"Identity Assertion Provider","text":"<p>Adding a new identity assertion provider is as simple as extending the AbstractIdentityAsserterDeploymentContributor and the CommonIdentityAssertionFilter from the gateway-provider-identity-assertion-common module to initialize any specific configuration from filter init params and implement two methods:</p> <ol> <li>String mapUserPrincipal(String principalName);</li> <li>String[] mapGroupPrincipals(String principalName, Subject subject);</li> </ol> <p>To implement a simple toUpper or toLower identity assertion provider:</p> <p><pre><code>package org.apache.knox.gateway.identityasserter.caseshifter.filter;\n\nimport org.apache.knox.gateway.identityasserter.common.filter.AbstractIdentityAsserterDeploymentContributor;\n\npublic class CaseShifterIdentityAsserterDeploymentContributor extends AbstractIdentityAsserterDeploymentContributor {\n\n  @Override\n  public String getName() {\n    return \"CaseShifter\";\n  }\n\n  protected String getFilterClassname() {\n    return CaseShifterIdentityAssertionFilter.class.getName();\n  }\n}\n</code></pre> We merely need to provide the provider name for use in the topology and the filter classname for the contributor to add to the filter chain.</p> <p>For the identity assertion filter itself it is just a matter of extension and the implementation of the two methods described earlier:</p> <pre><code>package org.apache.knox.gateway.identityasserter.caseshifter.filter;\n\nimport javax.security.auth.Subject;\nimport javax.servlet.FilterConfig;\nimport javax.servlet.ServletException;\nimport org.apache.knox.gateway.identityasserter.common.filter.CommonIdentityAssertionFilter;\n\npublic class CaseShifterIdentityAssertionFilter extends CommonIdentityAssertionFilter {\n  private boolean toUpper = false;\n\n  @Override\n  public void init(FilterConfig filterConfig) throws ServletException {\n    String upper = filterConfig.getInitParameter(\"caseshift.upper\");\n    if (\"true\".equals(upper)) {\n      toUpper = true;\n    }\n    else {\n      toUpper = false;\n    }\n  }\n\n  @Override\n  public String[] mapGroupPrincipals(String mappedPrincipalName, Subject subject) {\n    return null;\n  }\n\n  @Override\n  public String mapUserPrincipal(String principalName) {\n    if (toUpper) {\n      principalName = principalName.toUpperCase();\n    }\n    else {\n      principalName = principalName.toLowerCase();\n    }\n    return principalName;\n  }\n}\n</code></pre> <p>Note that the above: </p> <ol> <li>looks for specific filter init parameters for configuration of whether to convert to upper or to lower case</li> <li>it no-ops the mapGroupPrincipals so that it returns null. This indicates that there are no changes needed to the groups contained within the Subject. If there are groups then they should be continued to flow through the system unchanged. This is actually the same implementation as the base class and is therefore not required to be overridden. We include it here for illustration.</li> <li>based upon the configuration interrogated in the init method the principalName is convert to either upper or lower case.</li> </ol> <p>That is the extent of what is needed to implement a new identity assertion provider module.</p>"},{"location":"dev-guide/book/#jersey-provider","title":"Jersey Provider","text":"<p>TODO</p>"},{"location":"dev-guide/book/#knoxsso-integration","title":"KnoxSSO Integration","text":"<pre><code>@startuml\n!include knoxsso_integration.puml\n@enduml\n</code></pre>"},{"location":"dev-guide/book/#health-monitoring-api","title":"Health Monitoring API","text":"<pre><code>@startuml\n!include knox_monitoring_api.puml\n@enduml\n</code></pre>"},{"location":"dev-guide/book/#auditing","title":"Auditing","text":"<pre><code>public class AuditingSample {\n\n  private static Auditor AUDITOR = AuditServiceFactory.getAuditService().getAuditor(\n      \"sample-channel\", \"sample-service\", \"sample-component\" );\n\n  public void sampleMethod() {\n      ...\n      AUDITOR.audit( Action.AUTHORIZATION, sourceUrl, ResourceType.URI, ActionOutcome.SUCCESS );\n      ...\n  }\n\n}\n</code></pre>"},{"location":"dev-guide/book/#logging","title":"Logging","text":"<pre><code>@Messages( logger = \"org.apache.project.module\" )\npublic interface CustomMessages {\n\n  @Message( level = MessageLevel.FATAL, text = \"Failed to parse command line: {0}\" )\n  void failedToParseCommandLine( @StackTrace( level = MessageLevel.DEBUG ) ParseException e );\n\n}\n</code></pre> <pre><code>public class CustomLoggingSample {\n\n  private static GatewayMessages MSG = MessagesFactory.get( GatewayMessages.class );\n\n  public void sampleMethod() {\n    ...\n    MSG.failedToParseCommandLine( e );\n    ...\n  }\n\n}\n</code></pre>"},{"location":"dev-guide/book/#internationalization","title":"Internationalization","text":"<pre><code>@Resources\npublic interface CustomResources {\n\n  @Resource( text = \"Apache Hadoop Gateway {0} ({1})\" )\n  String gatewayVersionMessage( String version, String hash );\n\n}\n</code></pre> <pre><code>public class CustomResourceSample {\n\n  private static GatewayResources RES = ResourcesFactory.get( GatewayResources.class );\n\n  public void sampleMethod() {\n    ...\n    String s = RES.gatewayVersionMessage( \"0.0.0\", \"XXXXXXX\" ) );\n    ...\n  }\n\n}\n</code></pre>"},{"location":"dev-guide/knox_monitoring_api/","title":"Monitoring API","text":""},{"location":"dev-guide/knox_monitoring_api/#health-monitoring-rest-api","title":"Health Monitoring REST API","text":"<p>Knox provides REST-ful API for monitoring the core service. It primarily exposes the health of the Knox service that includes service status (up/down) as well as other health metrics. This is a work-in-progress feature, which started with an extensible framework to support basic functionalities. In particular, it currently supports the API to  A) ping the service and B) time-based statistics related to all API calls.</p>"},{"location":"dev-guide/knox_monitoring_api/#health-monitoring-setup","title":"Health Monitoring Setup","text":"<p>The basic setup includes two major steps A) add configurations to enable the metrics collection and reporting B) write a topology file and upload it into topologies directory.</p>"},{"location":"dev-guide/knox_monitoring_api/#service-configurations","title":"Service Configurations","text":"<p>At first, we need to make sure the gateway configurations to gather and report to JMX are turned on in gateway-site.xml. The following two configurations into gateway-site.xml will serve the purpose.</p> <pre><code>&lt;property&gt;\n   &lt;name&gt;gateway.metrics.enabled&lt;/name&gt;\n   &lt;value&gt;true&lt;/value&gt;\n   &lt;description&gt;Boolean flag indicates whether to enable the metrics collection&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n   &lt;name&gt;gateway.jmx.metrics.reporting.enabled&lt;/name&gt;\n   &lt;value&gt;true&lt;/value&gt;\n   &lt;description&gt;Boolean flag indicates whether to enable the metrics reporting using JMX&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"dev-guide/knox_monitoring_api/#healthxml-topology","title":"health.xml Topology","text":"<p>In order to enable health monitoring REST service, you need to add a new topology file (i.e. health.xml). The following is an example that is configured to test the basic functionalities of Knox service. It is highly recommended using more restricted authentication mechanism.</p> <pre><code>&lt;topology&gt;\n\n    &lt;gateway&gt;\n\n        &lt;provider&gt;\n            &lt;role&gt;authentication&lt;/role&gt;\n            &lt;name&gt;ShiroProvider&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;!-- \n                session timeout in minutes,  this is really idle timeout,\n                defaults to 30 mins, if the property value is not defined,, \n                current client authentication would expire if client idles continuously for more than this value\n                --&gt;\n                &lt;name&gt;sessionTimeout&lt;/name&gt;\n                &lt;value&gt;30&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm&lt;/name&gt;\n                &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n                &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n                &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n                &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n                &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n                &lt;value&gt;simple&lt;/value&gt;\n            &lt;/param&gt;\n            &lt;param&gt;\n                &lt;name&gt;urls./**&lt;/name&gt;\n                &lt;value&gt;authcBasic&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n\n        &lt;provider&gt;\n            &lt;role&gt;authorization&lt;/role&gt;\n            &lt;name&gt;AclsAuthz&lt;/name&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n            &lt;param&gt;\n                &lt;name&gt;knox.acl&lt;/name&gt;\n                &lt;value&gt;admin;*;*&lt;/value&gt;\n            &lt;/param&gt;\n        &lt;/provider&gt;\n\n        &lt;provider&gt;\n            &lt;role&gt;identity-assertion&lt;/role&gt;\n            &lt;name&gt;Default&lt;/name&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n        &lt;/provider&gt;\n\n        &lt;provider&gt;\n            &lt;role&gt;hostmap&lt;/role&gt;\n            &lt;name&gt;static&lt;/name&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;param&gt;&lt;name&gt;localhost&lt;/name&gt;&lt;value&gt;sandbox,sandbox.hortonworks.com&lt;/value&gt;&lt;/param&gt;\n        &lt;/provider&gt;\n\n    &lt;/gateway&gt;\n\n    &lt;service&gt;\n        &lt;role&gt;HEALTH&lt;/role&gt;\n    &lt;/service&gt;\n\n&lt;/topology&gt;\n</code></pre> <p>Just as with any Knox service, the gateway providers protect the health monitoring REST service defined above it. In this case, the ShiroProvider is taking care of HTTP Basic Auth using LDAP. Once the user authenticates with LDAP, the request processing continues to the Health service that will perform the necessary actions.</p> <p>The authenticate/federation provider can be swapped out to fit your deployment environment.</p> <p>After creating the file health.xml with above contents, you need to copy the file to KNOX_HOME/conf/topologies directory. If Knox/gateway service is not running, you can start it using \"bin/gateway.sh start\". Otherwise the service would automatically pick this new 'health' service. When gateway service registers the new service, it displays the following log messages in log/gateway.log.</p> <pre><code>2017-08-22 03:44:25,045 INFO  knox.gateway (GatewayServer.java:handleCreateDeployment(677)) - Deploying topology health to /home/joe/knox/knox-0.12.0/bin/../data/deployments/health.topo.15e080a91c0\n2017-08-22 03:44:25,045 INFO  knox.gateway (GatewayServer.java:internalDeactivateTopology(596)) - Deactivating topology health\n2017-08-22 03:44:25,119 INFO  knox.gateway (DefaultGatewayServices.java:initializeContribution(197)) - Creating credential store for the cluster: health\n2017-08-22 03:44:25,142 INFO  knox.gateway (GatewayServer.java:internalActivateTopology(566)) - Activating topology health\n2017-08-22 03:44:25,142 INFO  knox.gateway (GatewayServer.java:internalActivateArchive(576)) - Activating topology health archive %2F\n</code></pre>"},{"location":"dev-guide/knox_monitoring_api/#verify","title":"Verify","text":"<p>Once the health service is active, you can verify it by using the following curl command. The 'ping' end point displays if the service is up. This end point can be utilized for monitoring the basic health of a Knox service.</p> <pre><code>$ curl -i -k -u guest:guest-password -X GET 'https://localhost:8445/gateway/health/v1/ping'\nHTTP/1.1 200 OK\nDate: Tue, 22 Aug 2017 07:09:37 GMT\nSet-Cookie: JSESSIONID=1o82bcvoqbhbb1apt7zs8ubybb;Path=/gateway/health;Secure;HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/health; Max-Age=0; Expires=Mon, 21-Aug-2017 07:09:37 GMT\nCache-Control: must-revalidate,no-cache,no-store\nContent-Type: text/plain; charset=ISO-8859-1\nContent-Length: 3\nServer: Jetty(9.2.15.v20160210)\n\nOK\n</code></pre> <p>To retrieve the meaningful metrics details of various service calls, you may need to run multiple REST calls such as the followings. After that, execute the metrics REST call as shown below with a sample output. As shown, metrics output is returned in JSON format.</p> <p><pre><code>curl -i -k -u guest:guest-password -X GET 'https://localhost:8445/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS'\n</code></pre> <pre><code>$ curl -i -k -u guest:guest-password -X GET 'https://localhost:8445/gateway/health/v1/metrics?pretty=true'\nHTTP/1.1 200 OK\nDate: Tue, 22 Aug 2017 07:10:44 GMT\nSet-Cookie: JSESSIONID=kqntcdaje9uai3pup7ffvfw4;Path=/gateway/health;Secure;HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nSet-Cookie: rememberMe=deleteMe; Path=/gateway/health; Max-Age=0; Expires=Mon, 21-Aug-2017 07:10:44 GMT\nContent-Type: application/json\nCache-Control: must-revalidate,no-cache,no-store\nTransfer-Encoding: chunked\nServer: Jetty(9.2.15.v20160210)\n\n{\n  \"version\" : \"3.0.0\",\n  \"gauges\" : { },\n  \"counters\" : { },\n  \"histograms\" : { },\n  \"meters\" : { },\n  \"timers\" : {\n    \"client./gateway/health/v1/metrics.GET-requests\" : {\n      \"count\" : 5,\n      \"max\" : 0.624587973,\n      \"mean\" : 0.027655743001736188,\n      \"min\" : 0.006145587,\n      \"p50\" : 0.010020548,\n      \"p75\" : 0.010020548,\n      \"p95\" : 0.074454725,\n      \"p98\" : 0.624587973,\n      \"p99\" : 0.624587973,\n      \"p999\" : 0.624587973,\n      \"stddev\" : 0.0929226225229978,\n      \"m15_rate\" : 2.657500857422334E-7,\n      \"m1_rate\" : 5.770087852901534E-89,\n      \"m5_rate\" : 4.769163772973399E-19,\n      \"mean_rate\" : 4.0952378345310894E-4,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/health/v1/ping.GET-requests\" : {\n      \"count\" : 1,\n      \"max\" : 0.017257638000000002,\n      \"mean\" : 0.017257638000000002,\n      \"min\" : 0.017257638000000002,\n      \"p50\" : 0.017257638000000002,\n      \"p75\" : 0.017257638000000002,\n      \"p95\" : 0.017257638000000002,\n      \"p98\" : 0.017257638000000002,\n      \"p99\" : 0.017257638000000002,\n      \"p999\" : 0.017257638000000002,\n      \"stddev\" : 0.0,\n      \"m15_rate\" : 0.18710139700632353,\n      \"m1_rate\" : 0.0735758882342885,\n      \"m5_rate\" : 0.1637461506155964,\n      \"mean_rate\" : 0.014990517517814805,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/sandbox/health/v1/.GET-requests\" : {\n      \"count\" : 1,\n      \"max\" : 4.01873E-4,\n      \"mean\" : 4.01873E-4,\n      \"min\" : 4.01873E-4,\n      \"p50\" : 4.01873E-4,\n      \"p75\" : 4.01873E-4,\n      \"p95\" : 4.01873E-4,\n      \"p98\" : 4.01873E-4,\n      \"p99\" : 4.01873E-4,\n      \"p999\" : 4.01873E-4,\n      \"stddev\" : 0.0,\n      \"m15_rate\" : 2.536740427767808E-7,\n      \"m1_rate\" : 7.074903404511115E-90,\n      \"m5_rate\" : 4.081014139447941E-19,\n      \"mean_rate\" : 8.179827684854002E-5,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/sandbox/v1/health/.GET-requests\" : {\n      \"count\" : 1,\n      \"max\" : 5.470700000000001E-4,\n      \"mean\" : 5.470700000000001E-4,\n      \"min\" : 5.470700000000001E-4,\n      \"p50\" : 5.470700000000001E-4,\n      \"p75\" : 5.470700000000001E-4,\n      \"p95\" : 5.470700000000001E-4,\n      \"p98\" : 5.470700000000001E-4,\n      \"p99\" : 5.470700000000001E-4,\n      \"p999\" : 5.470700000000001E-4,\n      \"stddev\" : 0.0,\n      \"m15_rate\" : 2.413022137213267E-7,\n      \"m1_rate\" : 3.341947732164585E-90,\n      \"m5_rate\" : 3.512561421726287E-19,\n      \"mean_rate\" : 8.149518570285245E-5,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/sandbox/webhdfs/v1/.GET-requests\" : {\n      \"count\" : 4,\n      \"max\" : 0.463745401,\n      \"mean\" : 0.024924118143299912,\n      \"min\" : 0.016542244,\n      \"p50\" : 0.024799078000000002,\n      \"p75\" : 0.033933548,\n      \"p95\" : 0.033933548,\n      \"p98\" : 0.033933548,\n      \"p99\" : 0.033933548,\n      \"p999\" : 0.033933548,\n      \"stddev\" : 0.007284773511002474,\n      \"m15_rate\" : 2.120680068580741E-8,\n      \"m1_rate\" : 4.7541228609699333E-91,\n      \"m5_rate\" : 1.5806080232092864E-20,\n      \"mean_rate\" : 2.7314359915623396E-4,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"service./gateway/sandbox/webhdfs/v1/.get-requests\" : {\n      \"count\" : 3,\n      \"max\" : 0.014635496000000001,\n      \"mean\" : 0.00342438191233768,\n      \"min\" : 0.0020088890000000002,\n      \"p50\" : 0.0020088890000000002,\n      \"p75\" : 0.005144646,\n      \"p95\" : 0.005144646,\n      \"p98\" : 0.005144646,\n      \"p99\" : 0.005144646,\n      \"p999\" : 0.005144646,\n      \"stddev\" : 0.0015604555820128599,\n      \"m15_rate\" : 1.9913776931949195E-8,\n      \"m1_rate\" : 3.1334281325640874E-91,\n      \"m5_rate\" : 1.055281734633953E-20,\n      \"mean_rate\" : 2.0486339070804923E-4,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"dev-guide/knox_monitoring_api/#rest-end-points","title":"REST End Points","text":"<p>As mentioned above, currently Knox provides a few monitoring APIs to start with. The list will gradually grow to support new use-cases.</p>"},{"location":"dev-guide/knox_monitoring_api/#ping","title":"/ping","text":"<p>This end-point can be used to determine if a Knox gateway service is alive or not. It is useful for basic health monitoring of the core service. Although most of the results of REST calls are in JSON format, this one (/ping) is in plain text.  </p> <p>Sample response</p> <pre><code>OK\n</code></pre>"},{"location":"dev-guide/knox_monitoring_api/#metrics","title":"/metrics","text":"<p>This end-point returns all Knox metrics grouped by individual call type. For example, timer metrics for all webhdfs calls are aggregated into one set of metrics and then returned in a separate JSON element. This end-point also supports an option (/metrics?pretty=true) to pretty print the metrics output.</p> <p>A sample response with pretty=true is shown below:</p> <pre><code>{\n  \"version\" : \"3.0.0\",\n  \"gauges\" : { },\n  \"counters\" : { },\n  \"histograms\" : { },\n  \"meters\" : { },\n  \"timers\" : {\n    \"client./gateway/health/v1/ping.GET-requests\" : {\n      \"count\" : 1,\n      \"max\" : 0.017257638000000002,\n      \"mean\" : 0.017257638000000002,\n      \"min\" : 0.017257638000000002,\n      \"p50\" : 0.017257638000000002,\n      \"p75\" : 0.017257638000000002,\n      \"p95\" : 0.017257638000000002,\n      \"p98\" : 0.017257638000000002,\n      \"p99\" : 0.017257638000000002,\n      \"p999\" : 0.017257638000000002,\n      \"stddev\" : 0.0,\n      \"m15_rate\" : 0.18710139700632353,\n      \"m1_rate\" : 0.0735758882342885,\n      \"m5_rate\" : 0.1637461506155964,\n      \"mean_rate\" : 0.014990517517814805,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/sandbox/v1/health/.GET-requests\" : {\n      \"count\" : 1,\n      \"max\" : 5.470700000000001E-4,\n      \"mean\" : 5.470700000000001E-4,\n      \"min\" : 5.470700000000001E-4,\n      \"p50\" : 5.470700000000001E-4,\n      \"p75\" : 5.470700000000001E-4,\n      \"p95\" : 5.470700000000001E-4,\n      \"p98\" : 5.470700000000001E-4,\n      \"p99\" : 5.470700000000001E-4,\n      \"p999\" : 5.470700000000001E-4,\n      \"stddev\" : 0.0,\n      \"m15_rate\" : 2.413022137213267E-7,\n      \"m1_rate\" : 3.341947732164585E-90,\n      \"m5_rate\" : 3.512561421726287E-19,\n      \"mean_rate\" : 8.149518570285245E-5,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    },\n    \"client./gateway/sandbox/webhdfs/v1/.GET-requests\" : {\n      \"count\" : 4,\n      \"max\" : 0.463745401,\n      \"mean\" : 0.024924118143299912,\n      \"min\" : 0.016542244,\n      \"p50\" : 0.024799078000000002,\n      \"p75\" : 0.033933548,\n      \"p95\" : 0.033933548,\n      \"p98\" : 0.033933548,\n      \"p99\" : 0.033933548,\n      \"p999\" : 0.033933548,\n      \"stddev\" : 0.007284773511002474,\n      \"m15_rate\" : 2.120680068580741E-8,\n      \"m1_rate\" : 4.7541228609699333E-91,\n      \"m5_rate\" : 1.5806080232092864E-20,\n      \"mean_rate\" : 2.7314359915623396E-4,\n      \"duration_units\" : \"seconds\",\n      \"rate_units\" : \"calls/second\"\n    }\n  }\n}\n</code></pre>"},{"location":"dev-guide/knoxsso_integration/","title":"KnoxSSO Integration","text":""},{"location":"dev-guide/knoxsso_integration/#knox-sso-integration-for-uis","title":"Knox SSO Integration for UIs","text":""},{"location":"dev-guide/knoxsso_integration/#introduction","title":"Introduction","text":"<p>KnoxSSO provides an abstraction for integrating any number of authentication systems and SSO solutions and enables participating web applications to scale to those solutions more easily. Without the token exchange capabilities offered by KnoxSSO each component UI would need to integrate with each desired solution on its own. </p> <p>This document examines the way to integrate with Knox SSO in the form of a Servlet Filter. This approach should be easily extrapolated into other frameworks - ie. Spring Security.</p>"},{"location":"dev-guide/knoxsso_integration/#general-flow","title":"General Flow","text":"<p>The following is a generic sequence diagram for SAML integration through KnoxSSO.</p> <p>&lt;&gt;"},{"location":"dev-guide/knoxsso_integration/#knoxsso-setup","title":"KnoxSSO Setup","text":""},{"location":"dev-guide/knoxsso_integration/#knoxssoxml-topology","title":"knoxsso.xml Topology","text":"<p>In order to enable KnoxSSO, we need to configure the IdP topology. The following is an example of this topology that is configured to use HTTP Basic Auth against the Knox Demo LDAP server. This is the lowest barrier of entry for your development environment that actually authenticates against a real user store. What\u2019s great is if you work against the IdP with Basic Auth then you will work with SAML or anything else as well.</p> <pre><code>        &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n        &lt;topology&gt;\n            &lt;gateway&gt;\n                &lt;provider&gt;\n                    &lt;role&gt;authentication&lt;/role&gt;\n                    &lt;name&gt;ShiroProvider&lt;/name&gt;\n                    &lt;enabled&gt;true&lt;/enabled&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;sessionTimeout&lt;/name&gt;\n                        &lt;value&gt;30&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapRealm&lt;/name&gt;\n                        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapContextFactory&lt;/name&gt;\n                        &lt;value&gt;org.apache.knox.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;\n                        &lt;value&gt;$ldapContextFactory&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;\n                        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;\n                        &lt;value&gt;ldap://localhost:33389&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;\n                        &lt;value&gt;simple&lt;/value&gt;\n                    &lt;/param&gt;\n                    &lt;param&gt;\n                        &lt;name&gt;urls./**&lt;/name&gt;\n                        &lt;value&gt;authcBasic&lt;/value&gt;\n                    &lt;/param&gt;\n                &lt;/provider&gt;\n            &lt;provider&gt;\n                    &lt;role&gt;identity-assertion&lt;/role&gt;\n                    &lt;name&gt;Default&lt;/name&gt;\n                    &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;/provider&gt;\n            &lt;/gateway&gt;\n        &lt;service&gt;\n                &lt;role&gt;KNOXSSO&lt;/role&gt;\n                &lt;param&gt;\n                    &lt;name&gt;knoxsso.cookie.secure.only&lt;/name&gt;\n                    &lt;value&gt;true&lt;/value&gt;\n                &lt;/param&gt;\n                &lt;param&gt;\n                    &lt;name&gt;knoxsso.token.ttl&lt;/name&gt;\n                    &lt;value&gt;100000&lt;/value&gt;\n                &lt;/param&gt;\n            &lt;/service&gt;\n        &lt;/topology&gt;\n</code></pre> <p>Just as with any Knox service, the KNOXSSO service is protected by the gateway providers defined above it. In this case, the ShiroProvider is taking care of HTTP Basic Auth against LDAP for us. Once the user authenticates the request processing continues to the KNOXSSO service that will create the required cookie and do the necessary redirects.</p> <p>The authenticate/federation provider can be swapped out to fit your deployment environment.</p>"},{"location":"dev-guide/knoxsso_integration/#sandboxxml-topology","title":"sandbox.xml Topology","text":"<p>In order to see the end to end story and use it as an example in your development, you can configure one of the cluster topologies to use the SSOCookieProvider instead of the out of the box ShiroProvider. The following is an example sandbox.xml topology that is configured for using KnoxSSO to protect access to the Hadoop REST APIs.</p> <pre><code>    &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;topology&gt;\n  &lt;gateway&gt;\n    &lt;provider&gt;\n        &lt;role&gt;federation&lt;/role&gt;\n        &lt;name&gt;SSOCookieProvider&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;param&gt;\n            &lt;name&gt;sso.authentication.provider.url&lt;/name&gt;\n            &lt;value&gt;https://localhost:9443/gateway/idp/api/v1/websso&lt;/value&gt;\n        &lt;/param&gt;\n    &lt;/provider&gt;\n    &lt;provider&gt;\n        &lt;role&gt;identity-assertion&lt;/role&gt;\n        &lt;name&gt;Default&lt;/name&gt;\n        &lt;enabled&gt;true&lt;/enabled&gt;\n    &lt;/provider&gt;\n  &lt;/gateway&gt;    \n  &lt;service&gt;\n      &lt;role&gt;NAMENODE&lt;/role&gt;\n      &lt;url&gt;hdfs://localhost:8020&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;JOBTRACKER&lt;/role&gt;\n      &lt;url&gt;rpc://localhost:8050&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;WEBHDFS&lt;/role&gt;\n      &lt;url&gt;http://localhost:50070/webhdfs&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;WEBHCAT&lt;/role&gt;\n      &lt;url&gt;http://localhost:50111/templeton&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;OOZIE&lt;/role&gt;\n      &lt;url&gt;http://localhost:11000/oozie&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;WEBHBASE&lt;/role&gt;\n      &lt;url&gt;http://localhost:60080&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;HIVE&lt;/role&gt;\n      &lt;url&gt;http://localhost:10001/cliservice&lt;/url&gt;\n  &lt;/service&gt;\n  &lt;service&gt;\n      &lt;role&gt;RESOURCEMANAGER&lt;/role&gt;\n      &lt;url&gt;http://localhost:8088/ws&lt;/url&gt;\n  &lt;/service&gt;\n&lt;/topology&gt;\n</code></pre> <ul> <li>NOTE: Be aware that when using Chrome as your browser that cookies don\u2019t seem to work for \u201clocalhost\u201d. Either use a VM or like I did - use 127.0.0.1. Safari works with localhost without problems.</li> </ul> <p>As you can see above, the only thing being configured is the SSO provider URL. Since Knox is the issuer of the cookie and token, we don\u2019t need to configure the public key since we have programmatic access to the actual keystore for use at verification time.</p>"},{"location":"dev-guide/knoxsso_integration/#curl-the-flow","title":"Curl the Flow","text":"<p>We should now be able to walk through the SSO Flow at the command line with curl to see everything that happens.</p> <p>First, issue a request to WEBHDFS through knox.</p> <pre><code>    bash-3.2$ curl -iku guest:guest-password https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op+LISTSTATUS\n\n    HTTP/1.1 302 Found\n    Location: https://localhost:8443/gateway/idp/api/v1/websso?originalUrl=https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op+LISTSTATUS\n    Content-Length: 0\n    Server: Jetty(8.1.14.v20131031)\n</code></pre> <p>Note the redirect to the knoxsso endpoint and the loginUrl with the originalUrl request parameter. We need to see that come from your integration as well.</p> <p>Let\u2019s manually follow that redirect with curl now:</p> <pre><code>    bash-3.2$ curl -iku guest:guest-password \"https://localhost:8443/gateway/idp/api/v1/websso?originalUrl=https://localhost:9443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\"\n\n    HTTP/1.1 307 Temporary Redirect\n    Set-Cookie: JSESSIONID=mlkda4crv7z01jd0q0668nsxp;Path=/gateway/idp;Secure;HttpOnly\n    Set-Cookie: hadoop-jwt=eyJhbGciOiJSUzI1NiJ9.eyJleHAiOjE0NDM1ODUzNzEsInN1YiI6Imd1ZXN0IiwiYXVkIjoiSFNTTyIsImlzcyI6IkhTU08ifQ.RpA84Qdr6RxEZjg21PyVCk0G1kogvkuJI2bo302bpwbvmc-i01gCwKNeoGYzUW27MBXf6a40vylHVR3aZuuBUxsJW3aa_ltrx0R5ztKKnTWeJedOqvFKSrVlBzJJ90PzmDKCqJxA7JUhyo800_lDHLTcDWOiY-ueWYV2RMlCO0w;Path=/;Domain=localhost;Secure;HttpOnly\n    Expires: Thu, 01 Jan 1970 00:00:00 GMT\n    Location: https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n    Content-Length: 0\n    Server: Jetty(8.1.14.v20131031)\n</code></pre> <p>Note the redirect back to the original URL in the Location header and the Set-Cookie for the hadoop-jwt cookie. This is what the SSOCookieProvider in sandbox (and ultimately in your integration) will be looking for.</p> <p>Finally, we should be able to take the above cookie and pass it to the original url as indicated in the Location header for our originally requested resource:</p> <pre><code>    bash-3.2$ curl -ikH \"Cookie: hadoop-jwt=eyJhbGciOiJSUzI1NiJ9.eyJleHAiOjE0NDM1ODY2OTIsInN1YiI6Imd1ZXN0IiwiYXVkIjoiSFNTTyIsImlzcyI6IkhTU08ifQ.Os5HEfVBYiOIVNLRIvpYyjeLgAIMbBGXHBWMVRAEdiYcNlJRcbJJ5aSUl1aciNs1zd_SHijfB9gOdwnlvQ_0BCeGHlJBzHGyxeypIoGj9aOwEf36h-HVgqzGlBLYUk40gWAQk3aRehpIrHZT2hHm8Pu8W-zJCAwUd8HR3y6LF3M;Path=/;Domain=localhost;Secure;HttpOnly\" https://localhost:9443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n\n    TODO: cluster was down and needs to be recreated :/\n</code></pre>"},{"location":"dev-guide/knoxsso_integration/#browse-the-flow","title":"Browse the Flow","text":"<p>At this point, we can use a web browser instead of the command line and see how the browser will challenge the user for Basic Auth Credentials and then manage the cookies such that the SSO and token exchange aspects of the flow are hidden from the user.</p> <p>Simply, try to invoke the same webhdfs API from the browser URL bar.</p> <pre><code>        https://localhost:8443/gateway/sandbox/webhdfs/v1/tmp?op=LISTSTATUS\n</code></pre> <p>Based on our understanding of the flow it should behave like:</p> <ul> <li>SSOCookieProvider checks for hadoop-jwt cookie and in its absence redirects to the configured SSO provider URL (knoxsso endpoint)</li> <li>ShiroProvider on the KnoxSSO endpoint returns a 401 and the browser challenges the user for username/password</li> <li>The ShiroProvider authenticates the user against the Demo LDAP Server using a simple LDAP bind and establishes the security context for the WebSSO request</li> <li>The WebSSO service exchanges the normalized Java Subject into a JWT token and sets it on the response as a cookie named hadoop-jwt</li> <li>The WebSSO service then redirects the user agent back to the originally requested URL - the webhdfs Knox service subsequent invocations will find the cookie in the incoming request and not need to engage the WebSSO service again until it expires.</li> </ul>"},{"location":"dev-guide/knoxsso_integration/#filter-by-example","title":"Filter by Example","text":"<p>We have added a federation provider to Knox for accepting KnoxSSO cookies for REST APIs. This provides us with a couple benefits: KnoxSSO support for REST APIs for XmlHttpRequests from JavaScript (basic CORS functionality is also included). This is still rather basic and considered beta code. A model and real world usecase for others to base their integrations on</p> <p>In addition, https://issues.apache.org/jira/browse/HADOOP-11717 added support for the Hadoop UIs to the hadoop-auth module and it can be used as another example.</p> <p>We will examine the new SSOCookieFederationFilter in Knox here.</p> <pre><code>package org.apache.knox.gateway.provider.federation.jwt.filter;\n\n    import java.io.IOException;\n        import java.security.Principal;\n        import java.security.PrivilegedActionException;\n        import java.security.PrivilegedExceptionAction;\n        import java.util.ArrayList;\n        import java.util.Date;\n        import java.util.HashSet;\n        import java.util.List;\n        import java.util.Set;\n\n        import javax.security.auth.Subject;\n        import javax.servlet.Filter;\n        import javax.servlet.FilterChain;\n        import javax.servlet.FilterConfig;\n        import javax.servlet.ServletException;\n        import javax.servlet.ServletRequest;\n        import javax.servlet.ServletResponse;\n        import javax.servlet.http.Cookie;\n        import javax.servlet.http.HttpServletRequest;\n        import javax.servlet.http.HttpServletResponse;\n\n        import org.apache.knox.gateway.i18n.messages.MessagesFactory;\n        import org.apache.knox.gateway.provider.federation.jwt.JWTMessages;\n        import org.apache.knox.gateway.security.PrimaryPrincipal;\n        import org.apache.knox.gateway.services.GatewayServices;\n        import org.apache.knox.gateway.services.security.token.JWTokenAuthority;\n        import org.apache.knox.gateway.services.security.token.TokenServiceException;\n        import org.apache.knox.gateway.services.security.token.impl.JWTToken;\n\n        public class SSOCookieFederationFilter implements Filter {\n          private static JWTMessages log = MessagesFactory.get( JWTMessages.class );\n          private static final String ORIGINAL_URL_QUERY_PARAM = \"originalUrl=\";\n          private static final String SSO_COOKIE_NAME = \"sso.cookie.name\";\n          private static final String SSO_EXPECTED_AUDIENCES = \"sso.expected.audiences\";\n          private static final String SSO_AUTHENTICATION_PROVIDER_URL = \"sso.authentication.provider.url\";\n          private static final String DEFAULT_SSO_COOKIE_NAME = \"hadoop-jwt\";\n</code></pre> <p>The above represent the configurable aspects of the integration</p> <pre><code>    private JWTokenAuthority authority = null;\n    private String cookieName = null;\n    private List&lt;String&gt; audiences = null;\n    private String authenticationProviderUrl = null;\n\n    @Override\n    public void init( FilterConfig filterConfig ) throws ServletException {\n      GatewayServices services = (GatewayServices) filterConfig.getServletContext().getAttribute(GatewayServices.GATEWAY_SERVICES_ATTRIBUTE);\n      authority = (JWTokenAuthority)services.getService(GatewayServices.TOKEN_SERVICE);\n</code></pre> <p>The above is a Knox specific internal service that we use to issue and verify JWT tokens. This will be covered separately and you will need to be implement something similar in your filter implementation.</p> <pre><code>    // configured cookieName\n    cookieName = filterConfig.getInitParameter(SSO_COOKIE_NAME);\n    if (cookieName == null) {\n      cookieName = DEFAULT_SSO_COOKIE_NAME;\n    }\n</code></pre> <p>The configurable cookie name is something that can be used to change a cookie name to fit your deployment environment. The default name is hadoop-jwt which is also the default in the Hadoop implementation. This name must match the name being used by the KnoxSSO endpoint when setting the cookie.</p> <pre><code>    // expected audiences or null\n    String expectedAudiences = filterConfig.getInitParameter(SSO_EXPECTED_AUDIENCES);\n    if (expectedAudiences != null) {\n      audiences = parseExpectedAudiences(expectedAudiences);\n    }\n</code></pre> <p>Audiences are configured as a comma separated list of audience strings. Names of intended recipients or intents. The semantics that we are using for this processing is that - if not configured than any (or none) audience is accepted. If there are audiences configured then as long as one of the expected ones is found in the set of claims in the token it is accepted.</p> <pre><code>    // url to SSO authentication provider\n    authenticationProviderUrl = filterConfig.getInitParameter(SSO_AUTHENTICATION_PROVIDER_URL);\n    if (authenticationProviderUrl == null) {\n      log.missingAuthenticationProviderUrlConfiguration();\n    }\n  }\n</code></pre> <p>This is the URL to the KnoxSSO endpoint. It is required and SSO/token exchange will not work without this set correctly.</p> <pre><code>    /**\n    * @param expectedAudiences\n    * @return\n    */\n    private List&lt;String&gt; parseExpectedAudiences(String expectedAudiences) {\n     ArrayList&lt;String&gt; audList = null;\n       // setup the list of valid audiences for token validation\n       if (expectedAudiences != null) {\n         // parse into the list\n         String[] audArray = expectedAudiences.split(\",\");\n         audList = new ArrayList&lt;String&gt;();\n         for (String a : audArray) {\n           audList.add(a);\n         }\n       }\n       return audList;\n     }\n</code></pre> <p>The above method parses the comma separated list of expected audiences and makes it available for interrogation during token validation.</p> <pre><code>    public void destroy() {\n    }\n\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) \n        throws IOException, ServletException {\n      String wireToken = null;\n      HttpServletRequest req = (HttpServletRequest) request;\n\n      String loginURL = constructLoginURL(req);\n      wireToken = getJWTFromCookie(req);\n      if (wireToken == null) {\n        if (req.getMethod().equals(\"OPTIONS\")) {\n          // CORS preflight requests to determine allowed origins and related config\n          // must be able to continue without being redirected\n          Subject sub = new Subject();\n          sub.getPrincipals().add(new PrimaryPrincipal(\"anonymous\"));\n          continueWithEstablishedSecurityContext(sub, req, (HttpServletResponse) response, chain);\n        }\n        log.sendRedirectToLoginURL(loginURL);\n        ((HttpServletResponse) response).sendRedirect(loginURL);\n      }\n      else {\n        JWTToken token = new JWTToken(wireToken);\n        boolean verified = false;\n        try {\n          verified = authority.verifyToken(token);\n          if (verified) {\n            Date expires = token.getExpiresDate();\n            if (expires == null || new Date().before(expires)) {\n              boolean audValid = validateAudiences(token);\n              if (audValid) {\n                Subject subject = createSubjectFromToken(token);\n                continueWithEstablishedSecurityContext(subject, (HttpServletRequest)request, (HttpServletResponse)response, chain);\n              }\n              else {\n                log.failedToValidateAudience();\n                ((HttpServletResponse) response).sendRedirect(loginURL);\n              }\n            }\n            else {\n              log.tokenHasExpired();\n            ((HttpServletResponse) response).sendRedirect(loginURL);\n            }\n          }\n          else {\n            log.failedToVerifyTokenSignature();\n          ((HttpServletResponse) response).sendRedirect(loginURL);\n          }\n        } catch (TokenServiceException e) {\n          log.unableToVerifyToken(e);\n        ((HttpServletResponse) response).sendRedirect(loginURL);\n        }\n      }\n    }\n</code></pre> <p>The doFilter method above is where all the real work is done. We look for a cookie by the configured name. If it isn\u2019t there then we redirect to the configured SSO provider URL in order to acquire one. That is unless it is an OPTIONS request which may be a preflight CORS request. You shouldn\u2019t need to worry about this aspect. It is really a REST API concern not a web app UI one.</p> <p>Once we get a cookie, the underlying JWT token is extracted and returned as the wireToken from which we create a Knox specific JWTToken. This abstraction is around the use of the nimbus JWT library which you can use directly. We will cover those details separately.</p> <p>We then ask the token authority component to verify the token. This involves signature validation of the signed token. In order to verify the signature of the token you will need to have the public key of the Knox SSO server configured and provided to the nimbus library through its API at verification time. NOTE: This is a good place to look at the Hadoop implementation as an example.</p> <p>Once we know the token is signed by a trusted party we then validate whether it is expired and that it has an expected (or no) audience claims.</p> <p>Finally, when we have a valid token, we create a Java Subject from it and continue the request through the filterChain as the authenticated user.</p> <pre><code>    /**\n    * Encapsulate the acquisition of the JWT token from HTTP cookies within the\n    * request.\n    *\n    * @param req servlet request to get the JWT token from\n    * @return serialized JWT token\n    */\n    protected String getJWTFromCookie(HttpServletRequest req) {\n    String serializedJWT = null;\n    Cookie[] cookies = req.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        if (cookieName.equals(cookie.getName())) {\n          log.cookieHasBeenFound(cookieName);\n          serializedJWT = cookie.getValue();\n          break;\n        }\n      }\n    }\n    return serializedJWT;\n    }\n</code></pre> <p>The above method extracts the serialized token from the cookie and returns it as the wireToken.</p> <pre><code>    /**\n    * Create the URL to be used for authentication of the user in the absence of\n    * a JWT token within the incoming request.\n    *\n    * @param request for getting the original request URL\n    * @return url to use as login url for redirect\n    */\n    protected String constructLoginURL(HttpServletRequest request) {\n    String delimiter = \"?\";\n    if (authenticationProviderUrl.contains(\"?\")) {\n      delimiter = \"&amp;\";\n    }\n    String loginURL = authenticationProviderUrl + delimiter\n        + ORIGINAL_URL_QUERY_PARAM\n        + request.getRequestURL().toString()+ getOriginalQueryString(request);\n        return loginURL;\n    }\n\n    private String getOriginalQueryString(HttpServletRequest request) {\n        String originalQueryString = request.getQueryString();\n        return (originalQueryString == null) ? \"\" : \"?\" + originalQueryString;\n    }\n</code></pre> <p>The above method creates the full URL to be used in redirecting to the KnoxSSO endpoint. It includes the SSO provider URL as well as the original request URL so that we can redirect back to it after authentication and token exchange.</p> <pre><code>    /**\n    * Validate whether any of the accepted audience claims is present in the\n    * issued token claims list for audience. Override this method in subclasses\n    * in order to customize the audience validation behavior.\n    *\n    * @param jwtToken\n    *          the JWT token where the allowed audiences will be found\n    * @return true if an expected audience is present, otherwise false\n    */\n    protected boolean validateAudiences(JWTToken jwtToken) {\n        boolean valid = false;\n        String[] tokenAudienceList = jwtToken.getAudienceClaims();\n        // if there were no expected audiences configured then just\n        // consider any audience acceptable\n        if (audiences == null) {\n            valid = true;\n        } else {\n            // if any of the configured audiences is found then consider it\n            // acceptable\n            for (String aud : tokenAudienceList) {\n            if (audiences.contains(aud)) {\n                //log.debug(\"JWT token audience has been successfully validated\");\n                log.jwtAudienceValidated();\n                valid = true;\n                break;\n            }\n        }\n    }\n    return valid;\n    }\n</code></pre> <p>The above method implements the audience claim semantics explained earlier.</p> <pre><code>    private void continueWithEstablishedSecurityContext(Subject subject, final      HttpServletRequest request, final HttpServletResponse response, final FilterChain chain) throws IOException, ServletException {\n    try {\n      Subject.doAs(\n        subject,\n        new PrivilegedExceptionAction&lt;Object&gt;() {\n          @Override\n          public Object run() throws Exception {\n            chain.doFilter(request, response);\n            return null;\n          }\n        }\n        );\n    }\n    catch (PrivilegedActionException e) {\n      Throwable t = e.getCause();\n      if (t instanceof IOException) {\n        throw (IOException) t;\n      }\n      else if (t instanceof ServletException) {\n        throw (ServletException) t;\n      }\n      else {\n        throw new ServletException(t);\n      }\n    }\n    }\n</code></pre> <p>This method continues the filter chain processing upon successful validation of the token. This would need to be replaced with your environment\u2019s equivalent of continuing the request or login to the app as the authenticated user.</p> <p><pre><code>    private Subject createSubjectFromToken(JWTToken token) {\n        final String principal = token.getSubject();\n        @SuppressWarnings(\"rawtypes\")\n        HashSet emptySet = new HashSet();\n        Set&lt;Principal&gt; principals = new HashSet&lt;Principal&gt;();\n        Principal p = new PrimaryPrincipal(principal);\n        principals.add(p);\n        javax.security.auth.Subject subject = new javax.security.auth.Subject(true, principals, emptySet, emptySet);\n        return subject;\n    }\n</code></pre> This method takes a JWTToken and creates a Java Subject with the principals expected by the rest of the Knox processing. This would need to be implemented in a way appropriate for your operating environment as well. For instance, the Hadoop handler implementation returns a Hadoop AuthenticationToken to the calling filter which in turn ends up in the Hadoop auth cookie.</p> <pre><code>    }\n</code></pre>"},{"location":"dev-guide/knoxsso_integration/#token-signature-validation","title":"Token Signature Validation","text":"<p>The following is the method from the Hadoop handler implementation that validates the signature.</p> <pre><code>    /** \n    * Verify the signature of the JWT token in this method. This method depends on the  * public key that was established during init based upon the provisioned public key.    * Override this method in subclasses in order to customize the signature verification behavior.\n    * @param jwtToken the token that contains the signature to be validated\n    * @return valid true if signature verifies successfully; false otherwise\n    */\n    protected boolean validateSignature(SignedJWT jwtToken){\n        boolean valid=false;\n        if (JWSObject.State.SIGNED == jwtToken.getState()) {\n            LOG.debug(\"JWT token is in a SIGNED state\");\n            if (jwtToken.getSignature() != null) {\n                LOG.debug(\"JWT token signature is not null\");\n                try {\n                    JWSVerifier verifier=new RSASSAVerifier(publicKey);\n                    if (jwtToken.verify(verifier)) {\n                    valid=true;\n                    LOG.debug(\"JWT token has been successfully verified\");\n                }\n            else {\n                LOG.warn(\"JWT signature verification failed.\");\n            }\n        }\n        catch (JOSEException je) {\n            LOG.warn(\"Error while validating signature\",je);\n        }\n    }\n    }\n    return valid;\n    }\n</code></pre> <p>Hadoop Configuration Example The following is like the configuration in the Hadoop handler implementation.</p> <p>OBSOLETE but in the proper spirit of HADOOP-11717 ( HADOOP-11717 - Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth RESOLVED )</p> <pre><code>    &lt;property&gt;\n        &lt;name&gt;hadoop.http.authentication.type&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop/security.authentication/server.JWTRedirectAuthenticationHandler&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre> <p>This is the handler classname in Hadoop auth for JWT token (KnoxSSO) support.</p> <pre><code>    &lt;property&gt;\n        &lt;name&gt;hadoop.http.authentication.authentication.provider.url&lt;/name&gt;\n        &lt;value&gt;http://c6401.ambari.apache.org:8888/knoxsso&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre> <p>The above property is the SSO provider URL that points to the knoxsso endpoint.</p> <pre><code>    &lt;property&gt;\n        &lt;name&gt;hadoop.http.authentication.public.key.pem&lt;/name&gt;\n        &lt;value&gt;MIICVjCCAb+gAwIBAgIJAPPvOtuTxFeiMA0GCSqGSIb3DQEBBQUAMG0xCzAJBgNV\n    BAYTAlVTMQ0wCwYDVQQIEwRUZXN0MQ0wCwYDVQQHEwRUZXN0MQ8wDQYDVQQKEwZI\n    YWRvb3AxDTALBgNVBAsTBFRlc3QxIDAeBgNVBAMTF2M2NDAxLmFtYmFyaS5hcGFj\n    aGUub3JnMB4XDTE1MDcxNjE4NDcyM1oXDTE2MDcxNTE4NDcyM1owbTELMAkGA1UE\n    BhMCVVMxDTALBgNVBAgTBFRlc3QxDTALBgNVBAcTBFRlc3QxDzANBgNVBAoTBkhh\n    ZG9vcDENMAsGA1UECxMEVGVzdDEgMB4GA1UEAxMXYzY0MDEuYW1iYXJpLmFwYWNo\n    ZS5vcmcwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAMFs/rymbiNvg8lDhsdA\n    qvh5uHP6iMtfv9IYpDleShjkS1C+IqId6bwGIEO8yhIS5BnfUR/fcnHi2ZNrXX7x\n    QUtQe7M9tDIKu48w//InnZ6VpAqjGShWxcSzR6UB/YoGe5ytHS6MrXaormfBg3VW\n    tDoy2MS83W8pweS6p5JnK7S5AgMBAAEwDQYJKoZIhvcNAQEFBQADgYEANyVg6EzE\n    2q84gq7wQfLt9t047nYFkxcRfzhNVL3LB8p6IkM4RUrzWq4kLA+z+bpY2OdpkTOe\n    wUpEdVKzOQd4V7vRxpdANxtbG/XXrJAAcY/S+eMy1eDK73cmaVPnxPUGWmMnQXUi\n    TLab+w8tBQhNbq6BOQ42aOrLxA8k/M4cV1A=&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre> <p>The above property holds the KnoxSSO server\u2019s public key for signature verification. Adding it directly to the config like this is convenient and is easily done through Ambari to existing config files that take custom properties. Config is generally protected as root access only as well - so it is a pretty good solution.</p>"},{"location":"dev-guide/knoxsso_integration/#public-key-parsing","title":"Public Key Parsing","text":"<p>In order to turn the pem encoded config item into a public key the hadoop handler implementation does the following in the init() method.</p> <pre><code>    if (publicKey == null) {\n     String pemPublicKey = config.getProperty(PUBLIC_KEY_PEM);\n     if (pemPublicKey == null) {\n       throw new ServletException(\n           \"Public key for signature validation must be provisioned.\");\n     }\n     publicKey = CertificateUtil.parseRSAPublicKey(pemPublicKey);\n   }\n</code></pre> <p>and the CertificateUtil class is below:</p> <pre><code>    package org.apache.hadoop.security.authentication.util;\n\n    import java.io.ByteArrayInputStream;\n    import java.io.UnsupportedEncodingException;\n    import java.security.PublicKey;\n    import java.security.cert.CertificateException;\n    import java.security.cert.CertificateFactory;\n    import java.security.cert.X509Certificate;\n    import java.security.interfaces.RSAPublicKey;\n\n    import javax.servlet.ServletException;\n\n    public class CertificateUtil {\n        private static final String PEM_HEADER = \"-----BEGIN CERTIFICATE-----\\n\";\n        private static final String PEM_FOOTER = \"\\n-----END CERTIFICATE-----\";\n\n     /**\n      * Gets an RSAPublicKey from the provided PEM encoding.\n      *\n      * @param pem\n      *          - the pem encoding from config without the header and footer\n      * @return RSAPublicKey the RSA public key\n      * @throws ServletException thrown if a processing error occurred\n      */\n    public static RSAPublicKey parseRSAPublicKey(String pem) throws ServletException {\n        String fullPem = PEM_HEADER + pem + PEM_FOOTER;\n        PublicKey key = null;\n        try {\n            CertificateFactory fact = CertificateFactory.getInstance(\"X.509\");\n            ByteArrayInputStream is = new ByteArrayInputStream(\n                fullPem.getBytes(\"UTF8\"));\n            X509Certificate cer = (X509Certificate) fact.generateCertificate(is);\n            key = cer.getPublicKey();\n        } catch (CertificateException ce) {\n            String message = null;\n            if (pem.startsWith(PEM_HEADER)) {\n                message = \"CertificateException - be sure not to include PEM header \"\n                    + \"and footer in the PEM configuration element.\";\n            } else {\n                message = \"CertificateException - PEM may be corrupt\";\n            }\n            throw new ServletException(message, ce);\n        } catch (UnsupportedEncodingException uee) {\n            throw new ServletException(uee);\n        }\n        return (RSAPublicKey) key;\n        }\n    }\n</code></pre>"},{"location":"knoxshell-guide/knoxshell_user_guide/","title":"Knox Shell","text":""},{"location":"knoxshell-guide/knoxshell_user_guide/#apache-knox-knoxshell-21x-user-guide","title":"Apache Knox - KnoxShell 2.1.x User Guide","text":""},{"location":"knoxshell-guide/knoxshell_user_guide/#introduction","title":"Introduction","text":"<p>The KnoxShell environment has been extended to provide more of an interactive experience through the use of custom commands and the newly added KnoxShellTable rendering and dataset representation class. This is provided through by integrating the power of groovysh extensions and the KnoxShell client classes/SDK and make for some really powerful command line capabilities that would otherwise require the user to SSH to a node within the cluster and use CLIs of different tools or components.</p> <p>This document will cover the various KnoxShell extentions and how to use them on their own and describe combinations of them as flows for working with tabular data from various sources.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#representing-and-working-with-tabular-data","title":"Representing and Working with Tabular Data","text":"<p>The ability to read, write and work with tabular data formats such as CSV files, JDBC resultsets and others is core to the motivations of this KnoxShell oriented work. Intentions include: the ability to read arbitrary data from sources from inside a proxied cluster or from external sources, the ability to render the resulting tables, sort the table, filter it for specific subsets of the data and do some interesting calculations that can provide simple insights into your data.</p> <p>KnoxShellTable represents those core capabilties with its simple representation of a table, operation methods and builder classes.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#knoxshelltable","title":"KnoxShellTable","text":"<p>KnoxShellTable has a number of dedicated builders that have a fluent API for building table representations from various sources.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#builders","title":"Builders","text":"<p>The following builders aid in the creation of tables from various types of data sources.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#jdbc","title":"JDBC","text":"<pre><code>    ports = KnoxShellTable.builder().jdbc().\n      connect(\"jdbc:hive2://knox-host:8443/;ssl=true;transportMode=http;httpPath=topology/cdp-proxy-api/hive\").\n      driver(\"org.apache.hive.jdbc.HiveDriver\").\n      username(\"lmccay\").pwd(\"xxxx\").\n      sql(\"select * FROM ports\");\n</code></pre> <p>Running the above within KnoxShell will submit the provided SQL to HS2, create and assign a new KnoxShellTable instance to the \"ports\" variable representing the border ports of entry data.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#csv","title":"CSV","text":"<p><pre><code>crossings = KnoxShellTable.builder().csv().\n    withHeaders().\n    url(\"file:///home/lmccay/Border_Crossing_Entry_Data.csv\")\n</code></pre> Running the above within KnoxShell will import a CSV file from local disk, create and assign a new KnoxShellTable instance to the \"result\" variable.</p> <p>A higher level KnoxShell Custom Command allows for easier use of the builder through more natural syntax and hides the use of the lower level classes and syntax.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#join","title":"Join","text":"<pre><code>crossings = KnoxShellTable.builder().join().\n  left(ports).\n  right(crossings).\n  on(\"code\",\"Port Code\"\n</code></pre> <p>Running the above within KnoxShell will import a join the two tables with a simple match of the values in left and right tables on each row that matches.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#json","title":"JSON","text":"<p><pre><code>tornados = KnoxShellTable.builder().json().\n  url(\"file:///home/lmccay/.knoxshell/.tables/tornados.json\")\n</code></pre> Running the above within KnoxShell will rematerialize a table that was persisted as JSON and assign it to a local \"tornados\" variable.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#persistence-and-publishing","title":"Persistence and Publishing","text":"<p>Being able to create tables, combine them with other datasets, filter them and add new cols based on calculations between cols, etc is all great for creating tables in memory and working with them.</p> <p>We also want to be able to persist these tables in a KnoxShellTable canonical JSON format of its own and be able to reload the same datasets later.</p> <p>We also want to be able to take a given dataset and publish it as a brand new CSV file that can be pushed into HDFS, saved to local disk, written to cloud storage, etc.</p> <p>In addition, we may want to be able to write it directly to Hive or another JDBC datasource.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#json_1","title":"JSON","text":"<p><pre><code>tornados.toJSON()\n</code></pre> The above will return and render a JSON representation of the tornados KnoxShellTable including: headers, rows, optionally title and optionally callHistory.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#csv_1","title":"CSV","text":"<p><pre><code>tornados.toCSV()\n</code></pre> The above will return and render a CSV representation of the tornados KnoxShellTable including: headers (if present), and all rows.</p> <p>Note that title and callhistory which are KnoxShellTable specifics are excluded and lost unless also saved as JSON.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#usecases","title":"Usecases","text":"<ul> <li>JDBC Resultset Representations</li> <li>CSV Representations</li> <li>General Table Operations</li> <li>Joining</li> <li>Sorting, Selecting, Filtering, Calculations</li> <li>Persistence and Publishing</li> <li>KnoxLine SQL Shell</li> <li>Custom GroovySh Commands</li> </ul> <p>Let's take a look at each usecase.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#jdbc-resultset-representations","title":"JDBC Resultset Representations","text":"<p>KnoxLine SQL Client requires a tabular representation of the data from a SQL/JDBC Resultset. This requirement led to the creation of the KnoxShellTable JDBC Builder. It may be used outside of KnoxLine within your own Java clients or groovy scripts leveraging the KnoxShell classes.</p> <pre><code>ports = KnoxShellTable.builder().jdbc().\n  connect(\"jdbc:hive2://knox-host:8443/;ssl=true;transportMode=http;httpPath=topology/datalake-api/hive\").\n  driver(\"org.apache.hive.jdbc.HiveDriver\").\n  username(\"lmccay\").pwd(\"xxxx\").\n  sql(\"select * FROM ports\");\n</code></pre> <p>It can create the cols based on the metadata of the resultset and accurately represent the data and perform type specific operations, sorts, etc.</p> <p>A higher level KnoxShell Custom Command allows for the use of this builder with Datasources that are managed within the KnoxShell environment and persisted to the users' home directory to allow continued use across sessions. This command hides the use of the underlying classes and syntax and allows the user to concentrate on SQL.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#csv-representations","title":"CSV Representations","text":"<p>Another dedicated table builder is provided for creating a table from a CSV file that is imported via URL.</p> <p>Combined with all the general table operations and ability to join them with other KnoxShellTable representations, this allows for CSV data to be combined with JDBC datasets, filtered and republished as a new dataset or report to be rendered or even reexecuted later.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#general-table-operations","title":"General Table Operations","text":"<p>In addition to the builders described above, there are a number of operations that may be executed on the table itself.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#sorting","title":"Sorting","text":"<pre><code>tornados.sort(\"state\")\n</code></pre> <p>When a column is of String type values but they are numerics, you may also sort numerically.</p> <pre><code>tornados.sortNumeric(\"count\")\n</code></pre> <p>The above will sort the tornados table by the \"state\" column.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#selecting","title":"Selecting","text":"<p><pre><code>tornados.select(\"state,cat,inj,fat,date,month,day,year\")\n</code></pre> The above will return and render a new table with only the subset of cols selected.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#filtering","title":"Filtering","text":"<p><pre><code>tornados.filter().name(\"fat\").greaterThan(0)\n</code></pre> The above will return and render a table with only those tornados that resulted in one or more fatalities.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#fluent-api","title":"Fluent API","text":"<p>The above operations can be combined in a natural, fluent manner <pre><code>tornados.select(\"state,cat,inj,fat,date,month,day,year\").\n\n  filter().name(\"fat\").greaterThan(0).\n\n  sort(\"state\")\n</code></pre></p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#aggregating","title":"Aggregating","text":"<p>The following method allows for the use of table column calculations to build an aggregate view of helpful calculations for multiple columns in a table and summarizes them in a new table representation.</p> <pre><code>table.aggregate().columns(\"col1, col2, col3\").functions(\"min,max,mean,median,mode,sum\")\n</code></pre> <p>The above allows you to combine them by streaming them into each other in one line the select of only certain cols, the filtering of only those events with more than 0 fatalities and the much more efficient sort of the resulting table.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#knoxline-sql-shell","title":"KnoxLine SQL Shell","text":"<p>KnoxLine is a beeline like facility built into the KnoxShell client toolbox with basic datasource management and simple SQL client capabilities. ResultSets are rendered via KnoxShellTable but further table based manipulations are not available within the knoxline shell. This is purely dedicated to SQL interactions and table renderings.</p> <p>For leveraging the SQL builder of KnoxShellTable to be able to operate on the results locally, see the custom KnoxShell command 'SQL'.</p> <p></p> <p>Once connected to the datasource, SQL commands may be invoked via the command line directly.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#custom-groovysh-commands","title":"Custom GroovySh Commands","text":"<p>Groovy shell has the ability to extend the commands available to help automate scripting or coding that you would otherwise need to do programmatically over and over.</p> <p>By providing custom commands for KnoxShellTable operations,  builders and manipulation we can greatly simplify what would need to be done with the fluent API of KnoxShellTable and groovy/java code for saving state, etc.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#knoxshell-commands","title":"KnoxShell Commands:","text":"<ol> <li>Datasources (:datasource|:ds) CRUD and select operations for a set of JDBC datasources that are persisted to disk (KNOX-2128)</li> <li>SQL (:SQL|:sql) SQL query execution with persisted SQL history per datasource (KNOX-2128)</li> <li>CSV (:CSV|:csv) Import and Export from CSV and JSON formats</li> <li>Filesystem (:Filesystem|:fs) POSIX style commands for HDFS and cloud storage (mount, unmount, mounts, ls, rm, mkdir, cat, put, etc)</li> </ol>"},{"location":"knoxshell-guide/knoxshell_user_guide/#example-covid19-data-flow-into-datalake","title":"EXAMPLE: COVID19 Data Flow into DataLake","text":"<p>Let's start to put the commands and table capabilities together to consume some public tabular data and usher it into our datalake or cluster.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#build-table-from-public-csv-file","title":"Build Table from Public CSV File","text":"<p>The use of the CSV KnoxShell command above can be easily correlated to the CSV builder of KnoxShellTable. It is obviously less verbose and more natural than using the fluent API of KnoxShellTable directly and also leverages a separate capability for KnoxShell to assign the resulting table to a KnoxShell variable that can be references and manipulated afterward.</p> <p>As you can see the result of creating the table from a CSV file is a rendering of the entire table and often does not fit the screen propertly. This is where the operations on the resulting table come in handy for explorer the dataset. Let's filter the above dataset of COVID19 across the world to only a subset of columns and for only New Jersey by selecting, filtering and sorting numerically by number of Confirmed cases.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#select-columns-filter-and-sort-by-column","title":"Select Columns, Filter and Sort by Column","text":"<p>First we will interrogate the table for its column names or headers. Then we will select only those columns that we want in order to fit it to the screen, filter it for only New Jersey information and sort numerically by the number of Confirmed cases per county.</p> <p></p> <p>From the above operation, we can now see the COVID19 data for New Jersey counties for 4/10/2020 sorted by the number of Confirmed cases and the subset of cols of the most interest and tailored to fit our screen. From the above table, we can visually see a number of insights in terms of the most affected counties across the state of New Jersey but it may be more interesting to be able to see an aggregation of some of the calculations available for numeric columns through KnoxShellTable. Let's take a look at an aggregate table for this dataset.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#aggregate-calculations-on-columns-of-table","title":"Aggregate Calculations on Columns of Table","text":"<p>Since the KnoxShellTable fluent API allows us to chain such operations together easily, we will just hit the up arrow to get the previous table operation command and add the aggregate operation to the chain.</p> <p></p> <p>Now, by using both tables above, we can see that my county of Camden is both visually in approximately the center of the counties in terms of Confirmed case numbers but how it stands related to both the average and the median calculations. You can also see the sum of all of New Jersey and the number of those that belong to my county.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#persist-tables-to-local-disk","title":"Persist Tables to Local Disk","text":"<p>Next, we will persist these tables to our local disk and then push them into our HDFS based datalake for access by cluster resources and other users.</p> <p></p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#add-tables-to-datalake","title":"Add Tables to DataLake","text":"<p>Now that we have these tables persisted to local disk, we can use our KnoxShell Filesystem commands to add them to the datalake.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#building-knoxshell-truststore","title":"Building KnoxShell Truststore","text":"<p>Before we can access resources from datalake behind Knox we need to insure that the cert presented by the Knox instance is trusted. If the deployment is using certs signed by a well-known ca, then we generally don't have to do anything. If we are using Knox self-signed certs or certs signed by an internal ca of some sort then we must import them into the KnoxShell truststore. While this can be located in arbitrary places and configured via system properties and environment variables, the most common approach is to use then default location.</p> <pre><code>// exit knoxshell\n^C\n\nbin/knoxshell.sh buildTrustStore https://nightly7x-1.nightly7x.root.hwx.site:8443/gateway/datalake-api\n\nls -l ~/gateway-client-trust.jks\n\n// to reenter knoxshell\nbin/knoxshell.sh\n</code></pre> <p>With the default password of 'changeit'.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#mount-a-webhdfs-filesystem","title":"Mount a WebHDFS Filesystem","text":"<p>We may now mount a filesystem from the remote Knox instance by mounting the topology that hosts the WebHDFS API endpoint.</p> <pre><code>:fs mount https://nightly7x-1.nightly7x.root.hwx.site:8443/gateway/datalake-api nightly\n</code></pre>"},{"location":"knoxshell-guide/knoxshell_user_guide/#accessing-a-filesystem","title":"Accessing a Filesystem","text":"<p>Once we have the desired mount, we may now access it by specifying the mountpoint name as the path prefix into the HDFS filesystem. Upon mounting or first access, the KnoxShell will prompt for user credentials for use as HTTP Basic credentials while accessing WebHDFS API.</p> <p></p> <p>Once we authenticate to the mounted filesystem, we reference it by mountpoint and never concern ourselves with the actual URL to the endpoint.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#put-tables-into-datalake","title":"Put Tables into DataLake","text":"<p>Above, we have put the previously persisted CSV files into the tmp directory of the mounted filesystem to be available to other datalake users.</p> <p>We can now also access them from any other KnoxShell instance that has mounted this filesystem with appropriate credentials. Let's now cat the contents of one of the CSV files into the KnoxShell and then render it as a table from the raw CSV format.</p>"},{"location":"knoxshell-guide/knoxshell_user_guide/#pull-csv-files-from-webhdfs-and-create-tables","title":"Pull CSV Files from WebHDFS and Create Tables","text":"<p>Note that the cat command returns the CSV file contents as a string to the KnoxShell environment as a variable called '_' .</p> <p>This is true of any command in groovysh or KnoxShell. The previous result is always available as this variable. Here we pass the contents of the variable to the CSV KnoxShellTable builder string() method. This is a very convenient way to render tabular data from a cat'd file from your remote datalake. </p> <p>Also note that tables that are assigned to variables within KnoxShell will render themselves just by typing the variable name.</p>"}]}